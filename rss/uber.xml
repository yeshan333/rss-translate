<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>优步工程博客</title><link>https://www.uber.com/blog/engineering</link><description>Uber 工程背后的技术 - 由 RSSHub 用爱制作(https://github.com/DIYgod/RSSHub)</description><lastBuildDate>Tue, 28 May 2024 16:04:23 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>【Uber Becomes Kotlin™ Foundation Silver Member】Uber 成为 Kotlin™ 基金会银牌会员</title><link>https://www.uber.com/blog/kotlin-foundation-member/</link><description>&lt;p&gt;我们很高兴地宣布，Uber 已作为银牌会员加入 Kotlin &lt;sup&gt;™&lt;/sup&gt;基金会。&lt;/p&gt;&lt;p&gt; Uber 对 Kotlin 的承诺在我们的代码库中显而易见。我们正在积极为 Kotlin 生态系统做出贡献，包括开发 Kotlin 与 Buck、Bazel 和 Gradle 等构建系统的集成。 Uber 还为 Detekt 的开发做出了贡献，并发起了其编译器插件版本的创建。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;blockquote class="wp-block-quote has-000000-color has-white-background-color has-text-color has-background has-link-color wp-elements-267bc928adce8748455de4dd6b36c501 is-layout-flow wp-block-quote-is-layout-flow"&gt;&lt;p class="has-000000-color has-text-color has-link-color wp-elements-529663472951b895d728ed42ff0c690e"&gt; “我们很高兴加入 Kotlin 基金会，这突显了我们对 Kotlin 社区的承诺，以及我们对 Kotlin 作为帮助 Uber 成功的技术堆栈核心部分的信念。我们每天有数百万行 Kotlin 代码和数百名热情的开发人员使用它进行编写。作为从 2018 年开始的早期采用者和积极的贡献者，我们很自豪能够帮助成熟生态系统，并期待与这个充满活力、友好和创新的社区继续合作。”&lt;/p&gt; &lt;cite&gt;Ty Smith，Uber 首席工程师&lt;/cite&gt;&lt;/blockquote&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;除了为 Kotlin 做出贡献之外，Uber 还帮助建立了企业 Java 到 Kotlin 工作组，这是 Meta、Google、JetBrains 和 Uber 之间的合作，目标是为公司提供将大型遗留 Java 代码库迁移到 Kotlin 所需的工具和专业知识。 。&lt;/p&gt;&lt;p&gt;作为银牌会员，Uber 将在支持基金会的举措方面发挥至关重要的作用，包括针对开源库作者的资助计划和针对学生的 Kotlin 多平台竞赛。我们很高兴能与 Kotlin 基金会在未来的项目上进行合作。&lt;/p&gt;&lt;p&gt;访问 Kotlin 基金会&lt;a href="http://kotlinfoundation.org/" rel="noreferrer noopener" target="_blank"&gt;网站，&lt;/a&gt;详细了解为支持基金会使命而开展的重要工作。&lt;/p&gt;</description><pubDate>Wed, 22 May 2024 18:37:07 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/kotlin-foundation-member/</guid></item><item><title>【How Uber Accomplishes Job Counting At Scale】Uber 如何实现大规模工作统计</title><link>https://www.uber.com/blog/job-counting-at-scale/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;Uber 运营规模庞大，每个季度为超过 22 亿次出行提供便利。即使是简单的见解也需要规模化的解决方案。在我们的例子中，我们需要计算某人在 Uber 平台上、任意时间窗口内参与的作业数量。本文重点介绍我们将 Apache Pinot™ 集成到我们的解决方案中时面临的挑战和吸取的经验教训。&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-background-nbsp"&gt;背景&lt;/h2&gt;&lt;p&gt;具体来说，我们的解决方案需要解决：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;职位数量的几种排列，按角色、市场和完整性轴细分&lt;/li&gt;&lt;li&gt;给定行程或给定时间戳的时间点任期（即，按时间顺序，工作 X 位于人员 A 的工作历史记录中的什么位置？）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们之前的解决方案很简单：检索页面大小限制为 50 的给定主题的职位，并对结果进行分页，直到没有更多职位为止。在 Uber 的早期阶段，由于没有任何一个账户能够获得相对多的任期，因此这种方式运作良好。然而，Uber 涉足新的垂直领域，一些账户开始出现数以万计的任期，很明显，我们需要一个更强大的解决方案。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-trip-specific-tenures-are-a-narrow-use-case"&gt;特定旅行的任期是一个狭窄的用例&lt;/h3&gt;&lt;p&gt;主要的产品要求是该解决方案必须能够计算任期回顾。这本身是站得住脚的，但伴随着我们的数据保留政策，我们的下游团队认为这是不合理的。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-access-rescission-of-data-older-than-2-years"&gt; 2年以上数据的访问撤销&lt;/h3&gt;&lt;p&gt;出于节省成本的考虑，同一团队确定将超过 2 年的数据隔离到更高延迟的存储层。然而，项目中期计划的改变导致他们完全放弃了对这些数据的在线访问。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-reduced-complexity"&gt;降低复杂性&lt;/h3&gt;&lt;p&gt;我们当时的解决方案需要为 Uber 的每个市场拼接三个数据源：乘车和餐饮。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-similar-use-cases-in-the-broader-team"&gt;更广泛的团队中的类似用例&lt;/h3&gt;&lt;p&gt;在向更广泛的团队展示我们的设计后，我们立即发现 Uber 的各个团队都有类似需求的相邻项目，每个团队都重新实现了自己的解决方案来独立计算工作任期。&lt;/p&gt;&lt;p&gt;考虑到这些限制，我们考虑了几种架构。我们认真考虑过的一个方案是由 Apache Hive™ 和 Docstore（Uber 的内部分布式数据库）支持，最终采用了一种利用 Apache Pinot™ 的解决方案。这里的一个有趣的功能是混合表，它提供了一个将实时和离线数据缝合在一起的无缝界面。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-architecture"&gt;建筑学&lt;/h2&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large is-resized"&gt;&lt;img alt="" class="wp-image-1089197" height="2850" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/figure1-1.png" style="width: 700px; height: auto;" width="3617" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：用户权属存储架构图。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-development-challenges"&gt;发展挑战&lt;/h2&gt;&lt;p&gt;Apache Pinot™ 是一款极其强大的产品，为我们的架构提供了无与伦比的灵活性。然而，我们在此过程中遇到了一些障碍，下面，我们详细介绍了这些挑战以及我们对这些问题的解决方案，可能会对读者有所启发。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-challenge-1a-capacity-planning"&gt;&lt;strong&gt;挑战 #1a – 容量规划&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们的第一个挑战是制定容量要求，为我们的专用租户提供所需的硬件。 Apache Pinot™ 利用压缩技术，因此很难预先测量磁盘空间。在这里，我们选择采用 10% 的样本数据集，并且根据样本占用的空间来预测磁盘使用情况在我们的案例中就足够了。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-challenge-1b-query-performance"&gt;&lt;strong&gt;挑战 #1b – 查询性能&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;这里一个有趣的旁注是，虽然我们能够通过采样来估计磁盘上的数据集大小，但我们无法准确预测查询性能。在这种情况下，我们必须等到扩大整个数据集之后才能获取关键指标，例如 p99 读取时间、磁盘读取吞吐量等。一旦我们能够这样做，生产规模的流量就会带来我们的专用集群崩溃了，读取时间超过了代理限制 10 秒，SSD 上的读取吞吐量达到最大，CPU 使用率也达到了极限。我们立即着手研究优化，这并不意外，因为每个查询都相当于加载全表扫描。&lt;/p&gt;&lt;p&gt;作为参考，我们的查询具有以下形式：&lt;/p&gt;&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;&lt;p&gt;&lt;em&gt;选择&lt;/em&gt;&lt;em&gt;&lt;br /&gt;&lt;/em&gt;&lt;em&gt;*&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;从&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;皮诺混合表&lt;br /&gt;其中provider_id = &amp;#39;...&amp;#39;&lt;br /&gt; AND requester_id = &amp;#39;...&amp;#39;&lt;br /&gt; AND 时间戳 &amp;gt;= ... AND 时间戳 &amp;lt;= ...&lt;/em&gt; &lt;/p&gt;&lt;/blockquote&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/mziEvOUPidZxsIp2eyVVTr3vJyvJpPwk-syVG52f6ZIVB6DZIeIxsNlTGW2RzcUFBCJGmw2vJgCNOtBoXggHKwQFIvyv-QvjxLO4oNCyLnrbdD7Sr6ybJfltL3xspF4eZEHrXs-297Y3Mzm-IlR8YHE" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：由于代理超时导致 Apache Pinot™ 查询失败。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;这关系到团队，我们采取了多项措施来提高绩效，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;已排序的&lt;em&gt;provider_id&lt;/em&gt;列&lt;/strong&gt;&lt;br /&gt;这会将同一提供商在同一天进行的所有行程并置，从而最大限度地减少每个查询访问的路段。&lt;br /&gt;如果没有这一点，供应商在某一天完成的所有工作将平均分配给该天的所有部门。因此，如果提供者在同一天执行了多个作业，为了让代理完成我们的读取查询，它将快速收敛到检索提供者每天完成一项工作的所有段。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;在&lt;em&gt;provider_id&lt;/em&gt;和&lt;em&gt;requester_id&lt;/em&gt;上添加倒排索引&lt;br /&gt;&lt;/strong&gt;我们还在&lt;em&gt;provider_id&lt;/em&gt;和&lt;em&gt;requester_id&lt;/em&gt;列上启用了倒排索引。与排序列相结合，这在&lt;em&gt;provider_id&lt;/em&gt;列上提供了排序倒排索引。这允许&lt;em&gt;log(n)&lt;/em&gt;的查找时间复杂度，因为它执行二分搜索来查找与给定的&lt;em&gt;provider_id&lt;/em&gt;值对应的行。&lt;br /&gt;&lt;br /&gt;一个令我们措手不及的令人惊讶的事实是，这些倒排索引是&lt;em&gt;为每个细分市场&lt;/em&gt;创建的。这是与传统 RDBMS 索引之间非常显着的区别。该索引不是所有段共享的全局结构，直接指向正在查询的数据。在我们的例子中，代理仍然必须对表中每个段的一部分进行内存映射才能使用索引。如果不实施额外的段修剪技术，这是非常低效的，我们很快就实施了。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;在&lt;em&gt;provider_id&lt;/em&gt;和&lt;em&gt;requester_id&lt;/em&gt;上添加布隆过滤器&lt;/strong&gt;&lt;br /&gt;布隆过滤器是一种概率数据结构，测试一个元素是否是集合的成员，给出两个答案：可能在集合中，或不在集合中。当为列启用时，Apache Pinot™ 为每个段创建一个布隆过滤器，并允许代理在完成查询时完全跳过段。如果段上的列存在布隆过滤器，并且查询中存在该列的相等谓词，则代理能够快速确定该记录是否存在于段中。由于我们的数据集并不完全适合内存，因此我们选择了 MMAP（内存映射）堆外配置，其中段被延迟加载到内存中，如果操作系统物理内存不足，则先前加载的段将被取消映射（就像我们的例子一样）。然而，该段的关联布隆过滤器可以存储在堆上（内存中），并且将保留在那里，即使它们的底层段不再位于物理内存中。&lt;br /&gt;&lt;br /&gt;观察到的加速应该与跳过的段数相关，因此它有利于读取模式不需要获取大部分段的数据集（例如全表扫描）。&lt;br /&gt;&lt;br /&gt;请参见图 3 和图 4，其中可以观察到启用 Bloom 过滤器后 ( &lt;em&gt;numSegmentsQueried&lt;/em&gt; – &lt;em&gt;numSegmentsProcessed)&lt;/em&gt;显着下降。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;每天增加的路段&lt;/strong&gt;&lt;br /&gt;为了创建 Apache Pinot™ 段，我们安排每天运行一个 Apache Spark™ 作业，创建并上传组成离线表的新段。每个计划间隔创建的分段数量是可调的，我们最初每天创建四个分段。然而，随着段开始变得非常大（每个段超过 4GB），我们逐渐将其增加到 8 个，然后是 16 个，最后达到每天 32 个段。&lt;br /&gt;&lt;br /&gt;这里所做的权衡是，虽然增加的段数可能会导致 Zookeeper 元数据存储上的负载增加，并增加 Apache Pinot™ 服务器的内存堆使用量，但较小的段大小会导致更快地从磁盘上读取段，并且相应地会增加磁盘上的数据。要包含在结果中的段。根据经验，我们观察到 p99 读取延迟显着减少，并且代理 CPU 使用率增加不明显。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;向我们的上游消费者添加了跨数据中心缓存&lt;/strong&gt;&lt;br /&gt;虽然不是特定于 Apache Pinot™，但我们的主要上游消费者在临时环境和生产环境之间执行相同的查询。虽然我们期望查询之间的时间延迟最小，但我们认为 30 分钟的陈旧是可以接受的。&lt;/li&gt;&lt;/ul&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;统计&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;在布隆过滤器之前&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;布隆过滤器之后&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;使用时间（毫秒）&lt;/td&gt;&lt;td&gt;第387章&lt;/td&gt;&lt;td&gt;1740&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;已扫描文档数&lt;/td&gt;&lt;td&gt;21&lt;/td&gt;&lt;td&gt; 21&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;总文档数&lt;/td&gt;&lt;td&gt;50,520,067,053&lt;/td&gt;&lt;td&gt; 50,550,326,486&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;查询的服务器数&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt; 18&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;响应服务器数&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt; 18&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;查询的段数&lt;/td&gt;&lt;td&gt;20,491&lt;/td&gt;&lt;td&gt; 20,488&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;已处理的段数&lt;/td&gt;&lt;td&gt;4,829&lt;/td&gt;&lt;td&gt; 48&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;匹配的段数&lt;/td&gt;&lt;td&gt;17 号&lt;/td&gt;&lt;td&gt;17 号&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;查询的消费段数&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt; 2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;已扫描条目数&lt;/td&gt;&lt;td&gt;16,466,904&lt;/td&gt;&lt;td&gt; 147&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;图 3：通过 Pinot 查询控制台实施布隆过滤器之前和之后的查询统计信息&lt;/em&gt;。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1089225" height="575" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/figure5-1024x575.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：有和没有布隆过滤器的性能比较。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-challenge-2-business-edge-cases"&gt;&lt;strong&gt;挑战 #2 – 业务边缘案例&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;随着 Uber 不断向平台添加功能，它们对源数据的下游影响也在不断增加。目前，我们可以通过三种方式接收行程级别信息：Apache Hive™、Apache Kafka™ 主题和 API 响应，每种方式都有不同的模式。以合理的方式在现有模式中改造和表示新功能可能很困难，特别是 Apache Hive™ 模式。历史数据具有很大的惯性，可能会使迁移模式变得不合理。&lt;/p&gt;&lt;p&gt;例如，考虑票价分摊功能，其中乘车费用可以由多个乘客分摊。在此功能之前，一项工作总是有一个骑手，而该骑手始终是付款人，并且每条 Hive 记录都意味着司机在订单上执行了一项工作。这些不变量不再成立。此处选择复制记录并将状态设置为&lt;em&gt;FARE_SPLIT&lt;/em&gt; ，同时将&lt;em&gt;driver_uuid&lt;/em&gt;列设置为&lt;em&gt;NULL&lt;/em&gt; 。&lt;/p&gt;&lt;p&gt;正是这样的复杂性使得执行简单的&lt;em&gt;COUNT DISTINCT&lt;/em&gt;聚合变得不可能。对于每个业务案例，必须清楚地考虑决定记录是否有助于主题的任期：&lt;/p&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;重新调度行程（派遣另一名司机来完成已分配的工作）&lt;/td&gt;&lt;td&gt;工作任期应归属于谁？两位司机？如果这些记录表示为多条记录，那么它是否应该算作骑手的 2 份工作？&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;宾客乘坐&lt;/td&gt;&lt;td&gt;由于这些帐户的性质，它们可能会导致热分片，从而导致表扫描查询的成本高昂。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;未履行的订单&lt;/td&gt;&lt;td&gt;未履行的订单是否应该有助于任期？&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;请求者取消、订单失败等&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;同一订单上同一供应商的多次派送&lt;/td&gt;&lt;td&gt;由同一司机完成多项工作的 1 个订单应为请求者贡献多少任期？&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;预定订单&lt;/td&gt;&lt;td&gt;尚未发生的命令是否应该针对某个主题累积任期？ &lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-challenge-3-slow-data"&gt;&lt;strong&gt;挑战 #3 – 数据缓慢&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们必须解决的另一个挑战是上游数据缓慢。虽然大多数数据会在几秒钟内到达，但行程可能长达一周不会出现在上游数据源中。为了解决这个问题，我们创建了一个动态生成回填管道的管道，但仅安排在 T – 7d 和 T 之间运行。&lt;/p&gt;&lt;p&gt;除此之外，我们还执行离线数据质量检查，即 T – 1d 日期的简单&lt;em&gt;COUNT(*)&lt;/em&gt;查询，以确保我们的源 Apache Hive™ 表与 Apache Pinot™ 混合表的结果同步。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-challenge-4-bursty-upstream-loads"&gt;&lt;strong&gt;挑战 #4 – 突发的上游负载&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;流量模式尖峰的上游流量也是一个问题。在这种情况下，特定的速率限制实施会导致流量每十秒出现一次大幅峰值，每个流量都违反了十秒的 Apache Pinot™ 代理超时，导致请求失败。&lt;/p&gt;&lt;p&gt;我们通过在请求时向上游客户端添加抖动来解决这个问题，以便随着时间的推移更均匀地分布我们的查询。&lt;/p&gt;&lt;p&gt;克服这些挑战后，我们已经为实时生产流量提供了近一年的服务，并且执行负载测试显示，在包含故障转移流量的缓冲区后，至少有 200% 的空间。我们的 p99 读取延迟约为 1 秒：令人印象深刻，因为我们的一些上游查询可以命中超过 2,000 个段，每个段消耗大约 90 MB 的磁盘空间。在迭代我们的解决方案后，我们开始发布将之前的下游与我们的解决方案进行比较的指标。我们基于 Apache Pinot™ 的解决方案提供了几乎 1:1 的精度，让我们有信心信赖它。我们首先使用配置标志来控制更改，一段时间后，完全切换并弃用以前的实现。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;通过一些简单的补充，我们有信心能够回答更强大的问题。例如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;最近 50 次旅行中多次前往哪个城市？&lt;/li&gt;&lt;li&gt; Uber 的员工中谁是高级职位？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在工作粒度上找到时间点任期的成本很高。提高性能和降低存储成本的途径仍在探索中。虽然仍处于设计过程中，但我们预计通过降低作业粒度要求，我们应该能够显着提高读取吞吐量。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-acknowledgments"&gt;致谢&lt;/h3&gt;&lt;p&gt;特别感谢 Caner Balci、Qiaochu Liu、Jacob Sevart 和 Ujwala Tulshigiri 对本文的贡献。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; &lt;em&gt;Apache &lt;sup&gt;®&lt;/sup&gt; 、Apache Hive™、Apache Kafka®、Apache Spark™ 和 Apache Pinot™ 是&lt;/em&gt;&lt;a href="http://www.apache.org/" rel="noreferrer noopener nofollow" target="_blank"&gt;&lt;em&gt;Apache Software Foundation&lt;/em&gt;&lt;/a&gt;&lt;em&gt;在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/em&gt;&lt;/p&gt;&lt;p class="has-small-font-size"&gt;封面照片归属： &lt;a href="https://www.flickr.com/photos/54966739@N00"&gt;blaahhi&lt;/a&gt;的“ &lt;a href="https://www.flickr.com/photos/54966739@N00/3597105175"&gt;Abacus&lt;/a&gt; ”已获得&lt;a href="https://creativecommons.org/licenses/by/2.0/?ref=openverse"&gt;CC BY 2.0&lt;/a&gt;许可。&lt;/p&gt;</description><pubDate>Wed, 22 May 2024 06:41:19 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/job-counting-at-scale/</guid></item><item><title>【Upgrading M3DB from v1.1 to v1.5】将 M3DB 从 v1.1 升级到 v1.5</title><link>https://www.uber.com/blog/upgrading-m3db/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;&lt;a href="https://m3db.io/" rel="noreferrer noopener" target="_blank"&gt;M3DB&lt;/a&gt;是一个可扩展的&lt;a href="https://github.com/m3db/m3" rel="noreferrer noopener" target="_blank"&gt;开源&lt;/a&gt;分布式时间序列数据存储。它用作&lt;a href="https://www.uber.com/en-IN/blog/m3/" rel="noreferrer noopener" target="_blank"&gt;M3 堆栈&lt;/a&gt;的持久存储，为所有 Uber 服务提供基于指标的可观察性。这些指标随后用于生成实时警报。&lt;/p&gt;&lt;p&gt;距离我们上次将 M3DB 的开源版本部署到 Uber 云中已经过去了 3 年。自上次部署以来，服务器代码中添加了一系列性能增强功能。其中一些增强功能包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; ( &lt;a href="https://github.com/m3db/m3/releases/tag/v1.2.0" rel="noreferrer noopener" target="_blank"&gt;V1.2.0&lt;/a&gt; ) 和 ( &lt;a href="https://github.com/m3db/m3/releases/tag/v1.5.0" rel="noreferrer noopener" target="_blank"&gt;V1.5.0&lt;/a&gt; ) 中完成的&lt;strong&gt;资源利用率改进&lt;/strong&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Bootstrap 性能改进 –&lt;/strong&gt;改进&lt;strong&gt;&amp;nbsp;&lt;/strong&gt; ( &lt;a href="https://github.com/m3db/m3/releases/tag/v1.2.0" rel="noreferrer noopener" target="_blank"&gt;V1.2.0&lt;/a&gt; )&lt;/li&gt;&lt;li&gt;&lt;strong&gt;引导和快照可靠性 –&lt;/strong&gt; ( &lt;a href="https://github.com/m3db/m3/releases/tag/v1.4.0" rel="noreferrer noopener" target="_blank"&gt;V1.4.0&lt;/a&gt; ) 中进行的改进&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-scope-of-work"&gt;工作范围&lt;/h2&gt;&lt;p&gt;尽管开源项目引入了重大改进，但我们一直在旧版本的 M3DB 上运行的原因之一是，我们没有经过充分测试的 M3DB 验证和部署流程。&lt;/p&gt;&lt;p&gt;这项工作的主要目标之一是找到一种可重复且可靠的方法将 M3DB 部署到 Uber 云中，以便我们能够更一致地进行升级。我们希望自动化尽可能多的步骤，同时为升级 M3DB 队列的下一次迭代提出一系列改进。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-scale-amp-performance-considerations"&gt;&lt;strong&gt;规模和性能考虑因素&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;开源项目中有一个集成套件的基本版本，但不适用于 Uber 规模（10 亿写入 RPS、8000 万读取 RPS 和大约 6,000 台服务器）。&lt;/p&gt;&lt;p&gt;考虑到 M3DB 在 Uber 内部的运营规模，即使性能/效率/可靠性下降 5-10% 也可能对我们的可观察性平台构成严重风险。除非在生产类似规模下进行测试，否则评估 M3DB 新版本的正确性是没有意义的。&lt;/p&gt;&lt;p&gt;此外，在大规模运行时，常见的场景之一是 M3DB 集群处于动态状态，即节点经常且几乎总是由底层基础设施自动化触发添加、删除或替换。因此，集成套件也应该在这些混乱的场景下测试功能、正确性和性能。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-quantifying-concrete-gains-for-uber-workloads"&gt;&lt;strong&gt;量化 Uber 工作负载的具体收益&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;开源版本中的更改已经显示出性能改进，但从未在 Uber 这样庞大和复杂的环境中进行过测试。没有可用的数据来表明我们通过迁移到最新工作负载会看到的确切收益。考虑到在 M3DB 中审查和发布新版本所涉及的工作，我们需要证明进行这项工作的投资回报率是合理的。&lt;/p&gt;&lt;p&gt;因此，新版本测试工作的一部分将包括在 Uber 生产类似流量下运行它，并将关键性能指标与生产中运行的现有版本进行比较，以了解开源版本与 Uber 规模相比如何。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-rollout-challenges"&gt;&lt;strong&gt;推出挑战&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;考虑到集群规模巨大（大约100个集群，总共5500个节点）以及每个节点的高分配（目前每个节点28核和230G内存），M3DB的任何变化的推出也会在可靠性和可操作性方面带来挑战。我们希望评估一个安全、稳定且需要最少人力或干预的发布过程。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-rollout-strategy"&gt;推出策略&lt;/h2&gt;&lt;p&gt;我们有超过数百个 M3DB 集群在生产中运行 - 按保留期和可用区域进行分片。每个集群可以有数百个节点，其中绝大多数平均每个集群有 100 个节点左右。虽然集群本身可以并行升级，但在集群内，我们有一个额外的限制，即一次只能升级一个节点，以避免数据丢失，因为写入是在 Uber 内以三分之二多数仲裁执行的。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-testing-strategy"&gt;测试策略&lt;/h2&gt;&lt;p&gt;我们考虑并实施了两项广泛的测试策略：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;离线模式&lt;/strong&gt;，我们在各种模拟/实验综合测试负载和关键客户端性能指标（如正确性、延迟、成功率）的基准下运行 M3DB。此外，我们还对服务器性能特征进行基准测试，例如服务器响应时间、引导持续时间、滴答持续时间以及重新启动和替换之间的数据持久性。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;在线模式，&lt;/strong&gt;我们在重复和采样的生产流量下运行 M3DB，以观察现有生产集群的性能和正确性回归。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-offline-mode"&gt;离线模式&lt;/h3&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/XOs_e9tRumTWl2neEx1-qOK17aHObrN3xrkVXsbzqNbLNkWbyeoo9Wfs9_Ia8s7F8Q_MIs-aieAxmUiZ9oo9F2bNSRV0ZEfvS_7ZD0SCMqUhAl20JuSmCgPAXM2DgNGyPpg8ZLBlwT8puQv9SgPGLlc" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：离线测试策略。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;对于离线模式，我们需要一个能够大规模生成负载的 M3DB 基准测试工具。通过调整基准工具的输入参数或根据需要调整集群设置来模拟需要测试集群的各种场景。为了扩展我们的基准测试工具以模仿甚至超越我们的生产负载，我们将基准测试工具服务部署为分布式服务。我们部署了功能强大的高容量机器，使系统承受巨大的工作负载。所有基准测试均在新创建的 M3DB 集群上运行，集群大小为 10 个节点。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-test-scenarios"&gt;测试场景&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;在旧版本上对 M3DB 集群进行基准测试&lt;/strong&gt;– 我们在旧版本上运行多个基准测试，并记录 M3DB 集群的延迟和正确性，以建立基线数字。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;运行拓扑操作 –&lt;/strong&gt;在集群内发生拓扑操作（节点添加、删除和替换）时运行基准测试套件。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;推出新版本 –&lt;/strong&gt;当我们在新版本上进行部署时运行基准套件。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对新版本进行基准测试 –&lt;/strong&gt;与上述旧版本类似，我们也对新版本运行完全相同的基准测试，并收集相同的指标。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;基准测试正确性 –&lt;/strong&gt;首先在只写模式下运行基准测试，并将写入的指标记录在文件中 – 随后在验证模式下运行基准测试，一旦新版本的部署完成，它将尝试进行读取。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;当集群位于较旧版本的 M3DB 上时，上面的&lt;strong&gt;“a”&lt;/strong&gt;和&lt;strong&gt;“b”&lt;/strong&gt;点都会执行一次，当集群位于较新版本上时，也会执行一次。&lt;/p&gt;&lt;p&gt;当在传输中测试场景中进行测试时，需要上面的&lt;strong&gt;“c”&lt;/strong&gt;和&lt;strong&gt;“d”&lt;/strong&gt;点。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;“e”&lt;/strong&gt;点讲的是正确性。正确性是一个基本要求：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;当升级发生时，旧服务器会将所有数据刷新到磁盘，新服务器版本将开始读取相同的数据。我们需要确保新节点能够读取旧版本刷新的数据。&lt;/li&gt;&lt;li&gt;我们需要在新版本将数据保存在缓冲状态和刷新状态的情况下进行正确性基准测试。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-online-testing"&gt;在线测试&lt;/h3&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/wtVD6vLneTmdQJjoTCOLl9E9QxlTzW99f22X06IK8oHL3AEB9Vxgi059Uh16_n4lE91SZvm0vrITtr7k4kqsiLFD71UiOcHCfkoJohK37_981kRdDdr_cvtqnMHqeh1nr2NHqN8RC0nzEyO6Pfa3swg" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：在线测试策略。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;虽然离线模式帮助我们在 M3DB 的一些关键性能指标上探索 M3DB 新版本，但它仍然没有让我们对新版本在 Uber 生产规模和指标有效负载方面的表现缺乏信心。&lt;/p&gt;&lt;p&gt;为了获得这种信心，我们最终实现了如下所示的影子流量管道设置，其中我们可以将可配置的写入和读取流量示例复制到具有生产流量的影子 M3DB 集群，尽管规模较小。&lt;/p&gt;&lt;p&gt;考虑两种类型的影子测试：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;影子测试用于比较运行相同百分比生产流量的旧版本和新版本&lt;/li&gt;&lt;li&gt;影子测试具有与生产集群成比例的流量和集群大小并进行比较&lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-shadow-testing-against-older-version-vs-newer-version-nbsp"&gt;针对旧版本与新版本的影子测试&lt;/h4&gt;&lt;p&gt;我们首先使用旧版本创建影子 M3DB 集群，并让它运行几周，以便我们获得各种性能指标（如 CPU、内存、引导延迟、滴答持续时间和服务器/客户端延迟）的基线数字。之后影子M3DB集群升级到最新版本，测量相同的性能指标并与基线数字进行比较。&lt;/p&gt;&lt;p&gt;在线测试分为3个阶段：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在旧版本的 M3DB 中运行影子集群以建立基线&lt;/li&gt;&lt;li&gt;在影子设置中推出较新版本的 M3DB&lt;ol&gt;&lt;li&gt;测量推出所需的时间&lt;/li&gt;&lt;li&gt;观察读/写失败&lt;/li&gt;&lt;li&gt;观察性能指标是否有变化&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;在较新版本的 M3DB 中运行影子集群以建立较新的性能指标并与基线数字进行比较&lt;/li&gt;&lt;li&gt;新的性能指标可以帮助量化我们的生产设置通过迁移到新版本可以获得的改进&lt;ol&gt;&lt;li&gt;验证在推出之前发送的写入是否在推出后保留&lt;/li&gt;&lt;li&gt;验证在部署之前发送的写入是否保留以进行节点替换&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-shadow-testing-against-production-cluster"&gt;针对生产集群的影子测试&lt;/h4&gt;&lt;p&gt;根据影子流量将影子集群大小减小到与生产相当的大小，并将性能与生产数量进行比较。&lt;/p&gt;&lt;p&gt;我们希望在大小成比例的集群上发生成比例的读写操作。在此测试设置中，我们还需要对阴影和生产设置运行拓扑操作以获取比较详细信息。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-key-performance-indicators"&gt;关键绩效指标&lt;/h3&gt;&lt;p&gt;我们需要在两种测试策略中观察以下指标：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;系统指标：&lt;/strong&gt; CPU、RSS 内存、总内存、磁盘使用情况、文件描述符以及 M3DB 进程生成的 goroutine 数量。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;延迟数字：&lt;/strong&gt;客户端和服务器端读/写延迟。&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;成功/失败：&lt;/strong&gt;仲裁成功与失败的读/写请求。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;节点/对等引导时间：&lt;/strong&gt;每当系统中出现新节点时，都会为其分配系统中可用分片的子集。现在，该节点尝试从其他对等点引导数据，其中也包含这些分片的副本数据。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;缓冲写入正确性：&lt;/strong&gt;每当数据写入 M3DB 时，数据都会根据命名空间配置（特别是配置的&lt;strong&gt;块大小&lt;/strong&gt;）保存在 M3DB 的内存缓冲区中。如果对 M3DB 的写入成功，我们应该能够在将数据刷新到磁盘之前从 M3DB 查询回相同的数据。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;Flushed Write Correctness：&lt;/strong&gt;与缓冲写入正确性类似，当块刷新到磁盘时，写入在刷新到磁盘后应该是可验证的。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-gains-observed-in-testing-nbsp"&gt;&lt;strong&gt;测试中观察到的收益&lt;/strong&gt;&lt;/h3&gt;&lt;h4 class="wp-block-heading" id="h-memory-improvements-nbsp"&gt;记忆力改善&lt;/h4&gt;&lt;p&gt;我们观察到新版本中 M3DB 的内存利用率提高了大约 20%。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/iw6yy3taG-gCKtsZFmnAx5rhVFCO2l1sRMlZsSjsJIXzzOngdzy8XuBhQhmOOkhAReRlgyE_CuF7qSfihkItMFvDk34engOYipE0-W_OOvp8FgWEt0f3Rz7TJxT1JFhtAA23xzEhcgB035Bd46SDvSA" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：内存改进。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-cpu-utilization-improvements-nbsp"&gt; CPU 利用率改进&lt;/h4&gt;&lt;p&gt;同样，我们观察到新版本中 M3DB 的内存利用率提高了大约 20%。 &lt;/p&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/UF5RX9fSgWn3XvjJU9x58KP-8V7B_PW3C47ZNSVqaPuBPQnd1tyeNaVyqIE6S7WPvnVs9lodzDpt4fKfPZzMgE5fO65Gt8kwnu1UJsFno3sxToEjt_WVsefGOHGH622EANAnRXbSzLeEhKJzs8G8sYs" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：CPU 利用率改进。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-node-bootstrap-time-improvements"&gt;节点引导时间改进&lt;/h4&gt;&lt;p&gt;每当将新节点添加到集群的拓扑中时，就会发生&lt;a href="https://m3db.io/docs/operational_guide/bootstrapping_crash_recovery/" rel="noreferrer noopener" target="_blank"&gt;节点引导&lt;/a&gt;，这可能是在替换旧节点或添加新节点时。在 M3DB 中，当旧节点进行替换时引导新节点是一个长时间运行的操作。&lt;br /&gt;我们在新版本的 M3DB 中一致地引导时看到了更好的性能。这个测试进行了很多轮，结果总是新版本更好。&lt;/p&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;节点大小&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;旧版本（分钟）&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;新版本（分钟）&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;改进％&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 80GiB&lt;/td&gt;&lt;td&gt; 77&lt;/td&gt;&lt;td&gt; 62&lt;/td&gt;&lt;td&gt; 19.48%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 224GiB&lt;/td&gt;&lt;td&gt; 218&lt;/td&gt;&lt;td&gt; 170&lt;/td&gt;&lt;td&gt; 22.00%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 518吉B&lt;/td&gt;&lt;td&gt;第317章&lt;/td&gt;&lt;td&gt;230&lt;/td&gt;&lt;td&gt; 27%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; 740吉布&lt;/td&gt;&lt;td&gt;第424章&lt;/td&gt;&lt;td&gt;365&lt;/td&gt;&lt;td&gt; 14%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;更换引导时间改进%&lt;/strong&gt; &lt;/figcaption&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-production-rollout-nbsp"&gt;&lt;strong&gt;生产推广&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;当所有测试完成后，我们开始分阶段部署到生产集群。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-challenges-in-production-rollout"&gt;生产推广中的挑战&lt;/h3&gt;&lt;p&gt;即使经过彻底的基准测试和生产样本流量测试，我们在部署的集群中也遇到了一些挑战。下面，我们概述了这些问题，并解释了为什么它们在基准测试过程中难以捉摸，并详细介绍了为解决这些问题而实施的最终解决方案。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-tail-latency-spikes-nbsp"&gt;尾部延迟峰值&lt;/h4&gt;&lt;p&gt;部署后不久，我们注意到写入操作的尾部延迟显着增加，特别是在 P85 百分位数及以上。&lt;/p&gt;&lt;p&gt;为了确定问题的根本原因，我们将集群中的多个节点恢复到以前的版本，同时在其余节点上维护较新的版本。随后，我们对两个版本之间的CPU&lt;a href="https://go.dev/blog/pprof" rel="noreferrer noopener" target="_blank"&gt;性能分析&lt;/a&gt;进行了比较。然后我们能够将其隔离为一个简单的方法，这是有问题的。然后我们解决了导致回归的问题，之后延迟也下降了。&lt;/p&gt;&lt;p&gt;以下是 cpu 分析练习中的一些图像： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-full is-resized"&gt;&lt;img alt="" class="wp-image-1088725" height="591" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/Figure-5_-CPU-profile-before-fix.png" style="width: 700px; height: auto;" width="1581" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：修复前的 CPU 配置文件。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/7TqEH_gvJJTSI6-dL2-Nk_Ls1BhN_msDWj10PVCAlzucvwh9LPUPAi-D8M7b1JSU_4vNjndcz47l0omcoiKExxYlLUHy-OM6eStBGCFU9NtncZJ3AB417N7J9_ZJXdWvrR6rq74e2WJ7U91bnUxZa0o" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：修复后的 CPU 配置文件。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;从上图中我们可以看出，有问题的方法在修复后显着减少了其占用空间。&lt;/p&gt;&lt;p&gt;当我们推出这个新版本时，我们的客户端延迟再次恢复正常，如下所示： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/hc4ldFNdM05ZalywrXAWlsLHvHVSLq84hw3hCXoQHEDrCtw9BgXV9k5w-V7leY6FnjBDg_k5sXyPb-WEsX9CP_hS5EweJT1IbFcC_hbn_DTbq2ZlkSUpUCUBTaisqp-GhF6HuDxO3FAuZaJSX-93OaQ" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 7：修复前后的 P99 写入延迟。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;我们将此修复贡献回上游开源存储库 - 更改的详细信息可以在： &lt;a href="https://github.com/m3db/m3/pull/4253" rel="noreferrer noopener" target="_blank"&gt;Github Pull Request#4253&lt;/a&gt;中找到。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-data-loss-during-the-upgrade"&gt;升级期间数据丢失&lt;/h4&gt;&lt;p&gt;在上线过程中，集群中的所有节点都会依次进行版本升级。这涉及关闭现有节点并将其在新版本中恢复。因此，节点会丢失其在内存中保存的数据，并且还会错过在其关闭的时间窗口内传入的写入操作。该节点尝试通过&lt;a href="https://m3db.io/docs/operational_guide/bootstrapping_crash_recovery/" rel="noreferrer noopener" target="_blank"&gt;引导&lt;/a&gt;并从对等节点获取数据来恢复该数据。&lt;/p&gt;&lt;p&gt;在集群中发布后，我们的客户发现他们在读取法定人数方面遇到了问题。这基本上意味着客户端观察到读取数量增加，其中集群中只有一个副本有数据，而其他两个副本没有任何数据。&lt;/p&gt;&lt;p&gt;我们从许多查询中分离出单个查询，然后我们能够检查分片的各个副本，并能够发现数据丢失，并且丢失数据的时间范围与节点升级的时间相匹配。我们观察到一个副本有数据，而另外两个副本缺少数据。然后，我们将其与以下事实关联起来：拥有更多数据的副本能够在其他两个副本升级之前将数据刷新到磁盘。刷新取决于&lt;a href="https://m3db.io/docs/operational_guide/namespace_configuration/" rel="noreferrer noopener" target="_blank"&gt;命名空间&lt;/a&gt;配置，因此如果命名空间配置规定刷新应每 24 小时发生一次，则所有节点将几乎同时经历刷新周期。&lt;/p&gt;&lt;p&gt;这个问题最终被证明是一个配置错误的问题，我们必须为数据库指定正确的引导程序模式，只有在这之后它才会尝试引导在从对等点升级期间丢失的数据。我们使用默认的引导配置，但理想情况下我们应该指定模式为“prefer_peers”或“exclude_commitlog”，前者意味着在从提交日志引导之前优先考虑对等点，后者意味着完全排除提交日志引导。我们继续使用后者，因为我们还没有在生产工作负载中真正使用&lt;a href="https://en.wikipedia.org/wiki/Write-ahead_logging" rel="noreferrer noopener" target="_blank"&gt;WAL&lt;/a&gt; 。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-gains-observed-in-production-after-rollout"&gt;推出后生产中观察到的收益&lt;/h3&gt;&lt;p&gt;生产服务器受内存限制而不是 CPU 限制，因此我们能够观察到内存使用量显着下降，而不是 CPU 使用量。下图也可以看到同样的情况： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/SWwyAQHUM00RoVQMX2WCnTYPA_IX9XdO80xLtOdbH0DqfqUHhXwzFZtVcXK0v3KdSC2w2Zl3Na-GMAT-WrICFiSFJcN5lJBef-g990kq17tV066c1HmH3FFa42tqJOpZp6iGpEXWxnWVdRfKoJgoSLU" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 8：生产中的内存 RSS 改进。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;节点引导时间的改进也与我们在离线测试阶段的发现一致，并帮助我们提高了生产集群的稳定性。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;h2 class="wp-block-heading" id="h-plans-for-contribution-back-to-open-source-project"&gt;回馈开源项目的计划&lt;/h2&gt;&lt;p&gt;我们希望为该项目做出一些贡献：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基准测试实用程序有助于在生产规模上对新版本进行基准测试。&lt;/li&gt;&lt;li&gt;文档中对数据丢失事件（以及如何解决该问题）的描述。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在生产环境中升级高QPS、低延迟时间序列数据库的旅程，尤其是像Uber这样规模的公司，无疑充满了无数的挑战和挣扎。我们的一些主要经验包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;升级应该是一个持续的过程，而不是一次性完成。&lt;/li&gt;&lt;li&gt;处理开源软件总是充满挑战，因此我们应该有办法：&lt;ul&gt;&lt;li&gt;大规模评估正确性/性能。&lt;/li&gt;&lt;li&gt;制定标准的交错推出和回滚机制。&lt;/li&gt;&lt;li&gt;采用数据驱动的方法来验证与两个发行版本之间的比较有关的每个假设/声明。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从解决复杂的技术问题到管理性能优化和维持正常运行时间之间的微妙平衡，每个障碍都提供了成长和学习的机会。当工程师正面应对这些挑战时，他们对系统架构、可扩展性和弹性有了更深入的了解。&lt;/p&gt;&lt;p&gt;此外，升级此类关键基础设施的过程促进了工程团队内部的创新和协作文化。通过团队合作和集体解决问题，工程师甚至可以克服最艰巨的障碍，最终形成更强大、更高效的系统。&lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p class="has-small-font-size"&gt;封面照片归属： &lt;a href="https://www.flickr.com/photos/61846758@N02"&gt;ChrisA1995&lt;/a&gt;的“ &lt;a href="https://www.flickr.com/photos/61846758@N02/6331318875"&gt;Nature&lt;/a&gt; ”已获得&lt;a href="https://creativecommons.org/licenses/by/2.0/?ref=openverse"&gt;CC BY 2.0&lt;/a&gt;许可。&lt;/p&gt;</description><pubDate>Thu, 16 May 2024 09:18:05 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/upgrading-m3db/</guid></item><item><title>【DataK9: Auto-categorizing an exabyte of data at field level through AI/ML】DataK9：通过 AI/ML 在现场级别自动分类 EB 数据</title><link>https://www.uber.com/blog/auto-categorizing-data-through-ai-ml/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;数据分类——根据数据的特征和本质对数据进行分类的过程——是任何隐私或安全计划的基本支柱。细粒度数据分类的有效性对于实施隐私和安全控制（例如访问策略和加密）以及管理数据资产的生命周期（包括保留和删除）至关重要。本博客深入探讨了 Uber 通过利用各种 AI/ML 技术实现大规模数据分类的方法。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-why-auto-categorization"&gt;为什么要自动分类？&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;庞大的规模和成本&lt;/strong&gt;：许多公司管理分布在各种存储系统中的大量数据集。新数据集的定期生成进一步加剧了这一规模。我们面临的挑战的核心是在现场级别标记大量列，每个列可能需要多个标签。手动标记需要大量时间和资源。此外，需要持续投资来标记新创建的数据集。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据所有者的参与&lt;/strong&gt;：识别数据所有者并让其参与是一个复杂的现实。由于每个数据元素需要评估多个标签，因此辨别标签定义之间的细微差别变得复杂且耗时，从而导致错误分类。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑到巨大的工程成本以及通过分散的努力进行手动分类的不切实际，我们的经验使我们优先考虑自动分类。为此，我们推出了一种名为&lt;strong&gt;DataK9&lt;/strong&gt;的新颖解决方案，这是 Uber 数据的自动分类平台。主要目标是最大限度地减少和消除用户参与，从而解决规模、成本和数据所有者参与带来的挑战。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-challenges"&gt;挑战&lt;/h2&gt;&lt;p&gt;考虑一个包含纯数字内容的单列表。仅检查数据本身无法辨别这些数字的性质，这些数字可能代表纬度、经度、汇率等。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1088500" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/Screenshot-2024-05-06-at-2.53.45%E2%80%AFPM-1024x290.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：分类中的列名称。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;但是，如果我们了解列名称（例如图 1 中的&lt;em&gt;纬度&lt;/em&gt;和&lt;em&gt;经度）&lt;/em&gt; ，它将极大地有助于准确地对这些列进行分类。&lt;/p&gt;&lt;p&gt;然而，挑战仍然存在，特别是当列名称是通用的时，如图 2 中最近添加的&lt;em&gt;“注释”&lt;/em&gt;列所示。这可能包含从余额和成本到两个位置之间的距离等信息。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1088501" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/Screenshot-2024-05-06-at-2.54.02%E2%80%AFPM-1024x345.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：通用列名称。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;此外，即使在同一类型的位置数据中，分类也可以根据精度进行细微差别。例如，纬度和经度指定为三位或更多小数位的位置可以被视为精确位置。如果与个人身份相关联，则可能会被归类为高度限制类别。&lt;/p&gt;&lt;p&gt;让我们深入研究图 3 中的另一个示例： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1088502" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/05/Screenshot-2024-05-06-at-2.54.21%E2%80%AFPM-1024x240.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：地址列名称。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;在缺乏明确上下文的情况下，区分&lt;em&gt;个人&lt;/em&gt;地址和&lt;em&gt;企业&lt;/em&gt;地址变得很困难，尽管两者都是地址。&lt;/p&gt;&lt;p&gt;鉴于这些复杂性，没有通用的解决方案可以解决所有分类挑战。因此，我们选择利用基于概率的方法（即人工智能/机器学习）来为此努力提供帮助。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-strategy"&gt;战略&lt;/h2&gt;&lt;p&gt;人工智能/机器学习技术的应用构成了我们自动分类计划的基石。在本节中，我们将提出总体策略，描述新框架内各个系统之间更广泛的流程和依赖关系。&lt;/p&gt;&lt;p&gt;评估新自动化流程的有效性至关重要，并且取决于一小部分（&amp;lt;1%）的标记（黄金）数据集。这些数据集最好由领域专家或所有者手动分类。然后，自动分类机制利用经过审查的训练数据来标记剩余的数据集 (&amp;gt;99%)。本质上，该解决方案采用混合方法，将手动分类与自动化相结合，封装在三个相互关联的阶段中，如下图 4 所示。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/wQRv72HPpi7ZE6eo9u0s6UjgYz7wwLExhcEkpdLdHwRZhw1xz1njMh661wIfwvhFGHOw1P9Ac4GLuXKowp-gbqa_-BaQKdwL2LOFHgxAOinjYHauzsEJ-AYPW3uvSEJuTBbsWSUB8RaFYgwTZBkyoLM" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：分类策略。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-building-baseline"&gt;建立基线&lt;/h2&gt;&lt;p&gt;我们建议将 &amp;lt;1% 的数据集（近 1,000 个）分类为由人类在隐私和领域专家的密切监督下关键且高度利用的表。这些数据集被视为“黄金”数据集，具有很高的分类准确性，并用于衡量自动化准确性。虽然“黄金”数据集的数量相对较少，但有一些显着的好处：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;模型训练&lt;/strong&gt;：训练是任何基于监督机器学习的解决方案中最关键的步骤。如果没有经过适当训练的模型，机器甚至不知道首先要理解什么。我们利用手动分类的数据作为人工智能方法的基线。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;风险缓解&lt;/strong&gt;：我们有一些关键业务数据集，我们不想因为错误标记而冒这些风险。这就是为什么我们由所有者或专家手动标记那些具有高度影响力的数据集。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;理想情况下，用于标记的数据集的选择应随机进行，以尽可能避免在标记数据集中插入统计偏差。这种统计偏差降低了离线模型性能测试的可靠性，并可能导致生产中的系统性分类错误。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-training"&gt;训练&lt;/h2&gt;&lt;p&gt;对于其余 99% 的数据集（&amp;gt;400K），我们将主要使用名为 DataK9 的新自动化系统（稍后小节将详细介绍）。首先，DataK9 针对手动分类（黄金）数据集进行训练，直到准确性达到令人满意的水平而不会过度调整。具体来说，将有两个子阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们基于黄金数据集的子集迭代训练模型，然后针对其余黄金数据集（测试数据）运行该模型。我们在每次测试运行后分析标准指标（即准确性、精确度、召回率、F2 分数等）。如果指标不可接受，我们会调整模型并重新运行/重新评估它，直到获得满意的结果。&lt;/li&gt;&lt;li&gt;用于标记数据的检测规则列表由领域专家创建。定义规则后，它会不断进行测试运行和规则调整，直到达到可接受的指标值。最初的规则定义和早期的规则迭代将有更多的人为参与。我们将逐步进入规则的自动调整，使调整过程自动化，并推出规则更改，从而产生更好的预测。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-serving-in-production"&gt;服务于生产&lt;/h2&gt;&lt;p&gt;在 Datak9 展示了其令人满意的训练数据集指标（即准确率 &amp;gt;90%、F2 分数 &amp;gt;85%）后，它将准备好对其余数据集进行生产标记。不过，我们计划在大规模分类的早期阶段采取一些防御流程。它包括在数据集（暂时）自动分类后自动创建票证以进行审查（对于数据所有者或经过培训的隐私专家来说更佳）。当所有者盖章后，分类将被视为最终分类。此外，在这个过渡阶段，我们将密切监控进展情况，尤其是错误分类率。如果审稿人的反馈可以接受，我们将大规模部署数十万个数据集。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-architecture"&gt;建筑学&lt;/h1&gt;&lt;p&gt;本节介绍基于 AI 的核心平台的架构，以及 DataK9 如何在框架的不同部分和阶段采用 ML/AI 技术，如图 5 所示。本节我们重点关注标记数据集的主要用例。首先识别和收集数据集的基本特征，然后使用 ML/AI 技术确定特定标签的列权重/分数。然后，我们讨论如何将特定领域的多个信号组合成最终决策。最后，我们详细介绍了根据原始标记中所犯错误调整规则和 ML 模型的主动学习过程。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/d8h-crQtz9vDMKFyrGRuALVxL8rWsE4srz6RSopnKAU0rfOwdIrIa9X_pY-XR7a4qi6BGiKx1R7UemrVXIwahIopFhKNv6a9SwrclnWzCLVraWsriXdXEjtKz2KmJxLkxEwy_1BFkrCZMPMdvTNcEaQ" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：Data K9 架构。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-key-features-nbsp"&gt;主要特征&lt;/h2&gt;&lt;p&gt;DataK9 使用数据集的以下信息，我们可以将其用作各种 ML/AI 分类技术中的潜在特征：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;元数据&lt;/strong&gt;：它包括数据集中每个字段的名称和类型。例如，数据集“ &lt;em&gt;uber_rides&lt;/em&gt; ”可能有 100 多个字段。 DataK9 利用所有字段的名称，例如“ &lt;em&gt;request_latitude&lt;/em&gt; ”和“ &lt;em&gt;email&lt;/em&gt; ”以及相关的类型，例如&lt;em&gt;double&lt;/em&gt;和&lt;em&gt;string&lt;/em&gt; 。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据&lt;/strong&gt;：数据集可能包含数十亿条记录。尽管 DataK9 最终会考虑所有行的内容，但我们将从数据集每次运行/扫描的记录随机样本 (~1%) 开始。更具体地说，我们将利用单元格内容来确定列类别。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;上下文&lt;/strong&gt;：虽然每个单元格值都提供了有价值的信息，但同一记录中的其他值可能会提供额外的信号。此外，上下文（例如表和数据库名称）会呈现有关列标记的提示。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;沿袭&lt;/strong&gt;：数据沿袭是数据旅程的地图，包括其起源、创建过程等。与 DataK9 相关的数据沿袭信息有两种类型。&lt;ul&gt;&lt;li&gt;&lt;em&gt;表级沿袭&lt;/em&gt;：表沿袭将提供更高级别的信息，例如使用哪些表来创建新表。例如，表“ &lt;em&gt;uber_rides&lt;/em&gt; ”是在连接一些表（例如&lt;em&gt;drivers&lt;/em&gt;和&lt;em&gt;riders ）&lt;/em&gt;后创建的。这种类型的沿袭与派生表比原始表更相关。&lt;/li&gt;&lt;li&gt;&lt;em&gt;列级沿袭&lt;/em&gt;：此沿袭是指哪一列派生自不同表的哪一组依赖列。对于各种数据处理平台（例如 Hive™、Spark™ 等）来说，收集这些信息会更加复杂。但是，如果有可靠的列级谱系可用，DataK9 可以更好地预测标签。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-matching-strategy"&gt;匹配策略&lt;/h2&gt;&lt;p&gt;所提出的解决方案的本质是最终从所有数据存储中抓取数据（表或目录或文件）并定期扫描每个数据元素以获取不同标签的信号。我们在本节中讨论一些基本的匹配技术。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;个体与总体决策&lt;/strong&gt;：DataK9 在单元格级别扫描数据，并从内容和元数据中查找信号。由于有数百万个数据元素，DataK9 将检查每个元素并获取相应的指示。例如，假设单元格值为&lt;em&gt;john@gmail.com&lt;/em&gt; 。我们会将此电子邮件地址与每个标签进行匹配，并生成该值的匹配分数。然而，根据一个匹配分数做出最终决定可能会产生误导。实际上，电子邮件地址可能位于另一列（例如&lt;em&gt;Promotion_code&lt;/em&gt; ）中，这可能不会被解释为 PII 数据。此外，我们的主要目标是标记表的列，而不是单个列值。因此，我们将所有列值的这些单独分数合并为全局分数并最终做出决定。例如，如果列的 80% 的值与电子邮件匹配，我们可以安全地假设该列包含电子邮件 PII 数据并相应地对其进行标记。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;概率决策：&lt;/strong&gt; DataK9 寻找特定标签的特定匹配。然而，几乎没有找到匹配就能确保标签分配的情况。因此，我们为每项检查定义一个分数。分数越高，标签关联的可能性就越高。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;负分：&lt;/strong&gt; DataK9 允许负分来表明它不太可能是特定标签的一部分。例如，数据库名称模式“ &lt;em&gt;products&lt;/em&gt; ”下的任何数据不太可能具有任何 PII 数据。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-rule-based-ai"&gt;基于规则的人工智能&lt;/h2&gt;&lt;p&gt;当我们抓取新的数据集时，我们对样本数据应用两种人工智能方法，然后识别每个字段/列类别。我们在本节中解释&lt;em&gt;基于规则的内容&lt;/em&gt;，然后在下一节中&lt;em&gt;解释基于学习的内容&lt;/em&gt;。&lt;/p&gt;&lt;p&gt;我们将从一组手工制定的规则开始，并构建相关的规则语言和引擎。找到具有适当知识和经验（例如对数据内容、分类内部结构和工程能力的深入理解）的领域专家具有挑战性。本质上，设计规则需要考虑两个关键维度：40 多个不同的标签和基本功能。我们支持以下构建块来表达规则：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;布隆过滤器匹配&lt;/strong&gt;：我们根据特定实体的最常见数据值创建&lt;a href="https://en.wikipedia.org/wiki/Bloom_filter" rel="noreferrer noopener" target="_blank"&gt;布隆过滤器&lt;/a&gt;。然后在扫描每个列值的过程中，我们测试成员资格并决定可能的标签。例如，我们可以为 Uber 送货服务上个月使用的所有地址创建布隆过滤器。然后，在扫描过程中，我们可以测试每个数据单元格的成员资格以找到可能的匹配。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;字典匹配&lt;/strong&gt;：我们创建一个包含常用值列表的字典。在扫描过程中，K9为数据集的每个数据元素（单元格）查找字典，并根据规则定义决定匹配分数。例如，我们可以创建一个字典来包含 750 多个&lt;em&gt;机场代码&lt;/em&gt;来匹配位置信息。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;模式匹配&lt;/strong&gt;：我们定义一个正则表达式来查找与列内容或列名称的匹配。例如，我们可以使用模式&lt;em&gt;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,} $&lt;/em&gt;匹配电子邮件地址，例如&lt;em&gt;john@gmail.com&lt;/em&gt; 。需要注意的是，在某些情况下，模式匹配可能不足以确定最终标签。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;上下文匹配&lt;/strong&gt;：列名称和值本身可能无法提供有关其内容类型的任何强烈信号。因此，我们允许寻找单元格值本身之外的上下文。我们将支持三种不同的上下文：&lt;ul&gt;&lt;li&gt;&lt;em&gt;记录级别&lt;/em&gt;：基本原理是某些数据标签可能与同一数据集中的其他数据标签一起出现。例如，假设列中有一个纬度值。在这种情况下，它被认为是非敏感位置数据，因为相同的纬度可能存在数百万个位置点。因此，我们还应该在同一条记录中查找经度。&lt;/li&gt;&lt;li&gt;&lt;em&gt;表级别&lt;/em&gt;：在某些情况下，表名称模式在确定某些实例的数据类型方面发挥着重要作用。例如，“ &lt;em&gt;virtual_machine&lt;/em&gt; ”表中的全名不敏感，而“ &lt;em&gt;rider&lt;/em&gt; ”表（保存最终用户数据）则非常敏感。&lt;/li&gt;&lt;li&gt;&lt;em&gt;数据库级别&lt;/em&gt;：某些数据库名称模式可能指示数据库可能包含的数据类型。例如，如果数据库的名称中包含“ &lt;em&gt;finance”&lt;/em&gt; ，则它可能会包含一些交易数据。&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据类型匹配&lt;/strong&gt;：数据类型匹配包括类别的可能数据类型列表。如果任何字段不遵守标签的类型限制，K9 将跳过该标签的该列匹配。例如，个人电子邮件列的类型为&lt;em&gt;string&lt;/em&gt; ；因此，如果列的数据类型是非字符串，我们不需要检查电子邮件类别。这种类型的检查对每个文件或数据集进行一次的频率较低，因为它是在元数据级别。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-rule-language"&gt;规则语言&lt;/h3&gt;&lt;p&gt;定义规则是基于规则的人工智能的第一步，支持标准的规则定义语言或模板势在必行。本节介绍为特定标签定义规则的基本构建块。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;根据其检查的数据类型，规则中包含三个更广泛的子规则：列值匹配 ( &lt;em&gt;columnValueMatch&lt;/em&gt; )、元数据匹配 ( &lt;em&gt;columnNameMatch&lt;/em&gt; ) 和上下文匹配 ( &lt;em&gt;contextMatch&lt;/em&gt; )。&lt;/li&gt;&lt;li&gt;每个子规则都会有一个由规则管理员根据其领域专业知识指定的匹配“分数”。&lt;/li&gt;&lt;li&gt; &lt;em&gt;columnValueMatch&lt;/em&gt;规则主要检查数据集中每个数据元素的值。这些操作成本高昂且复杂，因为数据元素的数量和每个值的检查数量巨大。它需要以下过滤器和检查：&lt;ol&gt;&lt;li&gt;&lt;em&gt;布隆过滤器&lt;/em&gt;部分包含查找预先计算的布隆过滤器以检查成员资格和关联分数所需的配置。我们可以将布隆过滤器存储在文件或表中。&lt;/li&gt;&lt;li&gt;&lt;em&gt;字典&lt;/em&gt;匹配从最常用值的预定义列表中查找精确匹配。该字典可以是 HDFS 文件或 Hive 表。&lt;/li&gt;&lt;li&gt; &lt;em&gt;lengthRange&lt;/em&gt;定义内容的长度。此过滤器可以轻松地从昂贵的匹配中早期排除大量值。例如，纬度值的长度可以在3到10之间。K9不应考虑位置数据的该长度范围之外的任何内容。&lt;/li&gt;&lt;li&gt; &lt;em&gt;valueRange&lt;/em&gt;指定标签的有效值范围。例如，一个人的年龄可以在 0-125 之间。如果某列包含超出此范围的数据，K9 将跳过此列进行任何“年龄”类型检查。与 lengthRange 一样，valueRange 也简化了许多复杂的匹配。&lt;/li&gt;&lt;li&gt;&lt;em&gt;模式&lt;/em&gt;匹配指定与值的典型模式匹配的正则表达式。例如，电子邮件始终遵循一种模式，我们可以使用该模式来检查列值是否可能是潜在的电子邮件。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt; &lt;em&gt;columnNameMatch&lt;/em&gt;规则表示列名模式和列类型的匹配。它还可以提及排除的列名称和类型，以帮助避免误报。&lt;/li&gt;&lt;li&gt; &lt;em&gt;ContextMatch&lt;/em&gt;规则查找超出特定列值的特定上下文。目前，DataK9 支持两种不同的上下文以及各自的匹配分数：&lt;ol&gt;&lt;li&gt; &lt;em&gt;ResourceContext&lt;/em&gt;指定是否存在我们想要获取信号的任何表或数据库名称模式。例如，如果数据库名称包含“finance”，则该数据集中很有可能包含交易数据。&lt;/li&gt;&lt;li&gt; &lt;em&gt;categoryContext&lt;/em&gt;概述了哪些其他相关类别可以提供额外的信号。例如，1 级位置数据标签必须在同一数据集中具有可识别的个人类别。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt; DataK9 在一个或多个 YAML 文件中管理规则的定义。或者，DataK9 将支持存储在数据库表中的规则。该表更适合动态更新规则或相关分数。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们提供了一组“位置纬度”示例规则来识别图 6 中 YAML 格式的标签，这将有助于理解前面部分中描述的概念。绿色的内嵌注释包含了每个键值对的含义。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/4wEm-2So0frIQ5F3byMZNiANyt_bEf_njBgaz2TA3ckBrvkiG4Sn9YpCHNguU9mtB49UyXoU9D59hTEFzyQkjkS2trLvDVyhOmoUYZvOffEqGDmM3q0WrdRtiDzp8hpny5Rx3mMU0DYNtojTGBrjrmo" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：位置纬度分类配置。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-learning-based-ai"&gt;基于学习的人工智能&lt;/h2&gt;&lt;p&gt;分类在 AI/ML 领域得到了很好的研究。我们的分类问题非常适合多标签分类的监督学习。使用标准分类算法，我们打算使用元数据和单元格内容来预测实际标签。关键思想如图 7 所示，描述如下：&lt;/p&gt;&lt;p&gt;图 7：基于学习的人工智能&lt;/p&gt;&lt;ul&gt;&lt;li&gt;使用标记数据集来训练我们的模型。我们可以分别使用元数据和数据值来训练两个模型。&lt;/li&gt;&lt;li&gt;评估训练后的模型的典型准确度、精确度、召回率和 F2 得分指标。如果指标的值不可接受，我们会在调整参数和算法后重新训练我们的模型。否则，我们可以在生产中部署模型。如果模型评分分布一致，则可以采用阈值技术来权衡精度和召回率，并提供更好的控制。对于支持度较低的类别，可以通过额外的手动标记来改进模型。如果额外的手动标记失败，可以考虑合并类别。机器学习模型将接受完整的训练/测试实验，作为部署前的最终健全性检查。该实验还必须通过上述所有指标的既定阈值。 DataK9 检测器将根据训练模型预测列的标签。&lt;/li&gt;&lt;li&gt;机器学习算法和初始实验：大量机器学习算法以及各种输入转换和特征工程策略已经接受了测试。我们已经检查了经典的 ML 程序，例如线性支持向量机 (SVM)、K 最近邻 (KNN) 和朴素贝叶斯。到目前为止，做得最好的是线性 SVM。然而，机器学习建模仍有很艰巨的工作要做，特别是在这些低支持类别方面。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-aggregating-signals"&gt;聚合信号&lt;/h2&gt;&lt;p&gt;如上所述，DataK9 生成多个信号以及每列的相关分数值。一列可以有多个潜在标签以及通过以下方法估计的分数：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;基于规则的人工智能在为每个类别应用规则后创建标签和分数。&lt;/li&gt;&lt;li&gt;基于学习的人工智能将根据元数据和列值预测每列可能的标签对和分数。&lt;/li&gt;&lt;li&gt;谱系服务还可以为每列提供附加信号。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;收集这些信号后，DataK9 使用加权聚合方法合并每个标签、每列的所有分数。每个匹配的权重是根据经验确定的，并作为规则定义的一部分进行参数化。综合得分与预先指定的阈值进行比较后确定最终标签。该产品的后续版本可能会使用基于 ML 的集成方法。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-learning-feedback-loop"&gt;学习：反馈循环&lt;/h2&gt;&lt;p&gt;DataK9 严重依赖 ML/AI 技术，这些技术可能会在标签预测中出现潜在的错误。我们打算通过采用反馈循环来自动学习以避免类似的错误。本节简要描述我们如何将此方法融入到我们的框架中。&lt;/p&gt;&lt;p&gt;挑战的第一部分是找出错误。自动标记后，所有者或隐私专家可以使用提供的 UI 修改类别。我们将在一个中心位置跟踪所有这些修改的审计跟踪。下一步是找到可以自动调整 AI/ML 平台的典型错误模式。特别是，我们努力调整我们的规则数据库，其中我们使用了许多根据经验指定的参数。我们计划根据错误模式修改这些参数。此外，我们希望根据从错误中学习来改变不同的模型训练参数。虽然我们的最终目标是完全自动化，但我们将从人机交互开始调整规则。我们最终将走向基于我们对现实世界的理解的自动反馈循环。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-measuring-the-accuracy"&gt;测量精度&lt;/h2&gt;&lt;p&gt;在自动分类领域，准确性是至高无上的，因此，我们将精心测量和披露根据受众需求定制的各种级别的指标作为我们的首要任务。在我们报告层次结构的顶峰，我们将公布以下关键指标：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;分类的标准指标&lt;/strong&gt;：我们的自动化框架的质量对于评估和跟踪进度至关重要。然而，如果没有适当的基线，就不可能衡量自动分类的质量。因此，我们还建议所有者/专家对覆盖所有数据标签的 &amp;lt;1% 的数据集进行手动分类将提供现实的基线。在机器学习/人工智能领域，分类是经过深入研究的文献，专家们定义了一组指标来衡量分类方法的质量。我们在图 8 中用混淆矩阵展示了它们，并在下面进行了描述： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/zZ-qtmybdg0JJVBt6e6iQyOeaylKWmOz4qColn-CDR54eKCLGAemdWT5HBXZcoGJyOoO31BfRgcgeotJV1mRMDXiR26iF7_s0fmZYlXqoL3DQKxkEbgWmc1Z_vt0_mY3rA_8kgouIgfdVD9VDS9CvGI" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 8：准确性混淆矩阵。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;准确度&lt;/strong&gt;：分类准确度是正确预测的总数除以所有数据集上做出的预测的数量。换句话说，这表示机器为标记数据集适当标记了多少列。例如，如果我们有 100 列，K9 正确分类其中 90 列，则准确率为 90%。我们不能仅仅依赖准确性，因为包含 PII 的列只占所有列的一小部分。无法对任何内容进行分类的分类器将具有异常高的准确度，因为真阴性将超过假阴性。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;精度&lt;/strong&gt;：精度是指我们的积极预测的“正确”程度。误报越少，我们的精确度就越高。然而，它并没有讲述完整的故事。例如，如果我们进行单个预测，成功率为 100%，但未能对其他 9 个敏感列进行分类，则我们有 100% 的精确度，但准确度和召回率较低。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;召回率（敏感性）&lt;/strong&gt; ：召回率是指我们识别敏感信息的可能性。假阴性越少，我们的回忆就越好。然而，它并没有讲述完整的故事。例如，如果我们预测所有列都是敏感信息，我们将获得 100% 的召回率，但准确性和精确度较低。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;F2 分数&lt;/strong&gt;：F2 分数是一种在单个可测量指标中偏向于优化召回率而不是精确率的方法。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-precision-vs-recall"&gt;准确率与召回率&lt;/h2&gt;&lt;p&gt;准确率和召回率的影响会影响两个不同的受众。例如，工程师和数据科学家可能会担心低精度指标。较高的误报会不必要地限制对非敏感数据的访问或过早删除它们。如果存在大量漏报，系统可能无法限制敏感数据并强制执行适当的保留，这可能会违反合规性合同。&lt;/p&gt;&lt;p&gt;除了上述指标之外，我们还努力衡量一些额外的指标来跟踪工程进度。这些指标衡量以下观点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;自动化质量&lt;/strong&gt;：我们想要衡量有多少自动标签被数据所有者/管理员等人类推翻。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;规模&lt;/strong&gt;：由于我们必须标记数十万个数据集，因此我们需要测量每天可以加载多少数据集。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;重新分类&lt;/strong&gt;：何时根据架构更改或何时引入/更新新标签对任何数据集进行重新分类。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;效率&lt;/strong&gt;：该举措基于数据爬行，计算成本较高。我们将跟踪 Uber 为每个数据元素（即列或表）自动化支付的费用。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;卓越运营&lt;/strong&gt;：开发过程和初始入职结束后，我们将跟踪需要多少运营开销，例如支持、错误修复和待命。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;存储系统覆盖范围&lt;/strong&gt;：如上所述，我们有不同的存储技术和主干网；跟踪有多少存储实例参与分类工作至关重要。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-production-experiences"&gt;生产经验&lt;/h2&gt;&lt;h3 class="wp-block-heading" id="h-centralized-data-collection-system"&gt;集中数据采集系统&lt;/h3&gt;&lt;p&gt;Uber 在各种基础设施中采用了多种存储系统，每个系统都有自己独特的特点。有些系统遵循特定的模式，而其他系统则不然。即使在基于模式的系统中，模式结构也可能存在很大的差异。&lt;/p&gt;&lt;p&gt;为了简化这些不同系统中存储的数据的分类，我们实施了一个强大的数据收集系统。该系统对来自不同存储系统的数据进行采样，并将它们整合到一个集中式数据湖中。在这种统一方法下，采样数据通过标准化工作流程进行处理。&lt;/p&gt;&lt;p&gt;主要优点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;一致的处理：&lt;/strong&gt;通过将数据集中到公共数据湖中，我们可以促进使用标准化工作流程进行处理。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;简化管理：&lt;/strong&gt;该方法简化了分类作业的管理，提供了集中控制点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该策略不仅解决了存储系统的不同性质带来的挑战，而且还提高了数据处理和管理的效率。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-advancements-in-accuracy"&gt;准确性的进步&lt;/h3&gt;&lt;p&gt;自首次量产以来，DataK9 在过去几年中的准确性不断提高。我们采用两个关键指标来展示 DataK9 的整体准确性，如图 9 所示： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/tU27ocyWH8-RZAuYulUXribMOk2SxfJZryHDxaqmFB1ixYI4o8NxuolUVKCe3UgHCbTBT1g0nHBbT8fstC4laCj7koByY4rbwoENpB0_z8VmzynA3sXCJygI0HoFJZA2SKvJxofEN2bjg2z2FT7UVQM" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 9：准确度指标&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;黄金数据集的准确性：&lt;/strong&gt;在这种方法中，我们将标记结果与经过我们内部隐私专家仔细审查的黄金数据集进行比较。该指标反映了 DataK9 相对于隐私专家制定的标准的准确性和可靠性。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;所有者审查的数据集的准确性：&lt;/strong&gt;此外，我们通过将结果与数据所有者执行的分类进行比较来评估准确性。该指标可以深入了解 DataK9 与数据负责人定义的预期分类的一致性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些指标是强有力的指标，说明了 DataK9 对提高准确性和检查 DataK9 在满足内部隐私标准和数据所有者期望方面的有效性的持续承诺。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-success-metrics-amp-funnel-analysis"&gt;成功指标和漏斗分析&lt;/h3&gt;&lt;p&gt;在追求成功的过程中，我们实施了一套全面的指标来衡量和优化我们的自动化流程。精心设计了详细的漏斗（见下文），以方便调查和识别每个步骤的差距。这个宝贵的工具提供了一种系统方法来跟踪和监控整体分类状态，使我们能够做出明智的决策和改进。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/N0Snd-Ysm8WFVIl1YLVm_d5dDfd-9AKdbOORGr05RIowrgMxD97wBlBFVGmOBCHkHAScS_dxHlC_QF5fXBq_ZBf3Z_VkI0hS7G6gBnpgyeohouXbch4ABmujjZxLdWGx4Mp-2-3x6TZ0c1iNT9T9zYA" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 10：数据集分类漏斗。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;主要优点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;精细分析：&lt;/strong&gt;漏斗使我们能够将自动化流程分解为各个步骤，从而对每个阶段的性能进行精细分析。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;差距识别：&lt;/strong&gt;通过使用漏斗，我们可以有效地识别并缩小自动化流程中的差距，从而简化我们提高效率的工作。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;可追溯性：&lt;/strong&gt;漏斗作为一种可靠的跟踪机制，提供对分类状态的实时洞察，并允许我们跟踪一段时间内的进度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这种对成功指标和漏斗分析的细致方法强化了我们对持续改进的承诺，并使我们能够主动应对自动化流程中的挑战。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;Uber 的 DataK9 项目代表了一项开创性的努力，旨在通过实施人工智能和机器学习技术来解决大规模和现场级别的数据分类挑战。认识到数据分类对于隐私和安全举措的根本作用，Uber 采取了这一举措来自动化和简化流程。&lt;/p&gt;&lt;p class="has-small-font-size"&gt;&lt;em&gt;封面图片归属：&lt;/em&gt; &lt;a href="https://commons.wikimedia.org/wiki/User:MCruz_(WMF)"&gt;&lt;em&gt;MCruz (WMF)&lt;/em&gt;&lt;/a&gt;的“&lt;a href="https://commons.wikimedia.org/w/index.php?curid=38171185"&gt;&lt;em&gt;分类系统 2&lt;/em&gt;&lt;/a&gt; &lt;em&gt;”&lt;/em&gt;已获得&lt;a href="https://creativecommons.org/licenses/by-sa/4.0/?ref=openverse"&gt;&lt;em&gt;CC BY-SA 4.0&lt;/em&gt;&lt;/a&gt;&lt;em&gt;许可&lt;/em&gt;&lt;em&gt;。&lt;/em&gt;&lt;/p&gt;&lt;p class="has-small-font-size"&gt; &lt;em&gt;Apache®、Apache Hive、Hive、Apache Spark 和 Spark 是 Apache Software Foundation 在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/em&gt;&lt;/p&gt;</description><pubDate>Thu, 09 May 2024 06:56:57 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/auto-categorizing-data-through-ai-ml/</guid></item><item><title>【From Predictive to Generative – How Michelangelo Accelerates Uber’s AI Journey】从预测到生成——米开朗基罗如何加速 Uber 的人工智能之旅</title><link>https://www.uber.com/blog/from-predictive-to-generative-ai/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;在过去几年中，机器学习 (ML) 在 Uber 所有业务线的采用和影响都在加速。如今，机器学习在 Uber 的业务中发挥着关键作用，被用来制定关键业务决策，例如预计到达时间、乘客与司机匹配、Eats homefeed 排名和欺诈检测。&lt;/p&gt;&lt;p&gt;作为 Uber 的集中式 ML 平台，自 2016 年首次推出以来， &lt;a href="https://www.uber.com/blog/michelangelo-machine-learning-platform/" rel="noreferrer noopener" target="_blank"&gt;Michelangelo&lt;/a&gt;在推动 Uber ML 发展方面发挥了重要作用。它提供了一套涵盖端到端 ML 生命周期的全面功能，使 Uber 的 ML 从业者能够开发和产品化高-大规模的高质量机器学习应用程序。目前，Michelangelo 管理着大约 400 个活跃的机器学习项目，每月有超过 2 万个模型训练作业。目前有超过 5K 个模型正在生产中，峰值时每秒可提供 1000 万次实时预测。&lt;/p&gt;&lt;p&gt;如下图 1 所示，ML 开发人员经验是一个重要的倍增器，使开发人员能够交付现实世界的业务影响。通过利用米开朗基罗，Uber 的机器学习用例已经从简单的树模型发展到高级深度学习模型，并最终发展到最新的生成式人工智能。在这篇博客中，我们介绍了 Michelangelo 在过去八年中的演变，重点关注 Uber 机器学习开发人员体验的不断增强。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/7H03P6ohPRRzlipNe07kKr3cGsFo3FYOQ1XQNZZbipKWQ5_mLrCuIaDCMtSQyrTGSJ4P-hLG7y7Z_n4C4xIA7Way05VtOWqTigGi1Haq7bBehIOMMi2d7TEW833CMJpgqqXDwBSlq2mGoq_fK8tel14" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：ML 开发人员体验是实现 ML 业务影响的倍增器。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-journey-of-ai-ml-uber"&gt; Uber 的 AI/ML 之旅&lt;/h2&gt;&lt;p&gt;目前，Uber 在 70 多个国家的 10,000 多个城市开展业务，平台每天服务 2500 万次出行，每月活跃用户达 1.37 亿。机器学习几乎已融入 Uber 日常运营的各个方面。事实上，Uber 应用程序中的每一次交互都涉及幕后的机器学习。以骑手应用程序为例：当用户尝试登录时，机器学习用于检测欺诈信号，例如可能的帐户接管。在许多司法管辖区的应用程序中，机器学习被部署来建议目的地自动完成并对搜索结果进行排名。一旦选择了目的地，机器学习就会发挥多种功能，包括预计到达时间计算、行程价格计算、考虑安全措施的乘客与司机匹配以及行程路线选择。行程完成后，机器学习有助于检测支付欺诈、预防退款，并将其范围扩展到为客户服务聊天机器人提供支持。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/p3awCLhFgkOjDsk_T8R0DomPhWJWm9vkP8ZTb2VqKOHi7UN1Ous3e_wqGnM-CBBkOVBnw-pmFRYtF6Ik6kl9e31_t9k-BM6BGs9532Hc3b5u6Bej89QBOlJedgeT23t7mm-iTmTq8RRFKhCpMbWFrYY" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：实时 ML 支撑 Rider 应用程序用户流程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;如图 2 所示，实时 ML 为骑手应用程序中的用户流程提供动力，对于 Eats 应用程序（以及许多其他应用程序）也是如此，如下图 3 所示。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/Yw8-VN8tnl7p9vyZHpX0l4Su9yIzWGHlF2f3mrYbi569DAiz2us-0FLr5Ti7be5HDUFEpyZrNg7SXfBi8edCm3FpklgTkxpr1jMyhuIkuM-oNrfcZpMUjP1BYMTfy3rhCP6OL98YjobFh6GkWR3v0h0" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：实时机器学习支撑 Eater 应用核心用户流程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;回顾 Uber 机器学习的演变，可分为三个不同的阶段：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;2016 年至 2019 年：&lt;/strong&gt;在这个初始阶段，Uber 主要将预测机器学习用于表格数据用例。 XGBoost 等算法用于 ETA 预测、风险评估和定价等关键任务。此外，Uber 在自动驾驶汽车的 3D 映射和感知等关键领域深入研究了深度学习 (DL) 领域，因此需要在 GPU 调度和分布式训练方法（如 Horovod®）方面进行大量投资。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;2019 – 2023：&lt;/strong&gt;第二阶段共同推动高影响力机器学习项目采用深度学习和协作模型开发。重点是模型迭代作为 ML monorepo 中的代码，并支持 DL 作为米开朗基罗的一等公民。在此期间，超过60%的一级模型在生产中采用了深度学习，并显着提升了模型性能。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;从 2023 年开始：&lt;/strong&gt;第三阶段代表新一波生成式 AI 的最新发展，重点是改善 Uber 的最终用户体验和内部员工生产力（在&lt;a href="https://www.uber.com/blog/the-transformative-power-of-generative-ai/" rel="noreferrer noopener" target="_blank"&gt;之前的博客&lt;/a&gt;中进行了描述）。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/QNrwoFNrmVffQxsYS6KkqfXjHLJBBT1QHK99UO3o9KmMdC2t3DP_wUNIvUoU0dK2TOI4krdwKwb_EmV71O2Lm3vqqiYjyuzEwFq6oQZBVro17q1Xr45lpIPyEtaGpgYUEcVNXA60CxYwlJ7YJwFGJ28" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：Uber 从 2016 年到 2023 年的机器学习历程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;在这一变革之旅中，米开朗基罗在提升机器学习能力和帮助团队构建行业领先的机器学习应用程序方面发挥着关键作用。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-michelangelo-1-0-2016-2019"&gt;米开朗基罗 1.0（2016 – 2019）&lt;/h2&gt;&lt;p&gt;当 Uber 于 2015 年踏上 ML 之旅时，应用科学家使用 Jupyter Notebooks™ 来开发模型，而工程师则构建定制管道以将这些模型部署到生产中。没有适当的系统来构建可靠且可重复的管道来大规模创建和管理训练和预测工作流程，也没有简单的方法来存储或比较训练实验结果。更重要的是，没有既定的路径可以在不创建自定义服务容器的情况下将模型部署到生产中。&lt;/p&gt;&lt;p&gt; 2016 年初，Michelangelo 推出，通过端到端系统标准化 ML 工作流程，使 Uber 的 ML 开发人员能够轻松地大规模构建和部署 ML 模型。它首先解决了可扩展模型训练和部署到生产服务容器的挑战（ &lt;a href="https://www.uber.com/blog/michelangelo-machine-learning-platform/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）。然后，构建了一个名为 Palette 的特征存储，以更好地管理和跨团队共享特征管道。它支持批量和近实时特征计算用例。目前，Palette 拥有超过 20,000 个功能，Uber 团队可以直接利用这些功能来构建强大的 ML 模型（ &lt;a href="https://www.infoq.com/presentations/michelangelo-palette-uber/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）。&lt;/p&gt;&lt;p&gt;发布的其他关键 Michelangelo 组件包括但不限于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;图库：&lt;/strong&gt;米开朗基罗的模型和机器学习元数据注册表，为所有类型的机器学习实体提供全面的搜索 API。 （ &lt;a href="https://openproceedings.org/2020/conf/edbt/paper_217.pdf" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Manifold：&lt;/strong&gt; Uber 的 ML 模型无关的可视化调试工具。 （ &lt;a href="https://www.uber.com/blog/manifold/?uclick_id=91e0edf5-abbe-49f9-b9ee-2a7c598a6a35" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）&lt;/li&gt;&lt;li&gt; &lt;strong&gt;PyML：&lt;/strong&gt;一个加速 Python ML 模型原型设计、验证和生产周期的框架。 （&lt;a href="https://www.uber.com/blog/michelangelo-pyml/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;扩展米开朗基罗的模型表示以实现大规模的灵活性。 （ &lt;a href="https://www.uber.com/blog/michelangelo-machine-learning-model-representation/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Horovod&lt;/strong&gt;用于分布式训练。 （&lt;a href="https://www.uber.com/blog/horovod/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;） &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-michelangelo-2-0-2019-2023"&gt;米开朗基罗 2.0（2019 – 2023）&lt;/h2&gt;&lt;p&gt;米开朗基罗的最初目标是在 Uber 引导机器学习并使之民主化。到 2019 年底，Uber 的大多数业务线都已将机器学习集成到他们的产品中。随后，米开朗基罗的重点开始从“让机器学习无处不在”转向“加倍投入高影响力的机器学习项目”，以便开发人员可以提升这些项目的模型性能和质量，从而为 Uber 带来更高的商业价值。考虑到这些项目的复杂性和重要性，需要更先进的机器学习技术，特别是深度学习，并且通常需要许多不同的角色（例如数据科学家和工程师）更快地协作和迭代模型，如图 5 所示这给米开朗基罗 1.0 带来了一些挑战，如下所列。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/2F31vZ3o80U_oyCz7LSHemUXQWNzYy6xLCkn4jIALAjnj83oO6Q8uAiCDOSo3YGDzJBCUOsgI6dKigsqOWqfmKnasgL2vQ3BQP8BOEJZkJBmpj9L4zk9sIMKaYVaHDTO6J7opoEBmGhpIz6OIGvGafU" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：ML 生命周期是迭代的，并且与许多不同的角色协作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt; &lt;strong&gt;1.缺乏全面的机器学习质量定义和项目分层&lt;/strong&gt;：与具有明确定义的质量标准和最佳实践的微服务不同，当时没有一致的方法来衡量全方位的模型质量。例如，许多团队只测量 AUC 和 RMSE 等离线模型性能，而忽略了在线模型性能、训练数据的新鲜度和模型再现性等其他关键指标。这导致模型性能的可见性很低、生产中的模型过时以及数据集覆盖率较差。&lt;/p&gt;&lt;p&gt;此外，重要的是要认识到机器学习项目在业务影响方面存在很大差异。由于缺乏独特的机器学习分层系统，导致在资源分配、支持和管理中断方面采用统一的方法，而不管项目的影响如何。这导致高影响力项目投资不足或没有得到应有的优先考虑。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;2. 对深度学习模型的支持不足：&lt;/strong&gt;直到 2019 年，Uber 的机器学习用例主要使用基于树的模型，这本质上不利于采用自定义损失函数、增量训练和嵌入等先进技术。相反，Uber 拥有适合训练 DL 模型的大量数据，但基础设施和开发人员体验方面的挑战阻碍了这一方向的进展。许多团队（例如 Maps ETA 和 Rider 激励团队）必须花费数月时间来开发自己的 DL 工具包，然后才能成功训练第一个版本的 DL 模型。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;3. 对协作模型开发的支持不足&lt;/strong&gt;：早期，大多数机器学习项目都是小规模的，并且从开始到生产仅由单个开发人员编写和迭代。因此，Michelangelo 1.0 并未针对高度协作的模型开发进行优化，并且 Michelangelo 1.0 UI 和 Jupyter Notebook 中的协作很困难，并且通常通过手动复制和合并来完成，而无需版本控制或分支。  此外，没有针对 UI 模型配置更改或笔记本编辑的代码审查流程，并且缺乏 ML 代码和配置的集中存储库导致它们分散在各个来源中。这些对我们的工程流程构成了重大威胁，并使跨众多机器学习项目的大规模模型探索变得困难。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;4. 碎片化的 ML 工具和开发人员体验：&lt;/strong&gt;自 2015 年以来，除了 Michelangelo 之外，Uber 的不同团队还为 ML 生命周期和用例的子集构建了许多 ML 工具，例如来自数据团队的&lt;a href="https://www.uber.com/blog/evolution-ds-workbench/" rel="noreferrer noopener" target="_blank"&gt;Data Science Workbench&lt;/a&gt; (DSW)，用于托管Jupyter Notebooks、来自 Marketplace 团队的用于 ML 工作流程编排和自动化的 ML Explorer，以及来自 Risk 团队的 uFlow/uScorer，专门用于来自其自己团队的训练和推理模型。为不同的模型类型开发 ML 模型也有不同的方法，例如，用于 SparkML 和 XGBoost 模型的 Michelangelo UI、用于 DL 模型的 Jupyter Notebook 以及用于基于 Python 的自定义模型的&lt;a href="https://www.uber.com/blog/michelangelo-pyml/" rel="noreferrer noopener" target="_blank"&gt;PyML&lt;/a&gt; 。启动一个机器学习项目通常需要在这种半隔离的工具之间不断切换，这些工具是用不同的 UI 模式和用户流程构建的，导致用户体验碎片化并降低了生产力。&lt;/p&gt;&lt;p&gt;为了应对这些挑战，米开朗基罗 2.0 将分散的 ML 平台重新架构为具有统一 UI 和 API 的单一连贯产品，用于端到端 ML 生命周期。 Michelangelo 2.0 有四个面向用户的主题：(1) 模型质量和项目分层，(2) 通过 Canvas 将模型迭代为代码，(3) DL 作为一流的平台公民，(4) 通过 MA 统一 ML 开发人员体验工作室。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-architectural-overview"&gt;架构概览&lt;/h3&gt;&lt;p&gt;米开朗基罗 2.0 以四个支柱为中心。在最底层，我们正在启用一个允许即插即用平台组件的架构。一些组件是内部构建的，其他组件可以是来自开源或第三方的最先进的商品。最重要的是迎合应用科学家和机器学习工程师的开发和生产经验。为了提高模型开发速度，我们正在简化开发体验和支持协作、可重用开发的技术。  我们相信这种方法将使我们能够在平台级别跟踪和执行合规性。  我们正在投资模型的安全部署和自动模型再训练等生产经验，以方便大规模维护和管理模型。最后，我们专注于模型的质量，并投资于能够在所有阶段衡量模型质量并系统地改进模型的工具。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/gRuNaUCrkkMDsG1LwyLo43lqxK0Sl_dtJXQU3o2NX2VKH1wPy9SOvlPbk91_8o6dOGEsbSQz336xH1u9Z_RVi5vfoS9A2TjQg5_T5sQ7-jCrMkfRrxL4supkxIoviPLMvO8Gs0iV0QA2O0p-rxUIk_g" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：米开朗基罗 2.0 架构的高级概念。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;以下是米开朗基罗2.0的一些架构设计原则：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;定义项目分层，并专注于高影响力的用例，以最大限度地提高 Uber 的机器学习影响力。为长尾机器学习用例提供自助服务，以便他们可以利用平台的强大功能。&lt;/li&gt;&lt;li&gt;大多数 ML 用例可以利用 Michelangelo 的核心工作流程和 UI，而 Michelangelo 还支持深度学习等高级用例所需的更多定制工作流程。&lt;/li&gt;&lt;li&gt;单片与即插即用。架构将支持不同组件的即插即用，但托管解决方案将仅支持其中的一部分以获得最佳用户体验。为高级用例带来您自己的组件。&lt;/li&gt;&lt;li&gt; API/代码驱动与 UI 驱动。遵循 API 优先原则，利用 UI 实现可视化和快速迭代。支持模型迭代作为版本控制和代码审查的代码，包括 UI 中所做的更改。&lt;/li&gt;&lt;li&gt;构建与购买决策。利用 OSS 或云或内部构建的一流产品。 OSS 解决方案可能优先于专有解决方案。请谨慎对待云解决方案的容量成本。&lt;/li&gt;&lt;li&gt;将最佳 ML 实践（例如安全模型部署、模型再训练和平台中的功能监控）编入规范。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;该系统由三个平面组成，即控制平面、离线和在线数据平面。控制平面定义面向用户的 API 并管理系统中所有实体的生命周期。离线数据平面负责大数据处理的繁重工作，例如特征计算、模型训练和评估、离线批量推理等。在线数据平面处理实时模型推理和特征服务，供其他微服务使用。&lt;/p&gt;&lt;p&gt;控制平面采用相同的&lt;a href="https://github.com/cncf/tag-app-delivery/blob/163962c4b1cd70d085107fc579e3e04c2e14d59c/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md" rel="noreferrer noopener" target="_blank"&gt;Kubernetes™ Operator 设计模式，&lt;/a&gt;实现模块化和可扩展性。 Michelangelo API 还遵循相同的&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md" rel="noreferrer noopener" target="_blank"&gt;Kubernetes API 约定&lt;/a&gt;，并对 ML 相关实体（如项目、管道、PipelineRun、模型、修订、推理服务器、部署等）的操作进行标准化。通过利用 Kubernetes API 机制（包括 API 服务器、 &lt;a href="https://etcd.io/" rel="noreferrer noopener" target="_blank"&gt;etcd&lt;/a&gt;和控制器）通过 Manager，所有 Michelangelo API 都可以以一致的方式访问，从而带来更加用户友好和简化的用户体验。此外，声明式 API 模式对于 Michelangelo 支持 GIT 存储库中的 UI 和代码变更也至关重要，稍后将详细介绍。&lt;/p&gt;&lt;p&gt;离线数据平面由一组 ML 管道组成，包括训练、评分、评估等，这些管道被定义为步骤的 DAG。 ML 管道支持中间检查点并在步骤之间恢复，以避免重复执行先前的步骤。步骤在 Ray™ 或 Spark™ 等框架之上执行。在线数据平面管理 RPC 服务和流处理作业，为在线预测、在线特征访问和近实时特征计算提供服务。&lt;br /&gt;&lt;/p&gt;&lt;p&gt;图7显示了Michelangelo 2.0系统的详细设计，它降低了工程复杂性并简化了对其他基础设施组件的外部依赖。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/MqljT9LUabDAsMHNb1k6y5LqyILCQdzWU1zi1ZD_XWneCBiYqo3Wtoda2_ysExjD0prHflRC8yBVK5Cziy2VkrQHuThszXcgvWMS4CfSZDllePznu6UB_Jw_mOZE25UA4o5gPy46woAP9uSFXrJlctI" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 7：Michelangelo 2.0 的详细系统设计，包括离线、在线和控制平面。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-model-quality-and-project-tiering"&gt;模型质量和项目分层&lt;/h3&gt;&lt;p&gt;生产就绪的机器学习系统的开发和维护非常复杂，涉及模型生命周期的多个阶段和复杂的支持基础设施。通常，ML 模型会经历特征工程、训练、评估和服务等阶段。缺乏全面的 ML 质量测量导致 ML 开发人员对模型生命周期不同阶段的各种质量维度的了解有限。此外，这种差距阻碍了组织领导者就机器学习项目的质量和影响做出充分知情的决策。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/bIbKLbwB7em5FhgTLHym9OHRHcaORl4UASUzEquCX0O11RkyCoOCxYbiANmykUDdFUoAZGfX7EZ_kCLyeloxxy82D_gvWZX7aYpfQmVZWPb68632BR5YjLcCSSLOznLU8Yr-Ei7QbnqSuNYiuTyu33w" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 8：典型 ML 系统中的 ML 质量维度示例（黄色）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;为了弥补这些差距，我们推出了模型卓越评分（MES），这是一个用于测量和监控模型生命周期每个阶段的关键维度和指标的框架，例如训练模型准确性、预测准确性、模型新鲜度和预测特征质量，确保采用全面、严格的方法大规模部署机器学习。该框架利用站点可靠性工程师 (SRE) 和 DevOps 专业人员在生产环境中管理微服务可靠性时常用的相同&lt;a href="https://en.wikipedia.org/wiki/Service-level_agreement" rel="noreferrer noopener" target="_blank"&gt;服务级别协议&lt;/a&gt;(SLA) 概念。通过与 SLA 工具集集成，MES 建立了衡量和确保 Uber ML 模型质量的标准。此外，MES 跟踪并可视化模型的合规性和质量，从而为整个组织的 ML 计划提供更清晰、更全面的视图。请参阅&lt;a href="https://www.uber.com/blog/enhancing-the-quality-of-machine-learning-systems-at-scale/" rel="noreferrer noopener" target="_blank"&gt;MES 博客&lt;/a&gt;了解更多详细信息。&lt;/p&gt;&lt;p&gt;为了区分高影响力和长尾用例，我们引入了定义明确的机器学习项目分层方案。该计划由四层组成，其中第一层是最高的。一级项目由服务于核心出行和核心乘客流程中关键功能的模型组成，例如预计到达时间计算、安全性和欺诈检测等。只有直接影响核心业务运营的模型才有资格获得一级状态。相反，第四级项目通常包含实验性和探索性用例，直接业务影响有限。这种分层方案使我们能够就机器学习项目中断处理的资源分配、资源投资、最佳实践实施和合规性事务等方面做出明智的决策。它确保对每个项目的关注和资源投入与其相对优先级和影响相称。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-model-iterations-as-code-nbsp"&gt;将模型迭代作为代码&lt;/h3&gt;&lt;p&gt;为了提高 ML 开发人员的工作效率、促进无缝团队协作并提高 2020 年 ML 应用程序的整体质量，我们推出了 Project Canvas。该项目旨在将软件工程最佳实践应用于 ML 开发生命周期，实施版本控制，利用 Docker 容器的强大功能、集成 CI/CD 工具，并通过引入标准化 ML 应用程序框架来加快模型开发。 Canvas 的关键组件包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;ML 应用程序框架 (MAF)&lt;/strong&gt; ：预定义但可自定义的 ML 工作流程模板，为 ML 开发提供代码和配置驱动的方式，专为深度学习等复杂的 ML 技术量身定制。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ML Monorepo&lt;/strong&gt; ：一个集中存储库，将所有 ML 开发事实来源存储为代码，具有强大的版本控制功能。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ML 依赖管理&lt;/strong&gt;：使用 Bazel 和 docker 构建提供软件依赖管理。每个 ML 项目都有自己定制的 docker 镜像。除了软件依赖之外，模型训练和服务代码将被打包到一个不可变的 docker 镜像中，用于生产模型重新训练和服务。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;机器学习开发环境：&lt;/strong&gt;为机器学习开发人员提供一致的本地开发和远程生产执行环境，以便他们可以在本地测试和调试模型，然后在远程生产环境中运行模型，以实现快速模型迭代。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ML 持续集成/持续交付&lt;/strong&gt;：针对主分支进行持续集成，并通过各种测试和验证将 ML 模型自动部署到 ML monorepo 主分支的生产环境。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ML 工件管理：&lt;/strong&gt;为工件和沿袭跟踪提供支持。工件是 ML 对象，例如模型、数据集和评估报告及其相应的元数据。对象将存储在分布式存储中，元数据将被完全索引和搜索。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;MA Assistant (MAA)：&lt;/strong&gt;米开朗基罗的 AutoML 解决方案，用于自动模型架构搜索和特征探索/修剪。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/Teij0cWxZToDYuyzL5LIk0iUWeSr1N3cpFnCpJGVHg8M8Hj8twroleP_flTmf6kSCkgHPXcdcTBg8-wuGN2SoaA0squnLErPVSDf2xBqPV6bz9pFlvRRXQz56LOZfWrCau7nHkfqWeTCmxCoXS5eh_A" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 9：Canvas：简化端到端 ML 开发人员体验。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt; Canvas 还通过利用 Bazel 和 docker 构建简化了 ML 依赖关系管理。每个 ML 项目都会有其自定义的 docker 镜像，模型训练和服务代码将被打包到一个不可变的 docker 镜像中，用于生产模型重新训练和服务。此外，Canvas 为 ML 开发人员提供了一致的本地和远程开发环境，可以在本地测试和调试模型，然后在远程生产环境中运行模型以实现快速模型迭代。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-deep-learning-as-a-first-class-platform-citizen"&gt;深度学习作为一流平台公民&lt;/h3&gt;&lt;p&gt;采用自定义损失函数、增量训练和嵌入等先进技术带来了重大挑战。深度学习可以更灵活地应对这些挑战。此外，随着数据集变大，深度学习通常会表现出色，因为它可以利用更多数据来学习更复杂的表示。&lt;/p&gt;&lt;p&gt; 2019 年之前，Uber 的大多数 DL 模型都用于&lt;a href="https://www.uber.com/blog/machine-learning-model-life-cycle-version-control/" rel="noreferrer noopener" target="_blank"&gt;自动驾驶汽车&lt;/a&gt;（例如&lt;a href="https://proceedings.mlr.press/v87/yang18b/yang18b.pdf?uclick_id=91e0edf5-abbe-49f9-b9ee-2a7c598a6a35" rel="noreferrer noopener" target="_blank"&gt;3D 映射&lt;/a&gt;、感知）、计算机视觉（例如 &lt;a href="https://www.uber.com/blog/real-time-id-check/" rel="noreferrer noopener" target="_blank"&gt;驾驶员面部识别&lt;/a&gt;）和自然语言处理（例如&lt;a href="https://www.uber.com/en-AU/blog/cota/" rel="noreferrer noopener" target="_blank"&gt;客户痴迷&lt;/a&gt;）用例。然而，针对核心业务的深度学习模型非常少，特别是针对表格数据用例。阻碍深度学习采用的一个重要原因是米开朗基罗1.0缺乏端到端的深度学习支持。与基于树的模型不同，深度学习模型通常需要更复杂的机器学习平台支持，从特征转换和模型训练到模型服务和 GPU 资源管理。本节的其余部分将概述我们在 Michelangelo 2.0 中对深度学习支持的投资。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-feature-transformation"&gt;特征变换&lt;/h4&gt;&lt;p&gt;米开朗基罗 1.0 实现了用于特征转换的 DSL，例如在模型训练和服务路径中使用的标准化和分桶化。该转换与&lt;a href="https://www.uber.com/blog/michelangelo-machine-learning-model-representation/" rel="noreferrer noopener" target="_blank"&gt;Spark PipelineModel&lt;/a&gt;模型捆绑在一起，从而&lt;a href="https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew" rel="noreferrer noopener" target="_blank"&gt;消除了训练服务偏差的根源。&lt;/a&gt;然而，DSL 转换是作为 Spark 转换器实现的，无法在用于低延迟服务的 DL 模型的 GPU 上运行。在 Michelangelo 2.0 中，我们实现了一种新的 DL 原生转换解决方案，允许用户使用 Keras 或 PyTorch 运算符转换其特征，并为高级用户提供使用 Python 代码定义自定义特征转换的灵活性。与&lt;a href="https://blog.research.google/2017/02/preprocessing-for-machine-learning-with.html" rel="noreferrer noopener" target="_blank"&gt;TensorFlow 变换&lt;/a&gt;类似，变换图与 TensorFlow 或 TorchScript 中的模型推理图相结合，以在 GPU 上提供低延迟服务。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-model-training"&gt;模型训练&lt;/h4&gt;&lt;p&gt;Michelangelo 2.0 通过利用我们的分布式训练框架 Horovod，支持 TensorFlow 和 PyTorch 框架进行大规模深度学习模型训练。此外，我们还进行了以下改进，以实现更好的可扩展性、容错性和效率。&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;Ray 上的分布式 GPU 训练和调优。&lt;/strong&gt; （ &lt;a href="https://www.youtube.com/watch?v=gMT_ONmI9RM&amp;amp;list=PLzTswPQNepXmLUiL4F_1VHrPcCz1OeILw&amp;amp;index=47" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）。从历史上看，Michelangelo 的模型训练是在 Spark 上运行的。然而，DL 给 Spark 带来了新的挑战，例如缺乏 GPU 执行器、mini-batch shuffle 和 all-reduce。 &lt;a href="https://horovod.readthedocs.io/en/stable/spark_include.html" rel="noreferrer noopener" target="_blank"&gt;Horovod on Spark&lt;/a&gt;使用 Spark 估计器语法封装了深度学习训练，并提供了与训练管道的轻松集成。然而，它也引入了许多操作复杂性，例如单独的集群作业、生命周期管理和故障场景。在 Michelangelo 2.0 中，我们用基于 Ray 的训练器取代了基于 Spark 的 XGBoost 和 DL 训练器，以实现更好的可扩展性和可靠性。我们还从内部超参数调整解决方案切换到 RayTune。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;具有自动缩放和容错功能的 Elastic Horovod&lt;/strong&gt; 。 （&lt;a href="https://www.uber.com/blog/horovod-ray/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;）。  Elastic Horovod 允许分布式训练，在整个训练过程中动态扩展工作人员数量。现在，当机器来回工作时，工作人员可以在最小程度地中断的情况下继续培训。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;资源高效的增量培训&lt;/strong&gt;。深度学习的优点之一是能够使用额外的数据集增量训练模型，而无需从头开始训练。这显着提高了生产重新训练的资源效率，并增加了数据集覆盖范围以提高模型准确性。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Canvas 中的声明式深度学习训练管道&lt;/strong&gt;。深度学习模型需要自定义模型代码和损失函数等。在 Canvas 中，我们将训练管道设计为声明式和可扩展的，以便用户插入自定义模型代码，例如估计器、优化器和损失函数，如图 9 所示。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/wq4mV9-lSpxnYgUrC91PzO7JaMnSawey2wir-Ai0odqHzIaTkMA8erWscsRHZl22fpCQvzipR86Tkw_jgzFs3jokhk86Zqnc2OfeY882_3ChONlnLYAtEQpMCi8u6lMAw9PS8oOD-0WmtSsbEvs1bZA" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 9：Canvas 中深度学习模型的示例训练管道。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-model-serving"&gt;模特服务&lt;/h4&gt;&lt;p&gt;大多数采用深度学习的 Uber 一级 ML 项目对服务延迟非常敏感，例如地图 ETA 和 Eats homefeed 排名。此外，模型服务必须支持 TensorFlow 和 PyTorch DL 框架，但要从用户那里抽象出框架级细节。从历史上看， &lt;a href="https://github.com/uber/neuropod" rel="noreferrer noopener" target="_blank"&gt;Neuropod&lt;/a&gt;一直是 Michelangelo 中默认的深度学习服务引擎。然而，它缺乏持续的社区支持，并且正在被弃用。在 Michelangelo 2.0 中，我们将&lt;a href="https://github.com/triton-inference-server/server" rel="noreferrer noopener" target="_blank"&gt;Triton&lt;/a&gt;作为下一代模型服务引擎集成到我们的在线预测服务 (OPS) 中，作为新的 Spark 变压器。 Triton 是 Nvidia 开发的开源推理服务器，支持包括 TensorFlow、PyTorch、Python 和 XGBoost 在内的多种框架，它针对 GPU 进行了高度优化，可实现低延迟服务。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h4 class="wp-block-heading" id="h-gpu-resource-management"&gt; GPU资源管理&lt;/h4&gt;&lt;p&gt;深度学习训练和服务都需要大规模的 GPU 资源。目前，Uber 在本地数据中心和 OCI 和 GCP 等云提供商中管理着 5000 多个 GPU。这些 GPU 分布在多个区域、许多专区和集群中。计算集群正在从&lt;a href="https://kccna18.sched.com/event/GrTx/peloton-a-unified-scheduler-for-web-scale-workloads-on-mesos-kubernetes-min-cai-nitin-bahadur-uber" rel="noreferrer noopener" target="_blank"&gt;Peloton / Mesos&lt;/a&gt;迁移到&lt;a href="https://kccncna19.sched.com/event/Uaad/kubernetizing-big-data-and-ml-workloads-at-uber-mayank-bansal-min-cai-uber" rel="noreferrer noopener" target="_blank"&gt;Kubernetes&lt;/a&gt; 。为了最大限度地提高资源利用率，Uber 投资于不同团队之间的弹性 CPU 和 GPU 资源共享，以便每个团队都可以机会性地使用其他团队的闲置资源。在计算集群之上，我们跨多个 Kubernetes 集群构建了一个作业联合层，以隐藏区域、区域和集群详细信息，以实现更好的作业可移植性和轻松的云迁移。作业联合层采用与 Kubernetes 运算符相同的设计模式，并在 Michelangelo 的统一 API 框架中作为作业 CRD 控制器实现，如图 7 所示。目前，作业控制器支持 Spark 和 Ray 作业。&lt;/p&gt;&lt;p&gt;凭借米开朗基罗 2.0 中对深度学习的端到端支持，Uber 在不同业务线的深度学习采用方面取得了显着改进。在过去几年中，一级项目中的深度学习采用率从几乎为零增加到 60% 以上。例如， &lt;a href="https://www.uber.com/blog/deepeta-how-uber-predicts-arrival-times/" rel="noreferrer noopener" target="_blank"&gt;DeepETA&lt;/a&gt;模型拥有超过 1 亿个参数，训练次数超过 10 亿次。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-ma-studio-one-unified-web-ui-tool-for-everything-ml-uber"&gt; MA Studio – 一个统一的 Web UI 工具，适用于所有 ML @ Uber&lt;/h3&gt;&lt;p&gt;为了解决上述机器学习开发人员体验中的挑战，开发了 Michelangelo (MA) Studio，将现有的 Michelangelo 产品和新建的平台功能统一到一个用户旅程中，通过完全重新设计的 UI 和 UX 提供无缝的用户体验。 MA Studio 提供了简化的用户流程，涵盖了 ML 旅程的每一步，从特征/数据准备、模型训练、部署，一直到生产性能监控和模型 CI/CD，全部集中在一个地方，以提高 ML 开发人员的工作效率。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/23NZpcegPdWpU-kpA3971k3KKYGxjh-AMVlljf26FeKV1eAga8tCkU3vuZCgWOwlDYwDbkWXFJ0wDX-8DDsXKBGT1-ucOZbA375Pg7aectkscLfD01kJbB0EEp5vX1LSQ4ba6NhbS_N1fpX89-Dit9Y" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 10：MA Studio 项目登陆页面涵盖端到端 ML 开发生命周期。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt; MA Studio 拥有一系列额外优势：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;版本控制和代码审查&lt;/strong&gt;：所有与 ML 相关的代码和配置均受版本控制，所有更改都经过代码审查过程，包括从 UI 创建的模型。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;现代化的模型部署堆栈&lt;/strong&gt;：安全和增量的区域部署、自动回滚触发器和生产运行时验证。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;内置统一的ML可观测性工具包&lt;/strong&gt;：模型性能监控、特征监控、在线/离线特征一致性检查和MES。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;统一的 ML 实体生命周期管理&lt;/strong&gt;：用户受益于直观的 UI 和结构良好的用户流程，用于管理从模型和管道到数据集和报告的所有 ML 实体。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;增强的调试功能&lt;/strong&gt;：MA Studio 增强了调试功能并加速了 ML 管道故障的恢复。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/v2Dr41DHAjWT4JcMTHMRZ-EFv51edbx8RPw4BJ0MsqrnNAlI0lKoZJ-ZGRvUAZ5ez9VjNeKizUBwq-Hlxzp3uVs9i4NahPqDmgI9bOrJyBM_MC1NZjz7FD2Q46MJ_H32lP6VN3zqgEeeghht_r3mdrk" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 11：适用于标准和高级 ML 用例的 MA Studio 和 Canvas。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;对于 Uber 的任何机器学习需求，您只需要两个工具：Canvas 和 MA Studio。 MA Studio 的用户友好型 UI 涵盖了标准 ML 工作流程，可促进 XGB 模型训练和标准模型重新训练流程等任务，而无需编写任何代码。在处理更复杂的场景时，例如深度学习训练或定制的再训练流程，Canvas 是首选工具。无论您是通过 Canvas 还是 UI 构建管道，您都可以无缝执行和管理这些管道、部署经过训练的模型以及监控和调试模型性能 - 所有这些都可以通过 MA Studio UI 进行。值得注意的是，所有模型代码和相关配置现在都受到版本控制，任何更改都经过细致的代码审查过程，这极大地提高了 Uber 生产中的 ML 应用程序的质量。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-generative-ai-2023-now"&gt;生成式人工智能（2023 年至今）&lt;/h2&gt;&lt;p&gt;生成式人工智能的最新进展，特别是在大语言模型（LLM）领域，能够从根本上改变我们通过自然语言与机器的交互。 Uber 的多个团队正在积极研究使用法学硕士来通过助理提高内部生产力，通过自动化简化业务运营，并通过神奇的用户体验改进最终用户产品，同时解决&lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Large_language_models" rel="noreferrer noopener" target="_blank"&gt;与使用法学硕士相关的问题&lt;/a&gt;。图 12 显示了 Uber 这三类生成式 AI 用例的潜在价值。 &lt;a href="https://www.uber.com/blog/the-transformative-power-of-generative-ai/" rel="noreferrer noopener" target="_blank"&gt;了解更多&lt;/a&gt;。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/ymra4p74-37pbRET_WeL_zvyEI6I9LOg8RICp0dVRQYBEYEllRz9psRc7omJcLe6ohyJrEztIFvOK7egSKiev_hgSpI8L4CmZ_pntoRdOOaJT9DAAgWWOMonlGcLDZl0oC3OpEs21Te5R-hr7KLdG-I" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 12：Uber 的三类生成人工智能用例。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;为了开发生成式人工智能应用程序，团队需要通过第三方 API 访问外部法学硕士和/或内部托管的开源法学硕士。这是因为外部模型在需要常识和复杂推理的任务中具有卓越的性能，同时通过利用丰富的专有数据，我们可以微调开源模型，以在以 Uber 为中心的任务上实现高水平的准确性和性能，成本的一小部分和更低的延迟。这些经过微调的开源模型由内部托管。&lt;/p&gt;&lt;p&gt;因此，我们开发了 Gen AI Gateway，为团队提供统一的界面，以遵守安全标准和保护隐私的方式访问外部法学硕士和内部托管的法学硕士。 Gen AI 网关的一些功能包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;记录和审计：&lt;/strong&gt;确保全面的跟踪和问责。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;成本护栏和归因：&lt;/strong&gt;在归因使用情况的同时管理费用，并对过度使用发出警报。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;安全和政策护栏：&lt;/strong&gt;确保法学硕士的使用符合我们的内部准则。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;个人身份信息 (PII) 编辑：&lt;/strong&gt;对个人数据进行识别和分类，并在将输入发送到外部法学硕士之前对其进行编辑。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了加速 Uber 生成式 AI 应用程序的开发，我们对 Michelangelo 进行了扩展，以支持完整的 LLMOps 功能，例如微调数据准备、即时工程、LLM 微调和评估、LLM 部署和服务以及生产性能监控。一些关键组件包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型目录包含一系列预构建且随时可用的 LLM，可通过第三方 API（例如 GPT4、Google PaLM）或 Michelangelo 上内部托管的开源 LLM（例如 Llama2）访问。用户可以在目录中探索有关这些法学硕士的广泛信息并启动各种工作流程。这包括在 MA Studio 中微调模型或将模型部署到在线服务环境。该目录提供了多种预训练模型的选择，增强了平台的多功能性。&lt;/li&gt;&lt;li&gt; LLM评估框架使用户能够在不同方法（例如，内部与3P和提示与3P微调）中比较LLM，并通过提示和模型的迭代来评估改进。&lt;/li&gt;&lt;li&gt;提示工程工具包使用户可以通过完整版本控制和代码审核过程创建和测试提示，验证输出并将提示模板保存在集中式存储库中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了实现具有成本效益的LLM微调和低延迟LLM服务，我们已经对米开朗基罗培训和服务堆栈实施了一些重要的增强：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;与拥抱的脸部集成&lt;/strong&gt;：我们利用了&lt;a href="https://huggingface.co/models" rel="noreferrer noopener" target="_blank"&gt;拥抱面枢纽&lt;/a&gt;和相关库（如&lt;a href="https://huggingface.co/docs/peft/index" rel="noreferrer noopener" target="_blank"&gt;PEFT）&lt;/a&gt;上可用的开源LLM为LLMS实施了基于射线的培训师。微调的LLM和相关的元数据存储在Uber的模型存储库中，该存储库可从模型推理基础架构访问。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;启用模型并行性&lt;/strong&gt;：米开朗基罗以前不支持训练DL模型的模型并行性。该限制将可训练模型的大小限制在可用的GPU内存中，例如，在16 GB GPU上，理论上最大值为40亿个参数。在更新的LLM培训框架中，我们集成了DeepSpeed以启用模型并行性。这一突破消除了GPU内存限制，并允许训练更大的DL模型。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;弹性GPU资源管理：&lt;/strong&gt;我们已经通过Michelangelo工作控制器在GPU上提供了射线簇。该规定使LLM模型的培训能够在本地最强大的GPU上进行培训。此外，这种集成为使用云GPU的未来扩展设定了阶段，从而增强了可扩展性和灵活性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;利用Michelangelo提供的这些平台功能，Uber的团队热情地开发了LLM驱动的应用程序。我们期待很快分享我们在LLM的生产中的进步。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;ML已发展成为Uber关键业务领域的基本驱动力。该博客深入研究Uber的ML平台米开朗基罗的八年变革旅程，强调了ML开发人员体验的重大增强。这一旅程以三个不同的阶段展开：2016年至2019年表格数据的预测ML的基础阶段，2019年至2023年之间向深度学习的逐步转变，以及最近从2023年开始转向生成AI的企业。&lt;/p&gt;&lt;p&gt;在如此复杂的层面上构建大规模的端到端ML平台的知识经验教训，以Uber的规模支持ML用例。关键要点包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;建立一个集中的ML平台，而不是让单个产品团队建立自己的ML基础架构，可以显着提高中型或大型公司内的ML开发效率。理想的ML组织结构包括一个集中的ML平台团队，并由每个产品团队中嵌入的专门数据科学家和ML工程师进行补充。&lt;/li&gt;&lt;li&gt;以统一的方式提供基于UI的和代码/配置驱动的用户流，对于提供无缝的ML DEV体验至关重要，尤其是对于ML开发人员对Dev Tools的偏好的大型组织而言，Dev Tools的偏好在不同人群中很大变化。&lt;/li&gt;&lt;li&gt;为大多数用户提供具有预定义的工作流模板和配置的高级抽象层的策略，同时允许高级电源用户直接访问低级基础架构组件以构建自定义的管道和模板有效。&lt;/li&gt;&lt;li&gt;以模块化的方式设计平台体系结构，以便可以使用插件方法来构建每个组件，从而可以快速采用开源，第三方供应商或内部的最先进技术发展。&lt;/li&gt;&lt;li&gt;尽管深度学习证明在解决复杂的ML问题方面证明了强大的功能，但挑战在于支持大规模的DL基础架构并保持这些模型的性能。仅当其优势与特定要求保持一致时，只使用DL。 Uber的经验表明，在某些情况下，XgBoost在性能和成本方面都优于DL。&lt;/li&gt;&lt;li&gt;并非所有ML项目都是平等的。拥有明确的ML分层系统可以有效地指导资源和支持的分配。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;米开朗基罗的使命是为Uber的ML开发人员提供一流的ML功能和工具，以便他们可以大规模迅速构建，部署和迭代高质量的ML应用程序。作为AI平台团队，我们提供了深入的ML专业知识，推动ML技术的标准化和创新，建立信任并与我们的合作伙伴团队合作，并培养一种充满活力的ML文化，以便将ML拥抱和利用到其最大的潜力。我们对这项任务的承诺坚定不移，我们对前方有前途的未来充满热情。&lt;/p&gt;&lt;p&gt;如果您有兴趣加入我们的这项激动人心的冒险，请查看我们的&lt;a href="https://www.uber.com/us/en/careers/" rel="noreferrer noopener" target="_blank"&gt;工作网站&lt;/a&gt;以获取空缺。此外，我们期待与AI/ML空间中的其他团队合作，以建立一个强大的ML社区，并共同加快AI/ML技术的进步。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Apache®，Apache Spark，Spark和Star Logo是美国和/或其他国家/地区Apache Software Foundation的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Horovod和Kubenetes是美国和/或其他国家的LinuxFoundation®的注册商标或商标。 LinuxFoundation®的认可不受这些标记的使用暗示。&lt;/p&gt;&lt;p class="has-small-font-size"&gt;雷是美国和/或其他国家/地区的Enscale，Inc的注册商标或商标。&lt;/p&gt;</description><pubDate>Thu, 02 May 2024 09:46:07 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/from-predictive-to-generative-ai/</guid></item><item><title>【DragonCrawl: Generative AI for High-Quality Mobile Testing】DragonCrawl：用于高质量移动测试的生成式 AI</title><link>https://www.uber.com/blog/generative-ai-for-high-quality-mobile-testing/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;Uber 的开发者平台团队不断开发新的创新想法，以增强开发者的体验并增强我们应用程序的质量。质量和测试齐头并进，2023 年，我们接受了一项令人兴奋的新挑战，改变我们测试移动应用程序的方式，重点是机器学习 (ML)。具体来说，我们正在训练模型来测试我们的应用程序，就像真人一样。&lt;/p&gt;&lt;p&gt;移动测试仍然是一个尚未解决的挑战，尤其是在我们的规模下，包括数千名开发人员和 3,000 多个同时进行的实验。通常会进行手动测试，但开销较高，无法针对每一个微小的代码更改进行广泛的测试。虽然测试脚本可以提供更好的可扩展性，但它们也不能免受较小更新（例如新的弹出窗口和按钮更改）引起的频繁中断的影响。所有这些更改，无论多么微小，都需要定期手动更新测试脚本。因此，从事此工作的工程师将 30-40% 的时间投入到维护上。此外，这些测试的大量维护成本极大地阻碍了它们在不同城市和语言之间的适应性和可重用性（想象一下，必须为我们所使用的 50 多种语言雇用手动测试人员或移动工程师！），这使得我们真的很难有效地扩展测试并确保 Uber 在全球范围内高质量运营。&lt;/p&gt;&lt;p&gt;为了解决这些问题，我们创建了 DragonCrawl，这是一个使用大型语言模型（LLM）以人类直觉执行移动测试的系统。它根据看到的屏幕决定采取什么操作，并独立适应用户界面的变化，就像真人一样。&lt;/p&gt;&lt;p&gt;当然，新的创新也会带来新的错误、挑战和挫折，但这是值得的。我们没有放弃为 Uber 应用带来无代码测试的使命，并于 2023 年底推出了 DragonCrawl。从那时起，我们一直在不同的城市和语言中以高稳定性测试一些最重要的流程，并且无需维护它们。在 DragonCrawl 的帮助下，在如此多的语言和城市中扩展移动测试并确保质量从人类不可能变为可能。自推出 DragonCrawl 以来的三个月内，我们阻止了十个影响客户的高优先级错误，同时节省了数千个开发人员时间并降低了测试维护成本。&lt;/p&gt;&lt;p&gt;本博客将快速介绍大型语言模型，深入探讨我们的架构、挑战和结果。最后我们将简单介绍一下 DragonCrawl 的内容。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-what-are-large-language-models"&gt;&lt;strong&gt;什么是大型语言模型？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;大语言模型 (LLM) 是人工智能领域的变革性发展，特别是在自然语言处理 (NLP) 领域。从本质上讲，法学硕士是先进的模型，旨在以有意义且与上下文相关的方式理解、解释、生成和参与人类语言。这些模型在由各种来源的文本组成的庞大数据集上进行训练，使它们能够学习自然语言的细微差别、习语和语法。法学硕士最关键的方面之一是它们能够根据输入提示生成连贯且上下文相关的文本。此功能不仅限于简单的文本生成；它扩展到复杂的任务，如回答问题、翻译语言、总结文档，甚至创建诗歌或代码等内容。法学硕士的底层技术通常涉及神经网络架构，例如 Transformer，它们擅长处理顺序数据，并且可以捕获文本中的远程依赖关系。这使得它们对于需要理解较长文本的上下文的任务特别有效。现代大型语言模型是在多种语言上进行训练的，这意味着我们可以使用它们并在其他语言中获得合理的输出。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-did-we-choose-large-language-models-for-mobile-testing"&gt; &lt;strong&gt;为什么我们选择大型语言模型进行移动测试？&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;我们意识到我们可以将移动测试制定为语言生成问题。归根结底，移动测试是一系列步骤，可能会因应用程序、设备等的变化而遇到障碍和/或路线修正。为了成功克服这些障碍并完成测试，我们需要背景和目标，我们将这些提供给自动化系统的最简单方法是通过自然语言。我们向 DragonCrawl 提供当前屏幕的文本表示，以及我们想要执行的测试的目标，然后我们询问它应该做什么。在给定上下文的情况下，它会选择要与之交互的 UI 元素以及如何与之交互。由于这些模型已经过预先训练，并且在英语以外的语言中被证明具有弹性，因此我们可以使用其他语言的文本向 DragonCrawl 询问这些问题。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086897" height="325" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/juan_blog-1-1024x325.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：DragonCrawl 的高级概述。龙的图像由OpenAI的DALL·E生成&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-modeling"&gt;&lt;strong&gt;造型&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;MPNet，即“语言理解的屏蔽和排列预训练”，是自然语言处理中的一种先进方法，它将预训练语言模型中的屏蔽和排列策略相结合。它的工作原理是屏蔽某些单词并改变输入文本中其他单词的顺序，使模型不仅能够学习屏蔽单词的预测，还能学习更广泛的上下文和语言语法。这种双任务方法使 MPNet 能够更深入地理解语言语义，超越了仅专注于掩蔽或排列的传统模型。一旦在大型数据集上进行了训练，MPNet 就可以针对各种 NLP 任务进行微调，由于其对单词级和句子级上下文的全面掌握，从而在理解和生成语言方面提供增强的性能。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/GXXjoxKzdTjJWERTI0nBZMaQVMb4hnarA5_B-c4XoK_GjBwkPn2VeKaM39v62RRcbik-3erep9vkXF8we1npSfj6PK3GxtTGhLRzbfFNtzYe6dpY_g1294hlsgzoLhtTPK1NpSQy2w3ca6BGYbPBpPw" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：DragonCrawl 模型中的 Transformer 层。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-evaluation"&gt;&lt;strong&gt;评估&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在广阔而复杂的语言景观中，单词不仅仅是一串字母；而是一串字母。它们富含意义、上下文和微妙的细微差别，这就是嵌入发挥作用的地方。嵌入就像多维地图，其中每个单词都找到其独特的位置，不仅基于其自身的身份，还基于与其周围单词的关系。通过获得高质量的嵌入，我们确保我们的模型不会将语言视为单词的随机分类，而是将其视为连贯且相互关联的思想和含义结构。&lt;/p&gt;&lt;p&gt;我们将评估定义为检索任务，因为我们最终希望 DragonCrawl 模仿人类检索信​​息和做出决策的方式。就像我们在图书馆选择正确的书籍时付出一些努力一样，DragonCrawl 努力选择正确的行动来实现其目标。 Precision@N 指标类似于当您只能带几本书回家时找到合适的书，它向我们展示了该模型不仅能够检索，而且能够在众多可能性中找出最佳选择。通过 precision@N 测量和提高嵌入质量，我们确保 DragonCrawl 不仅能够理解语言，而且能够以近乎人类般的洞察力来理解语言。&lt;/p&gt;&lt;p&gt;为了为 DragonCrawl 选择正确的模型，我们调整并评估了多个模型。下表总结了我们的发现：&lt;br /&gt;&lt;/p&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;精度@1&lt;/td&gt;&lt;td&gt;精度@2&lt;/td&gt;&lt;td&gt;精度@3&lt;/td&gt;&lt;td&gt;参数&lt;/td&gt;&lt;td&gt;嵌入尺寸&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MPNet（基础）&lt;/td&gt;&lt;td&gt; 0.9723&lt;/td&gt;&lt;td&gt; 0.9623&lt;/td&gt;&lt;td&gt; 0.9423&lt;/td&gt;&lt;td&gt; 110M&lt;/td&gt;&lt;td&gt;第768章&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MPNet（大）&lt;/td&gt;&lt;td&gt; 0.9726&lt;/td&gt;&lt;td&gt; 0.9527&lt;/td&gt;&lt;td&gt; 0.9441&lt;/td&gt;&lt;td&gt; 340M&lt;/td&gt;&lt;td&gt;第768章&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T5&lt;/td&gt;&lt;td&gt; 0.97&lt;/td&gt;&lt;td&gt; 0.9547&lt;/td&gt;&lt;td&gt; 0.9338&lt;/td&gt;&lt;td&gt; 11B&lt;/td&gt;&lt;td&gt; 3584&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;罗伯塔&lt;/td&gt;&lt;td&gt;0.9689&lt;/td&gt;&lt;td&gt; 0.9512&lt;/td&gt;&lt;td&gt; 0.9464&lt;/td&gt;&lt;td&gt; 82M&lt;/td&gt;&lt;td&gt;第768章&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;T5（未调）&lt;/td&gt;&lt;td&gt; 0.9231&lt;/td&gt;&lt;td&gt; 0.9213&lt;/td&gt;&lt;td&gt; 0.9213&lt;/td&gt;&lt;td&gt; 11B&lt;/td&gt;&lt;td&gt; 3584&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;可以看出，所有模型的嵌入质量都很高，但延迟差异很大。最快的模型是基础 MPNet，具有约 110M 参数（从技术上讲，这使其成为中小型语言模型）。此外，它的嵌入大小为 768 维，这将使其他下游系统将来使用我们的嵌入的成本更低。&lt;/p&gt;&lt;p&gt;另一方面，考虑到这些数字，人们可能会说我们甚至不需要调整，但这不是我们选择的。未调整的 T5-11b 为我们提供了良好的精度@1、2 和 3，但考虑到我们计划使用该模型的频率，以及由于 Uber 应用程序不断变化而导致的数据变化，我们很快就会遭受这些额外的影响积分不是由非我们定制的模型提供的。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-challenges"&gt;&lt;strong&gt;挑战&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在开发过程中我们需要克服一些挑战。其中一些是 Uber 特有的，还有一些与大型语言模型的弱点有关。&lt;/p&gt;&lt;p&gt;我们在提出 DragonCrawl 的请求和完成行程流程时早期遇到的一个问题是设置 DragonCrawl 的（假）乘客和司机的 GPS 位置。 Uber 的匹配算法负责将乘客与司机联系起来，非常复杂，并且是为规模化而构建的，甚至考虑了一天中的时间、当前交通状况、未来需求等变量。但是，在使用 DragonCrawl 进行测试时，在任何给定时间，特定城市只会有 1 名乘客和 1 名司机，这不是 Uber 后端所期望的。因此，有时即使乘客和司机紧邻，也无法匹配。为了解决这个问题，我们必须调整骑手和司机的 GPS 位置，这样才能得到满意的结果。这是针对优步和/或网约车和食品配送的。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-adversarial-cases"&gt;&lt;strong&gt;对抗性案件&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在测试 Uber 的出行流程时，在一些城市，我们看到 DragonCrawl 做了一些奇怪的事情。在一些城市，它不再要求定期出行，而是要求定期出行。最让我们困惑的是，在仔细调试我们的工件后，DragonCrawl 实际上具备做出正确选择的所有条件（即触摸“选择 UberX”），但相反，它会选择预定的行程。然后，它将通过用户界面打开日历并选择预定行程的日期和时间，这令人印象深刻，但我们离题了。&lt;/p&gt;&lt;p&gt;上面的例子称为对抗性案例。对抗性案例或对抗性样本的概念在几年前得到了普及，当时研究人员发现，在根本不应该混淆的情况下，模型有可能会被混淆。让我们看一下下面的图片。在下图中，我们展示了如果我们在熊猫图像中添加一点噪声（这会产生几乎相同的熊猫），我们会如何混淆机器学习模型，以至于它会认为它是这样的是一只长臂猿（但我们都知道熊猫看起来不像长臂猿）。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter is-resized"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/xY5iJU7vEjDJEkoUq7KFH4QIEIm9n1ZMqAhHMeBFwh35nF-jMcz2FYbRRqvoMyWoQyQDsWbMqA7-6IlIcN-LApmJMB2W_HIuXrDCV27xpLZn6BGj3XePo29RGs1jVDXEPEwxzdByEgrbNzUlcXuZiPk" style="width: 700px; height: auto;" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：难以察觉的噪声如何欺骗机器学习模型的示例。这不是一个假设的例子， &lt;a href="https://www.technologyreview.com/2019/05/19/135299/how-we-might-protect-ourselves-from-malicious-ai/" rel="noreferrer noopener" target="_blank"&gt;请看一下&lt;/a&gt;。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;虽然不可能完全消除模型在对抗性案例中的弱点，但我们计划进行对抗性训练和验证以降低风险。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-steering-dragoncrawl-to-more-optimal-paths"&gt;&lt;strong&gt;引导 DragonCrawl 走向更优化的路径&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;在我们对 Uber 行程流程的离线测试中，我们看到 DragonCrawl 总是可以请求或完成行程，但有时会花费太长时间。有时，新的弹出窗口会让 DragonCrawl 添加另一位乘客/为其他人预订行程，然后会加载几个屏幕，其中包含 DragonCrawl 必须弄清楚的选项和设置。它会弄清楚它们，但由于需要几个步骤（而不仅仅是 1 或 2 个新步骤），因此需要更长的时间。由于我们的目标是在每次 Android 代码更改时运行 DragonCrawl，因此我们无法承受那些较长的路线，因此我们必须训练 Dragon 说“不”/跳过某些事情并说“是”/确认其他事情。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-hallucinations"&gt;&lt;strong&gt;幻觉&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;最后，一个经常讨论的话题是大型语言模型中的幻觉。用 Meta 副总裁兼首席人工智能科学家 Yann LeCun 的话说，大型语言模型“有时会胡言乱语”（参见&lt;a href="https://futurism.com/the-byte/yann-lecun-large-language-models-fad" rel="noreferrer noopener" target="_blank"&gt;文章&lt;/a&gt;）。事实上，我们需要注意的是，我们不能完全信任大型语言模型，或者至少不能没有护栏。在本节中，我们将讨论我们为防止幻觉伤害 DragonCrawl 而设置的护栏。&lt;/p&gt;&lt;p&gt;首先，DragonCrawl 的最大优势之一是它使用更小的模型。我们的模型大小为 110M 个参数，比流行的 GPT-3.5/4 小大约 3 个数量级。因此，这大大降低了它可以输出的答案的可变性和复杂性。换句话说，模型大小限制了模型的无意义。&lt;/p&gt;&lt;p&gt;即便如此，我们仍然收到了一些无效的输出，以下是我们处理它们的方法：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;部分无效操作：&lt;/strong&gt;模型可能会返回某些信息不正确的响应。例如，对于可滑动的 UI 元素，它可能会返回“touch”；或者它可能会输出正确的操作和正确的位置，但会混淆 UI 元素的名称（即 request_trip_button）。对于任何一种情况，由于我们可以从模拟器中读取有效的操作、正确的 UI 元素名称等，因此我们可以解决诸如前面提到的混淆。模拟器为我们提供了基本事实，我们可以使用它来根据 UI 元素的名称找到正确的操作；给定 UI 元素名称的正确位置；甚至是正确的 UI 元素名称（给定正确的位置）。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;完全无效的操作：&lt;/strong&gt;对于完全无效的操作，我们会将之前建议的操作附加到提示中，并指出它是无效的。这将导致模型建议采取不同的操作。对于无效操作持续存在的情况，我们将回溯并重试模型中的建议。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;循环/重复操作：&lt;/strong&gt;我们可能会陷入循环（即在提要中上下滚动）或重复操作（即重复等待）。我们通过跟踪特定序列中已采取的操作甚至屏幕截图来处理这种情况，因此很容易判断我们是否处于循环中。另外，由于 DragonCrawl 输出建议列表，我们可以尝试其他建议的操作。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity is-style-default" /&gt;&lt;h2 class="wp-block-heading" id="h-dragoncrawl-in-action"&gt;龙爬行在行动&lt;/h2&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter is-resized"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/Jwm8lAVWCTAaQWhF_TMwOPa1NghHQJybRrqTWTrc5_HgHErdHudy8fRP4KlCzvokvruiQvBUHMsadMaO7cwumQQwvf3jCNm18TWnAtOo6jqC9r-AcoFJAwHuUiEltRSurmepasnaGAgkYeFIcbo333o" style="width: 273px; height: auto;" /&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;我们已经看到 DragonCrawl 做了令人惊奇的事情，但在本节中，我们将讨论两个给我们留下深刻印象的场景。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-dragoncrawl-goes-online-in-australia"&gt; &lt;strong&gt;DragonCrawl在澳大利亚上线！&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt; 2023 年 10 月，我们在澳大利亚布里斯班通过 DragonCrawl 测试 Uber 的出行流程，并看到了一些意想不到的情况。 DragonCrawl 的假司机资料设置得很完美，但这一次，它在大约 5 分钟内无法上线。在那5分钟里，DragonCrawl反复按下“GO”上线按钮，直到最终上线。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086898" height="576" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/juan_blog-4-1024x576.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图4：DragonCrawl在澳大利亚布里斯班尝试5分钟后上线。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;我们感到惊喜。 DragonCrawl 是如此以目标为导向，以至于它经历了不友好的用户体验来实现其目标：上网、匹配（假）骑手，并进行假设的旅行。由于完成时间有限，我们知道我们必须进行调查。我们还了解到，正如下面详细讨论的那样，DragonCrawl 不会因轻微或不可重现的错误而失败，例如影响我们基于脚本的 QA 的错误。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-the-ultimate-solution-turn-it-off-and-then-turn-it-back-on"&gt;&lt;strong&gt;最终解决方案：将其关闭，然后重新打开&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;那是2023年9月，我们看到龙做了一件如此聪明的事，我们不知道是该笑还是该鼓掌。 Dragon 正在巴黎测试 Uber 的出行流程。它选择前往巴黎机场（CDG），当它到达屏幕选择付款方式时，付款方式未加载（很可能是我们使用的帐户中出现了问题）。龙做了什么？它关闭了应用程序，打开它，然后再次请求行程。第二次没有任何问题，龙完成了去机场的目的。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086900" height="576" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/juan_blog-5-1024x576.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：DragonCrawl 重新启动应用程序以请求行程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;很难用言语表达看到 DragonCrawl 做这些事情我们是多么兴奋和自豪。反复按下“上线”按钮只是为了能够使用 Uber 开车，或者打开和关闭应用程序以便它能够到达它想要的位置，这使得 DragonCrawl 比我们旧的基于脚本的测试模型更能适应较小的技术问题。&lt;/p&gt;&lt;p&gt;我们观察到，再多的代码也无法与 DragonCrawl 所展示的面向目标的行为相媲美，它所代表的开发人员生产力是令人兴奋的。可以创建符合 DragonCrawl 策略的脚本，但是需要编写多少数千（甚至数百万）行代码？在需要时更新所有这些代码的成本有多高？现在，想象一下当传统测试遇到我们刚才描述的场景时会发生什么：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;正常运行的驾驶员帐户在 5 分钟内无法上线：&lt;/strong&gt;如果测试团队没有发出警报，这会引起人们的注意。我们甚至可能认为出现了中断，这会提醒多个工程师，但实际上，这是一个暂时的问题。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;付款方式未加载：&lt;/strong&gt;票证将被归档并处于最高优先级。这将引发多次对话、检查，并且会尝试重现该问题，但这只是昙花一现。 &lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-dragoncrawl-running-on-uber-s-ci"&gt; &lt;strong&gt;DragonCrawl 运行在 Uber 的 CI 上&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;我们生产了我们的模型以及自 2023 年 10 月左右开始使用该模型的 CI 管道，并在年底取得了一些成果。截至 2024 年 1 月，DragonCrawl 每晚在 5 个不同城市执行一次核心行程流程，并且在将 Rider 和 Driver Android 应用程序发布给我们的客户之前也执行这些流程。自推出以来，我们观察到以下情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;高稳定性：&lt;/strong&gt; DragonCrawl 在 2023 年 11 月和 12 月执行的流程稳定性超过 99%。 Dragon 失败的罕见情况是由于我们使用的第三方系统的中断，以及由于高优先级错误导致的真正中断没有其他移动测试工具检测到。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;无需维护：&lt;/strong&gt;我们不需要手动更新和/或维护 DragonCrawl。每当应用程序发生变化时，DragonCrawl 就会弄清楚如何通过这些变化来实现其目标，这与我们的软件测试人员团队不同，他们在 2023 年花费了数百个小时维护测试用例。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;高可重用性：&lt;/strong&gt;我们对 89 个顶级城市的 DragonCrawl 进行了评估，DragonCrawl 在其中 85 个城市成功请求并完成了行程。这是 Uber 首次在全球 85 个城市成功执行像请求和完成行程这样复杂的移动测试，而无需调整代码。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;设备/操作系统弹性：&lt;/strong&gt;我们在 CI 中使用 3 种不同的 Android 设备和 3 个不同的操作系统版本测试了 Uber 的行程流程，我们甚至还改变了其他参数，例如可用磁盘、CPU 等。DragonCrawl 成功请求并完成了跨区域的行程所有这些组合都无需对我们的代码或模型进行调整，这在传统的移动测试中并不总是能得到保证。调整测试以处理不同的屏幕尺寸/分辨率和其他设备细节是传统移动测试的一个众所周知的麻烦。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-what-s-next"&gt;&lt;strong&gt;下一步是什么？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;我们在 2023 年奠定的基础为激动人心的 2024 年及以后铺平了道路。我们对较小语言模型的投资产生了具有非常高质量嵌入的基础模型，以至于它解锁了如下所示的架构： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086902" height="576" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/juan_blog-6-1024x576.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：未来移动测试作为由 Dragon 基础模型 (DFM) 提供支持的 RAG 应用程序&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;借助 Dragon 基础模型 (DFM)，我们可以使用小型数据集（数百或数十个数据点）和 DFM 来创建 RAG（检索增强生成）应用程序，更准确地模拟人类如何与我们的应用程序交互。那些较小的数据集（带有口头目标和偏好）会告诉 DragonCrawl 要优化什么，而这就是它所需要的。 DFM 可能是一个法学硕士，但它实际上是一个奖励模型，通过采取行动来实现其目标，正如我们所看到的，其中一些行动模仿了真人会做的事情。&lt;/p&gt;&lt;p&gt;到 2024 年，我们的一大投资领域将是构建子系统，使开发人员能够将测试构建为 RAG，并获得在许多城市、语言中完美执行的好处，并且维护成本极低（甚至为零） 。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;随着生成式人工智能在过去 4-6 个月中取得的所有进步，还有更多的事情需要评估，以改进我们的模型和应用程序的质量。我们计划评估更现代的大型语言模型，以进一步提高我们模型的质量。模型质量的每一次提高都会增加我们可以测试的组合，减少用户遇到的错误，从而提高生产力，使开发人员能够构建新的体验，并为 DragonCrawl 提供更多的测试内容。这是一个随着模型质量启动并加速的飞轮，我们将为这种加速提供动力。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/DFEnnSB2E5rc241fUWUqJVypOJkESM0sMISfkqL-Qtq3B4gP212JD-Lw9AqaP0pRkzpo_RO25TyEBxG6NuJmqy4aRCq9XQ93l7zbEBZKfUUJriwCZAo-00dz56fReF7GrGD1eijSgGiFrx8j6ARmN0o" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 7：模型质量的飞轮。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-acknowledgments"&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;像 DragonCrawl 这样复杂的事情，无需我们合作伙伴团队的帮助。我们非常感谢 Uber 的 CI、Mobile Foundations、Michelangelo、Mobile Release 和 Test 帐户。我们还要感谢创建&lt;a href="https://arxiv.org/abs/2004.09297"&gt;MPNet&lt;/a&gt; （我们使用的）、T5 和其他法学硕士的热情研究人员，感谢他们对该领域的贡献，并帮助其他人推进自己的领域。我们还要感谢&lt;a href="https://www.linkedin.com/in/dan-tsui-24624b6/"&gt;Daniel Tsui&lt;/a&gt;和&lt;a href="https://www.linkedin.com/in/sowjanyapuligadda/"&gt;Sowjanya Puligadda&lt;/a&gt;的领导、建议和持续支持，最后还要感谢&lt;a href="https://www.linkedin.com/in/srabontichakraborty/"&gt;Srabonti Chakraborti&lt;/a&gt;和我们的前实习生&lt;a href="https://www.linkedin.com/in/gustavonazarioperez/"&gt;Gustavo Nazario&lt;/a&gt; ，他们帮助我们将 DragonCrawl 变成了今天的样子。&lt;/p&gt;&lt;p class="has-small-font-size"&gt;封面照片归属：该图像是使用 OpenAI 的 DALL·E 生成的。&lt;/p&gt;</description><pubDate>Tue, 23 Apr 2024 05:00:00 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/generative-ai-for-high-quality-mobile-testing/</guid></item><item><title>【Ensuring Precision and Integrity: A Deep Dive into Uber’s Accounting Data Testing Strategies】确保准确性和完整性：深入探讨 Uber 的会计数据测试策略</title><link>https://www.uber.com/blog/accounting-data-testing-strategies/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;优步在全球不同地区经营多种业务。财务会计服务 (FAS) 平台（ &lt;a href="https://www.uber.com/en-IN/blog/ubers-finance-computation-platform/" rel="noreferrer noopener" target="_blank"&gt;详细架构&lt;/a&gt;）负责这些全球区域的财务会计，其设计遵循以下原则：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;遵守&lt;/li&gt;&lt;li&gt;可审计性&lt;/li&gt;&lt;li&gt;准确性&lt;/li&gt;&lt;li&gt;可扩展性&lt;/li&gt;&lt;li&gt;分析&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了维护这些原则，FAS 建立了强大的测试、监控和警报流程。这包括系统配置、业务会计和外部财务报告生成。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-challenges"&gt;挑战&lt;/h2&gt;&lt;p&gt;Uber 的财务会计服务平台在互联网规模上运营——每天大约 15 亿个日记账分录 (JE)，每天通过 ETL 和数据处理处理 1.2 亿笔交易，吞吐量为每秒 2,500 次查询（平均）。标准的现成会计系统无法支持我们不断发展的平台的交易规模和范围。此外，我们还出于会计目的管理来自超过 25 种不同服务的数据。为了处理这种规模的数据，我们的工程系统被设计为在事件级别和批处理模式下处理数据。当数据流经架构中的多个组件时，需要确保所有组件的设计都遵循上述定义的原则。&lt;/p&gt;&lt;p&gt;到 2023 年，该平台处理的年度预订和结算总额约为 120 亿美元以上。其交易规模（每年约 800 亿笔金融微交易）是旅行规模的 10 倍，目前提供 99.6% 的交易，并通过自动收入计算99.99% 的完整性、准确性保证和可审核性。我们进行了 600 多项业务变更，以支持 2023 年的业务扩展。该平台处理大数据并在 Schemaless 和 Apache Hive &lt;sup&gt;TM&lt;/sup&gt;中存储 PB 级数据。&lt;/p&gt;&lt;p&gt;会计流程有多个步骤，每个步骤都需要进行验证以遵守原则。以下是执行验证的各个步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;业务需求验证&lt;/li&gt;&lt;li&gt;会计入职&lt;/li&gt;&lt;li&gt;会计执行&lt;/li&gt;&lt;li&gt;报告生成&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了坚持我们既定的原则，我们在财务会计服务的各个阶段实施制衡：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;需求签核&lt;/li&gt;&lt;li&gt;回归测试&lt;/li&gt;&lt;li&gt;集成测试&lt;/li&gt;&lt;li&gt;UAT 验证&lt;ul&gt;&lt;li&gt;账本验证&lt;/li&gt;&lt;li&gt;交易级验证&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;影子验证&lt;/li&gt;&lt;li&gt;部署&lt;ul&gt;&lt;li&gt;金丝雀&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;健康检查&lt;ul&gt;&lt;li&gt;审核员检查&lt;/li&gt;&lt;li&gt;完整性检查&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;警报/监控&lt;/li&gt;&lt;li&gt;报告生成&lt;/li&gt;&lt;/ol&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-validations-life-cycle-of-accounting-processes-nbsp"&gt;会计流程的验证生命周期&lt;/h2&gt;&lt;p&gt;当数据流经金融科技系统的各个组件时，每个阶段都会进行制衡，以便系统和流程遵守原则。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-requirements-signoff"&gt;需求签核&lt;/h3&gt;&lt;p&gt;根据在不同国家/地区运营的业务模式和当地团队的期望，提供并跟踪需求。会计要求是根据&lt;a href="https://en.wikipedia.org/wiki/Generally_Accepted_Accounting_Principles_(United_States)"&gt;公认会计原则 (GAAP)&lt;/a&gt;提供的。然后将这些要求纳入我们的会计系统。 Uber 的金融科技系统拥有验证需求的内部工具，可以执行超过 15 项自动检查来验证预期输出。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-regression-testing"&gt;回归测试&lt;/h2&gt;&lt;h3 class="wp-block-heading" id="h-unit-testing"&gt;单元测试&lt;/h3&gt;&lt;p&gt;Uber 金融服务中的单元测试对于确保我们应用程序的准确性、安全性和可靠性至关重要。这些测试涉及隔离小部分代码并按预期验证功能。在优步，我们努力通过严格测试每个单元的正确运行来尽早发现并纠正错误，并确保从交易处理到财务报告的整体服务平稳、安全地运行。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-regression-kaptres"&gt;回归——Kaptres&lt;/h3&gt;&lt;p&gt; Kaptre（Capt &lt;s&gt;ure&lt;/s&gt; — Re &lt;s&gt;play&lt;/s&gt; ）是一种捕获和重放测试工具，主要用于功能和回归测试目的。以下是我们的金融系统如何使用其关键组件的详细说明：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;测试用例：&lt;/strong&gt;对于任何会计变更，至少一个新的 Kaptre 测试会添加到测试套件中，以便在连续运行中测试所有用例。每个测试用例包括输入（与 UAT 使用的相同）、期望和断言。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;捕获模式：&lt;/strong&gt;添加测试时，我们在“捕获模式”下运行。此模式执行新添加的测试的记账流程，并捕获在离线模式下重新执行所需的依赖项，例如来自上游的 API 请求/响应和预期的记账日记条目 (JElines)。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;重播模式：&lt;/strong&gt;后续测试运行涉及在重播模式下运行 Kaptre 回归测试套件。此模式使用最新的代码/配置版本创建新的输出，并且断言将它们与捕获的期望进行比较。如果断言失败，则会报告测试失败。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;捕获的响应的更改触发器：&lt;/strong&gt;捕获的响应会随着上游系统的更改、金融交易中的新字段添加或预期的会计更改而发生变化。验证后可以使用上述捕获模式更新这些测试。&lt;/p&gt;&lt;p&gt;这种方法确保回归测试准确反映捕获模式期间系统的行为，并随后在连续测试运行中对照预期结果进行验证。该设计可以适应上游、金融科技系统的变化和预期的会计修改，同时保持测试过程的完整性并降低测试过程中人为错误的风险。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/KFCn1YyYUZwB8_9hTsWv-M6EyBi-1rlFZuv3vnhyQmXFA2bLDQZ-7EIdWZuU_qteUyF9UBev3u0bJVNoKnQoroZrPcjFFPOcJI_RqNGDs7bIwDInlnZ3_R_CiCFKtbH0RcUwzsceuy9H4mTXpoWwY-Q" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：Kaptre（捕获重放框架的功能）。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-slate-short-lived-application-testing-environment-nbsp"&gt; SLATE（短期应用程序测试环境）&lt;/h3&gt;&lt;p&gt;在部署到生产环境之前在短期应用程序环境（又名 SLATE）中进行测试是 Uber 软件开发生命周期中的关键一步。 SLATE 测试帮助我们及早发现和解决问题，并降低将缺陷/问题引入生产环境的风险。在 SLATE 中执行各种类型的测试，包括集成、性能和安全测试。此测试的主要目的是在类似生产的环境中运行应用程序，在开发周期的早期识别和检测问题（例如运行时错误），并防止缺陷传播到更高的环境。&lt;/p&gt;&lt;p&gt;在&lt;a href="https://www.uber.com/en-IN/blog/simplifying-developer-testing-through-slate/" rel="noreferrer noopener" target="_blank"&gt;Slate Uber Eng 博客&lt;/a&gt;中查找更多详细信息。&lt;/p&gt;&lt;p&gt;总之，在短期应用程序环境中进行测试是一种最佳实践，有助于在将服务部署到生产之前提高服务的整体质量、可靠性和安全性。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-integration-testing-nbsp"&gt;集成测试&lt;/h2&gt;&lt;p&gt;Uber 的财务会计服务与众多上游系统（30 多个）合作，以丰富生成会计交易所必需的行程详细信息。集成测试对于金融系统和上游组件之间的无缝通信、识别接口问题并实现早期风险缓解至关重要。&lt;/p&gt;&lt;p&gt;然而，集成测试的一个显着挑战在于确定完整性。与具有明确的代码覆盖率指标的单元测试不同，集成测试缺乏对要覆盖的场景的洞察，并且没有用于衡量集成测试覆盖率的既定指标。这种差距导致依赖团队无法自动获知正在启动的新场景，并且缺乏理解所有场景的测试覆盖率的指标。&lt;/p&gt;&lt;p&gt;为了解决这个问题，我们开发了一个内部工具，可以自动检测、通知和确认所有依赖系统的就绪情况。该工具旨在确保无缺陷启动，并提供衡量集成测试覆盖率的机制。&lt;/p&gt;&lt;p&gt;这在收入系统中变得尤为重要，该系统位于数据流的终点并与多个服务交互。在这种情况下，意外的启动可能会造成会计流程中断的风险。例如，票价发布如果缺乏适当的沟通，可能会被路由到死信队列 (DLQ)，从而由于收入系统中的入职不足而导致会计不正确。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-uat-validations"&gt; UAT 验证&lt;/h2&gt;&lt;p&gt;用户验收测试 (UAT) 是 Uber 财务系统开发中的强制性步骤，会计团队严格验证财务报告的准确性。我们通过涵盖积极和消极场景的自动化测试对聚合和交易级分类账进行全面验证，从而简化了这一流程。这确保了资产负债表、损益表和其他关键财务报表的完整性。这种细致的方法可确保更新和补丁的无缝集成，而不会破坏现有功能，并在签收前进行超过 15 项质量检查。&lt;/p&gt;&lt;p&gt;根据要求设置会计配置后，会计团队就会对其进行验证，最终得到正式签字，表明批准并证明准确性和合规性。业务规则配置更改遵循严格的协议，在合并到主系统之前需要获得主要会计利益相关者的明确授权。 Uber 利用自动化的 Buildkite 作业来确保完整性和效率，并在发现代码库差异时系统地检查必要的批准。这种自动化增强了审批流程的严格性。&lt;/p&gt;&lt;p&gt;如果更改与既定协议相矛盾或绕过强制批准，则会立即提出自动标记以进行彻底审查。这种保护措施对于维护系统的完整性和合规性至关重要。&lt;/p&gt;&lt;p&gt; Uber 的财务系统采用两种主要类型的验证来确保最大的准确性和可靠性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;示例验证&lt;/li&gt;&lt;li&gt;账本验证&lt;/li&gt;&lt;/ul&gt;&lt;h3 class="wp-block-heading" id="h-sample-validations"&gt;示例验证&lt;/h3&gt;&lt;p&gt;对选定的一组样本订单执行验证，这些样本订单被选择来代表生产环境中的订单场景。在对金融系统进行增量改变时，这些验证通常是足够的。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large is-resized"&gt;&lt;img alt="" class="wp-image-1086095" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-2.17.10%E2%80%AFPM-1024x185.png" style="width: 747px; height: auto;" /&gt;&lt;figcaption class="wp-element-caption"&gt;表 1：交易级别验证/示例验证。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-ledger-validations"&gt;账本验证&lt;/h3&gt;&lt;p&gt;对于在国家或业务层面影响大量用例的变更，我们还执行账本验证以获得完整性保证。在我们在生产中实施这些变更之前，这些验证在特定时间内在总体水平上提供了额外的保证。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086096" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-2.17.42%E2%80%AFPM-1024x375.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;表 2：总账验证。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;这两种验证类型都是 Uber 致力于维持财务准确性和监管合规性最高标准的承诺的组成部分。他们协同工作，确保财务体系保持稳健、可靠，并反映公司的真实财务状况。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-shadow-validations"&gt;影子验证&lt;/h3&gt;&lt;p&gt;影子测试的目的是作为最终检查点，以在将构建转移到生产环境之前捕获任何潜在问题。它本质上是一种构建认证策略，有助于做出有关部署候选版本 (RC) 的明智决策。核心流程包括通过 RC 传递生产流量，并将其输出与当前生产版本的输出进行比较，以发现任何异常情况。&lt;/p&gt;&lt;p&gt;影子测试由三部分组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;捕获生产请求&lt;/strong&gt;：有多种策略可以实现这一目标。其中之一需要记录生产流量中的（请求、响应）对，以便与 RC 进行比较。&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;重放生产流量&lt;/strong&gt;：这些捕获的请求在 RC 上重放，并将响应与生产环境中的响应进行比较。记录任何差异以供进一步分析。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;分析差异&lt;/strong&gt;：包括对记录的差异进行彻底检查，以确定 RC 的置信水平。此步骤对于验证构建是否已准备好部署至关重要。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1086099" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-2.18.00%E2%80%AFPM-1024x455.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;表 3：预生产环境和生产环境之间的影子验证。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-challenges-and-solutions"&gt;挑战与解决方案&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;流量和上游调用&lt;/strong&gt;：由于金融科技服务每秒处理超过 10K+ 事件，根据影子构建重放所有这些流量是不切实际的，并且可能会因大量上游调用而导致速率限制问题。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;流量采样和负载分配&lt;/strong&gt;：为了缓解这一问题，我们采用了对一小部分流量（例如 10%）进行采样并在较长时间段（例如 5 小时）内分发重播的策略。这减少了每秒的调用次数，但也限制了测试范围。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了解决这个问题，我们缓存上游调用和响应，而不是进行实时网络调用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;缓存上游调用&lt;/strong&gt;：我们为上游（请求、响应）对实现了缓存机制，以避免重放期间的冗余调用，但代价是增加了存储费用。我们通过在旧数据被清理后保持 x 天的保留期来最大限度地减少增加的存储成本。我们总是根据最新的数据集进行回放。&lt;/li&gt;&lt;li&gt;这种方法缺乏检测由上游变化引起的实时问题的能力。为了缓解这个问题，我们正在开发一种对流量和负载分配进行采样的机制。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter is-resized"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/laVFJXmguJ5XutZD365Hu3SdXUMpGvotZ8HUAA4_OWYtYVcItW3zl7eJoF33tD9jVk1WuvBWkDlP4rQVZnPl75wcf_6mvIqg_Rq_fBR2FJg9hrFbCT-sTPkC1nG6KyTrlSRG-acIn4mPrBWU3OrsuFM" style="width: 700px; height: auto;" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：采样和存储用于影子测试的事件。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-replaying-captured-requests"&gt;重放捕获的请求&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;我们开发了专门的工作流程，用于针对 RC 重放给定时间戳范围内的存储请求。记录存储的响应和 RC 响应之间的差异以供分析。&lt;/li&gt;&lt;/ul&gt;&lt;h3 class="wp-block-heading" id="h-validating-and-analyzing-differences"&gt;验证和分析差异&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;此阶段涉及仔细检查已发现的差异。目的是区分预期差异和异常。这需要深入了解Banker系统的响应结构。&lt;/li&gt;&lt;li&gt;银行家的响应结构：银行家的输出是一系列称为“交易”的复杂数据类型，每个数据类型代表具有贷方、借方、总账帐户和业务范围等属性的多个日记帐分录。 &lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="" class="wp-image-1086101" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-2.18.19%E2%80%AFPM-1024x459.png" /&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;对建设的信心是通过货币差异偏离预定阈值的程度来衡量的。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-example-scenario-and-analysis"&gt;示例场景和分析&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;事务比较：&lt;/strong&gt;比较主构建和影子构建的事务。差异记录在数据存储中。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数据存储属性：&lt;/strong&gt;记录的数据包括事务、其源（主/影子）、重播工作流的 RunID 和时间戳。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;详细分析：&lt;/strong&gt;我们针对 LineOfBusiness 和 GlAccountNumber 等特定领域的异常情况进行分析，使用查询来识别货币差异。根据这些发现调整构建信心。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;例如，考虑 E1 是我们针对生产和发布候选者处理的事件。 TxnP 是我们从 Production 实例获得的交易，TxnS 是我们从 Release Candidate 获得的交易： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="" class="wp-image-1086104" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-2.18.29%E2%80%AFPM-1024x205.png" /&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt; TxnP 和 TxnS 在 LineOfBusiness 中的差异。因此，我们会将它们记录到数据存储中进行分析。我们的数据存储将包含四个属性：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;交易 -&amp;gt; 交易数组。&lt;/li&gt;&lt;li&gt;源 -&amp;gt; 主要/阴影。 （即）如果交易来自生产或影子构建。&lt;/li&gt;&lt;li&gt; RunID -&amp;gt; 重播工作流的 RunID。由于我们可以使用不同的构建运行多个影子测试工作流程，因此我们应该确保仅分析特定工作流程的差异。&lt;/li&gt;&lt;li&gt;时间戳。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;重放 E1 后，我们的数据存储将包含以下两条新记录：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; （TxnP、主要、运行 ID、当前时间戳）&lt;/li&gt;&lt;li&gt; （TxnS、Shadow、runID、当前时间戳）&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们可以通过多种方式来分析这些差异。对于我们的用例，我们最感兴趣的是捕获 LineOfBusiness 和 GlAccountNumer 中的任何异常情况。因此，我们编写了查询来识别这些 LOB 和 GlAccountNumber 维度上的生产候选者和发布候选者之间的货币差异。如果贷方或借方的货币差异超过一定阈值，我们将降低构建的置信度。偏离阈值越远，构建的置信度就越低。&lt;/p&gt;&lt;p&gt;对于上面的示例，LOB 差异的影子测试报告将如下所示： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="" class="wp-image-1086105" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Screenshot-2024-04-12-at-3.07.22%E2%80%AFPM-1024x166.png" /&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;假设我们为每个业务线定义了 1,000 美元的门槛。在这种情况下，100 的差异仍然远低于阈值，因此我们构建的置信度不会受到影响。&lt;/p&gt;&lt;p&gt;通过采用这种影子测试策略，您可以确保对候选版本进行全面彻底的评估。这种有条不紊的方法不仅可以识别潜在问题，还可以提供改进未来构建所需的见解，最终有助于部署过程的稳健性和可靠性。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/lWpk884yU_92bIFwEr29JfVH_HZpEPd6pXf2XMJl1hg42l9JLX39RzczjifWlo9nAjQRFFd9eu54LZgKKOvBigmIohSreFIAK_bTaHqPy_tFpgtQfVpXkS4B256K2gDy_sdQ3BSbP2DHwmAuvKEpAcY" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：影子测试工作流程。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-deployment-nbsp"&gt;部署&lt;/h2&gt;&lt;h3 class="wp-block-heading" id="h-canary-testing-nbsp"&gt;金丝雀测试&lt;/h3&gt;&lt;p&gt;在 Uber 的财务会计服务中，每天都会部署构建。为了确保每个构建的成功部署并最大限度地减少性能下降、错误率增加和资源耗尽等问题的影响，纳入金丝雀部署至关重要。该策略有助于控制释放，防止对整个流量产生直接影响。它可以在完成部署之前识别并解决潜在问题。&lt;/p&gt;&lt;p&gt;采用金丝雀发布方法来测试真实流量（&amp;lt;=2% 流量），影响最小。当新版本准备好部署时，金丝雀区域将作为初始部署目标。如果在此部署期间出现任何错误或问题，构建不会传播到其他生产区域，从而防止大范围中断并确保更受控的发布过程。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-deployment-monitoring-and-alerting"&gt;部署监控和警报&lt;/h3&gt;&lt;p&gt;财务会计服务平台消耗来自各种上游来源的数据。为了监控服务的运行状况，我们配置了多个指标和警报。跟踪指标并配置警报以暂停部署管道，如果我们在部署一段指定时间后收到警报，则将其回滚。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-dead-letter-queues"&gt;死信队列&lt;/h4&gt;&lt;p&gt;DLQ（死信队列）存储由于错误而未处理的事件。 DLQ 计数升高表明存在错误代码、损坏事件、上游服务问题或速率限制等问题。每个消息队列都有一个对应的DLQ，用于处理无法处理的事件。理想情况下，DLQ 必须有零个事件。我们使用基于阈值的警报来检测问题并在警报触发时调查根本原因。我们将所有错误（包括事件详细信息）记录到专用的 Apache Kafka &lt;sup&gt;Ⓡ&lt;/sup&gt;主题，并将它们提取到 Apache Hive &lt;sup&gt;TM&lt;/sup&gt;表中。我们配置了 Data Studio 仪表板来监控 Apache Hive &lt;sup&gt;TM&lt;/sup&gt;表，提供有关 DLQ 事件的影响、计数、新鲜度和趋势的见解。这种数据驱动的方法有助于快速识别问题并确定问题的优先级，以进行根本原因分析和系统改进。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-alerts-and-monitors"&gt;警报和监视器&lt;/h4&gt;&lt;p&gt;警报主要用于标记需要立即查看的紧急和关键问题。监视器是配置警报的仪表板。警报应该始终是可操作的。还建议使用相应的操作手册标记每个警报。&lt;/p&gt;&lt;p&gt;仅我们团队就配置了大约 400 多个警报，涵盖广泛的维度，包括但不限于 DLQ 计数、消息队列的消费者滞后、服务可用性等。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-completeness-checks"&gt;完整性检查&lt;/h4&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/TD2qCI_24RcKTtUAX12d2re1uJHo_00oLC6ItT3kfSvQFwEVGW4P20F71vlxJ_R4GkRxQrkNq7c9sRqGBx18KCfe_FDmGplL4j36asRsFeR8V1MaeKjFRX5zYV6Hx54BT7COj16ZgQI0AxNQh-xGfOA" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：完整性检查&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;金融服务收到的所有事件都需要考虑在内。当我们的服务收到事件时，它会记录在接收记录器中。处理事件的自然结果是以下之一&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;事件已处理&lt;/strong&gt;- 记录在事实表中&lt;/li&gt;&lt;li&gt;&lt;strong&gt;过滤或忽略的事件&lt;/strong&gt;– 记录在过滤记录器中&lt;/li&gt;&lt;li&gt;&lt;strong&gt;事件处理出错&lt;/strong&gt;– 记录在错误记录器中&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了确保所有传入事件都得到考虑，我们执行完整性检查。这些检查确认接收记录器中记录的所有事件也记录在错误记录器、过滤器记录器或事实表中（指示成功处理）。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-results"&gt;&lt;strong&gt;结果&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;利用上述测试和检测策略，我们的团队在 2023 年实现了一个非凡的里程碑：将会计事件数量减少到零。这一重大成就反映了我们对财务管理准确性和效率的致力于。此外，手工日记账分录显着减少，这是会计错误减少的直接结果。这一改进不仅使每月会计账簿能够及时关闭，而且还增强了我们在会计领域管理多个项目的信心，吞吐量提高了 17%。这些进步体现了我们对会计实践卓越性和可靠性的承诺。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;总之，事实证明，Uber 财务会计服务 (FAS) 采用的全面、多方面的金融科技测试策略取得了巨大成功。通过每一步严格的验证流程（从业务需求验证到报告生成），并采用回归测试、SLATE、集成测试和影子验证等先进技术，Uber 为财务系统的可靠性和准确性设立了新标准。&lt;/p&gt;&lt;p&gt;创新的解决方案解决了处理大量交易和数据的挑战，这些解决方案不仅可以满足当前的需求，还可以扩展以适应未来的增长。细致的测试和验证方法，加上金丝雀测试和警惕的监控和警报系统等部署策略，体现了 Uber 对维持金融技术最高标准的承诺。明年，我们将在测试策略中添加更多功能，以支持不良输入的检测和自动更正，以支持无错误的自助服务之旅。&lt;/p&gt;&lt;p&gt; Uber 完善其金融科技测试策略的历程为业内其他公司树立了标杆，强调了在不断发展的金融技术领域持续创新和严格测试的重要性。&lt;/p&gt;</description><pubDate>Thu, 18 Apr 2024 05:00:00 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/accounting-data-testing-strategies/</guid></item><item><title>【Migrating a Trillion Entries of Uber’s Ledger Data from DynamoDB to LedgerStore】将 Uber 的一万亿条账本数据从 DynamoDB 迁移到 LedgerStore</title><link>https://www.uber.com/blog/migrating-from-dynamodb-to-ledgerstore/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;上周，我们探索了&lt;a href="https://www.uber.com/blog/how-ledgerstore-supports-trillions-of-indexes/" rel="noreferrer noopener" target="_blank"&gt;LedgerStore&lt;/a&gt; (LSG)——Uber 的仅追加、分类账式数据库。本周，我们将深入探讨如何将 Uber 的关键业务账本数据迁移到 LSG。我们将详细介绍如何在不造成中断的情况下透明地移动超过一万亿个条目（构成几 PB 的数据），并且我们将讨论在迁移过程中学到的东西。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-history"&gt;历史&lt;/h3&gt;&lt;p&gt;湾流是 Uber 的支付平台。它&lt;a href="https://www.uber.com/blog/payments-platform" rel="noreferrer noopener" target="_blank"&gt;于 2017 年推出，&lt;/a&gt;使用 DynamoDB 进行存储。以 Uber 的规模，DynamoDB 变得昂贵。因此，我们开始在 DynamoDB 中仅保留 12 周的数据（即热数据），并开始使用 Uber 的 blobstore TerraBlob 来存储较旧的数据（即冷数据）。 TerraBlob 类似于 AWS S3。&lt;/p&gt;&lt;p&gt;对于长期解决方案，我们想使用&lt;a href="https://www.uber.com/blog/dynamodb-to-docstore-migration/" rel="noreferrer noopener" target="_blank"&gt;LSG&lt;/a&gt; 。它是专门为存储支付方式数据而构建的。其主要特点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;它是可验证的不可变的（即，您可以使用加密签名检查记录是否未被更改）&lt;/li&gt;&lt;li&gt;分层存储以管理成本（热数据保存在最适合服务请求的位置，冷数据存储在针对存储优化的位置）&lt;/li&gt;&lt;li&gt;更好的延迟以实现最终一致的二级索引&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，到 2021 年，湾流将使用 DynamoDB、TerraBlob 和 LSG 的组合来存储数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt; DynamoDB 过去 12 周的数据&lt;/li&gt;&lt;li&gt;TerraBlob，Uber 的内部 Blob 存储，用于存储冷数据&lt;/li&gt;&lt;li&gt;LSG，我们在其中写入数据，并希望迁移到它&lt;/li&gt;&lt;/ul&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-why-migrate"&gt;为什么要迁移？&lt;/h2&gt;&lt;p&gt;由于 LSG 的不变性，它更适合存储分类帐类型的数据。迁移到 LSG 可以显着节省经常性成本。&lt;/p&gt;&lt;p&gt;从三个存储变为单个存储将简化负责与存储交互和创建索引的湾流服务的代码和设计。这反过来又使得服务的理解和维护变得容易。&lt;/p&gt;&lt;p&gt; LSG 承诺更短的索引延迟（即写入记录和创建辅助索引之间的时间）。此外，它还会给我们带来更快的网络延迟，因为它是在 Uber 数据中心内本地运行的。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/ImSq9jhPp_-uGFsvBU0tmJA1MiY42lmKUH1fLoNJ036l2w7W9uLx89xRnBO30l7lRkHXQ0XQv_flDWeZB0pfRyYzczFfayP_Vi4j217OZt8fG5WxpM2FKdt3lB34s0emy0gp6nUKNfS2q7MR5z8ZUbc" /&gt;&lt;figcaption class="wp-element-caption"&gt;图1：迁移前后的数据流向&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-nature-of-data-amp-associated-risk"&gt;数据的性质和相关风险&lt;/h2&gt;&lt;p&gt;我们要迁移的数据是自 2017 年以来 Uber 所有业务的所有 Uber 账本数据：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;不可变记录 – 1.2 PB 压缩大小&lt;/li&gt;&lt;li&gt;二级索引 – 0.5 PB 未压缩大小&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不可变记录不应被修改。因此，出于所有实际目的，一旦我们编写了记录，就无法更改。我们确实可以灵活地修改二级索引数据来纠正问题。&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-checks"&gt;支票&lt;/h2&gt;&lt;p&gt;为了确保回填在各个方面都是正确且可接受的，我们需要检查我们是否可以处理当前流量以及当前未访问的数据是否正确。其标准是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;完整性：所有记录均已回填。&lt;/li&gt;&lt;li&gt;正确性：所有记录均正确。&lt;/li&gt;&lt;li&gt;负载：LSG 应能够处理当前负载。&lt;/li&gt;&lt;li&gt;延迟：LSG 的 P99 延迟在可接受的范围内。&lt;/li&gt;&lt;li&gt;滞后：二级索引是在后台创建的。我们希望确保索引创建过程的延迟在可接受的范围内。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;检查是使用&lt;em&gt;影子验证&lt;/em&gt;和&lt;em&gt;离线验证&lt;/em&gt;相结合的方式完成的。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-shadow-validation"&gt;影子验证&lt;/h3&gt;&lt;p&gt;这将我们在迁移之前返回的响应与以 LSG 作为数据源返回的响应进行了比较。这有助于我们确保当前的流量不会因数据迁移问题或代码错误而中断。根据影子验证的测量，我们希望回填的完成度和正确率至少达到 99.99%。我们还设定了 99.9999% 的上限。设置上限的原因是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在迁移历史数据时，总会存在数据损坏的问题。有时这是因为在服务的初始开发期间数据写入不正确。由于规模的原因，也可能会出现数据损坏的情况。例如，S3 提供 11 个 9 的持久性保证，那么您可以预期 1 万亿条记录中会出现 10 次损坏。&lt;/li&gt;&lt;li&gt;索引最终是一致的，这意味着一些记录会在几秒钟后出现。因此，影子验证会将它们标记为丢失。这是大规模出现的误报。&lt;/li&gt;&lt;li&gt;对于 6 个 9，您必须查看 1 亿次比较的数据才能可靠地给出结果。这意味着，如果您的影子验证每秒比较 1,000 条记录，那么您需要等待一天以上才能收集足够的数据。如果有 7 个 9，则需要等待 12 天。实际上，这将导致项目陷入停滞。&lt;/li&gt;&lt;li&gt;有了明确定义的上限，您就不必被迫查看您怀疑的每个潜在问题。假设问题的出现次数是上限的 1/10，您甚至不需要调查它。&lt;/li&gt;&lt;li&gt;如果有 6 个 9，我们最终可能会得到略多于 100 万条的损坏记录。  尽管确认 6 个 9 的正确性可能会给公司带来实际成本，但该项目所节省的成本超过了潜在成本。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在影子验证期间，您实质上是在 LSG 上复制生产流量。因此，通过监控 LSG，我们可以验证它是否可以处理我们的生产流量，同时满足我们的延迟和滞后要求。它让我们对为访问 LSG 数据而编写的代码充满信心。此外，它还让我们对数据的完整性和正确性有一定的信心，特别是当前正在访问的数据。我们开发了一个通用影子验证代码，可以在迁移的不同部分多次重复使用。&lt;/p&gt;&lt;p&gt;在迁移过程中，我们发现了由于不同部分存在多个错误而导致的延迟和滞后问题，并修复了这些问题。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分区键优化以更好地分布索引数据&lt;/li&gt;&lt;li&gt;索引问题导致扫描记录而不是点查找&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;不幸的是，实时影子验证无法为我们很少访问的历史数据语料库提供强有力的保证。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-offline-validation-amp-incremental-backfill"&gt;离线验证和增量回填&lt;/h3&gt;&lt;p&gt;这会将来自 LSG 的完整数据与来自 DynamoDB 的数据转储进行比较。由于各种数据问题，您必须跳过不良记录以确保您的回填能够通过。此外，回填作业本身也可能存在错误。离线验证可确保数据回填正确发生并覆盖完整数据。除了影子验证之外，还必须执行此操作，因为实时流量往往只访问最近的数据。因此，如果不经常访问的冷数据中潜伏着任何问题，影子验证将无法捕获它。&lt;/p&gt;&lt;p&gt;离线验证的关键挑战是数据大小。我们处理的最大数据大小为 70 TB 压缩数据（估计未压缩数据为 300 TB），并且我们在单个作业中比较了 7600 亿条记录。这种类型的 Apache Spark &lt;sup&gt;TM&lt;/sup&gt;作业需要数据混洗，而&lt;a href="https://www.uber.com/blog/ubers-highly-scalable-and-distributed-shuffle-as-a-service/"&gt;分布式混洗即 Spark 服务&lt;/a&gt;与动态资源分配和推测执行相结合，让我们在资源限制下以合理的速度准确地做到这一点。&lt;/p&gt;&lt;p&gt;离线验证发现缺失记录，其输出用于增量回填。我们在离线验证和回填之间进行迭代，以确保所有记录都已写入。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-backfill-issues"&gt;回填问题&lt;/h2&gt;&lt;p&gt;每次回填都有风险。我们使用 Uber 内部提供的 Apache Spark 进行回填。以下是我们遇到的不同问题以及我们如何处理这些问题。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-scalability"&gt;可扩展性&lt;/h3&gt;&lt;p&gt;您希望从小规模开始，逐渐扩大规模，直到达到系统的极限。如果您只是盲目地超越这一点，那么您实际上就是在对自己的系统发起 DDoS 攻击。此时，您想要找到瓶颈，解决它，然后扩大您的工作规模。大多数时候，这只是扩大下游服务的问题，有时可能会更复杂。无论哪种情况，您都不希望将回填作业扩展到超出系统瓶颈的能力。最好以小增量的方式进行扩展，并在每次扩展后密切监控。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-incremental-backfills"&gt;增量回填&lt;/h3&gt;&lt;p&gt;当您尝试在 3 个月内回填 3 年的数据时，您生成的流量将是正常流量负载的 10 倍，并且系统可能无法处理此流量。例如，当您的生产通常处理 1K/秒的速率时，您将需要 120 天以 10K/秒的速率回填 100B 记录。因此，您可以预期系统会过载。即使回填作业极有可能导致持续出现问题，您也必须将其关闭。因此，期望回填作业能够一次性从开始运行到结束是不现实的，因此您必须增量运行回填。&lt;/p&gt;&lt;p&gt;一种简单而有效的方法是将回填分成小批量，可以一点一点地完成，这样每批都可以在几分钟内完成。由于您的作业可能会在批处理中关闭，因此它必须是幂等的。每次完成一个批处理时，您都希望将统计信息（例如读取的记录、回填的记录等）转储到文件中。随着回填的继续，您可以汇总其中的数字以检查进度。&lt;/p&gt;&lt;p&gt;如果您可以删除或更新现有记录，则可以降低回填期间出现错误和代码错误的风险和成本。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rate-control"&gt;速率控制&lt;/h3&gt;&lt;p&gt;为了安全回填，您需要确保回填作业行为一致。因此，您的作业应该具有可以轻松调整以放大或缩小的速率控制。在 Java/Scala 中，您可以使用 Guava 的 RateLimiter。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-dynamic-rate-control"&gt;动态速率控制&lt;/h3&gt;&lt;p&gt;在某些情况下，当生产流量较少时，您可能可以加快速度。为此，您需要监视系统的当前状态并查看是否可以加快速度。我们根据&lt;a href="https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease" rel="noreferrer noopener" target="_blank"&gt;加法增加/乘法减少&lt;/a&gt;来调整 RPS。为了安全起见，我们对交通仍然有一个上限。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-emergency-stop"&gt;紧急停止&lt;/h3&gt;&lt;p&gt;迁移过程需要能够快速停止回填，以防出现中断甚至怀疑过载。停电期间的任何回填都必须停止，这既是预防措施，也是潜在的噪音源。即使在断电后，随着系统恢复，系统也往往会承受额外的负载。能够停止回填还有助于调试与规模相关的问题。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-size-of-data-file"&gt;数据文件大小&lt;/h3&gt;&lt;p&gt;转储数据时，将文件大小保持在 1GB 左右，两侧具有 10 倍的灵活性。如果文件太大，您会遇到&lt;a href="https://kb.databricks.com/cloud/s3-part-number-limit.html" rel="noreferrer noopener" target="_blank"&gt;不同工具的 MultiPart 限制&lt;/a&gt;等问题。如果您的文件很小，那么您的文件太多，甚至列出它们也将花费大量时间。在 shell 中运行命令时，您甚至可能开始达到 ARGMAX 限制。这变得足够重要，以确保每次对数据执行某些操作时，它都会应用于所有文件，而不仅仅是其中的一些文件。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-fault-tolerance"&gt;容错能力&lt;/h3&gt;&lt;p&gt;所有回填作业都需要某种数据转换。当您这样做时，您不可避免地会遇到数据质量/损坏问题。您不能每次发生这种情况时都停止回填作业，因为此类不良记录往往是随机分布的。但您也不能忽略它们，因为这也可能是由于代码错误造成的。为了解决这个问题，您可以单独转储有问题的记录并监视统计数据。如果失败率很高，那么您可以手动停止回填，修复问题，然后继续。否则，让回填继续并并行查看故障。&lt;/p&gt;&lt;p&gt;记录未写入的另一个原因是 RPC 超时。您可以重试，但在某些时候，无论出于何种原因，您都必须放弃并继续前进，以确保您能够取得进展。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-logging"&gt;记录&lt;/h3&gt;&lt;p&gt;在回填期间记录日志以帮助调试和监控进度是很诱人的，但这可能是不可能的，因为这会给日志基础设施带来压力。即使您可以保留日志，也会有太多的日志数据需要保留。解决方案是使用速率限制器来限制您生成的日志数量。您只需对产生大部分日志的部分进行速率限制。如果错误很少发生，您甚至可以选择记录所有错误。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/xWcB-v0gyFB4920hZx1tevZiHiSLhUKPvA7TZMvkCN6bsEmh5bZiTcZ0xYumbfjsgsG6Oz-Xnl85XeLhD4ofUc07poJ1OnsB4WlNCEyZzYmY9kuvfgCkSxzC4nSvqcBEmYQvANytw4oOyXA4wyQDias" /&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-mitigating-risk"&gt;降低风险&lt;/h2&gt;&lt;p&gt;除了分析来自不同验证和回填统计数据的数据外，我们对 LSG 的推出也持保守态度。我们在几周内推出了它，并得到了我们服务的主要呼叫者的值班工程师的批准。我们最初推出了回退（即，如果在 LSG 中找不到数据，我们将尝试从 DynamoDB 获取数据）。在删除回退之前，我们查看了回退日志。对于回退日志中标记为丢失的每条记录，我们检查了 LSG 以确保它并未真正丢失。即使在那之后，我们仍将 DynamoDB 数据保留了一个月，然后才停止向其中写入数据、进行最终备份并删除该表。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/P7cnF5dxZX7H5rA82nfZgC1ICBQX7Q928jAexep0GBSR39-B2kDw44hHTtEQQuBNmqw9ZeFz_KYY39uqlUBSEErrb2XWhWagcpWKrsb8tiDX_6CYfOXACQst7Wak7mIewxy4WvI3gW3Vza3altpn0Tc" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：LSG 推出&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;在本文中，我们介绍了将大量业务关键型货币数据从一个数据存储迁移到另一个数据存储的过程。我们涵盖了迁移的不同方面，包括迁移标准、检查、回填问题和安全性。我们能够在两年内完成此迁移，在迁移期间或迁移后没有任何停机或中断。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-acknowledgments"&gt;致谢&lt;/h3&gt;&lt;p&gt;感谢 Amit Garg 和 Youxian Chen 帮助我们将数据从 TerraBlob 迁移到 LSG。感谢 LSG 团队的 Jaydeepkumar Chovatia、Kaushik Devarajaiah 和 Rashmi Gupta 在整个工作中对我们的支持。感谢李梦涵为&lt;a href="https://www.uber.com/en-EG/blog/cashless-payments-with-uber-cash/" rel="noreferrer noopener" target="_blank"&gt;Uber Cash&lt;/a&gt;的账本迁移数据。&lt;/p&gt;&lt;p class="has-small-font-size"&gt;封面照片归属： &lt;a href="https://www.flickr.com/photos/51986662@N05"&gt;USFWS Mountain Prairie&lt;/a&gt;拍摄的“&lt;a href="https://www.flickr.com/photos/51986662@N05/51912457870"&gt;休伦湿地管理区日落时的水禽迁徙&lt;/a&gt;”，标有&lt;a href="https://creativecommons.org/publicdomain/mark/1.0/?ref=openverse"&gt;公共领域标记 1.0&lt;/a&gt; 。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Amazon Web Services、AWS 和 Powered by AWS 徽标是 Amazon.com, Inc. 或其附属公司的商标。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Apache®、Apache SparkTM 和 SparkTM 是 Apache Software Foundation 在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/p&gt;</description><pubDate>Thu, 11 Apr 2024 05:30:00 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/migrating-from-dynamodb-to-ledgerstore/</guid></item><item><title>【How LedgerStore Supports Trillions of Indexes at Uber】LedgerStore 如何支持 Uber 的数万亿个索引</title><link>https://www.uber.com/blog/how-ledgerstore-supports-trillions-of-indexes/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;介绍&lt;/h1&gt;&lt;p&gt;Uber 将物理世界和数字世界连接起来，只需轻按一下按钮即可实现移动。每个季度，Uber 都会为收入者、消费者和商家进行数十亿次出行、送货和数百亿次金融交易。 LedgerStore 是 Uber 的一个不可变存储解决方案，提供可验证的数据完整性和正确性保证，以确保这些交易的数据完整性。&lt;br /&gt;&lt;br /&gt;考虑到账本是 Uber 任何财务事件或数据移动的真实来源，因此能够通过索引从各种访问模式中查找账本非常重要。这就需要&lt;strong&gt;数万亿&lt;/strong&gt;个索引来索引数千亿的账本。之前的一篇&lt;a href="https://www.uber.com/en-US/blog/dynamodb-to-docstore-migration/" rel="noreferrer noopener" target="_blank"&gt;博文&lt;/a&gt;讨论了 LedgerStore 的背景以及存储后端是如何重新架构的。本博客介绍了 LedgerStore 索引及其架构的重要性，该架构为数万亿个索引提供支持，并具有 PB 级索引存储空间。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-types-of-indexes"&gt;索引类型&lt;/h1&gt;&lt;p&gt;账本需要支持各种类型的索引。让我们探索它们以及相应的用例。&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-strongly-consistent-indexes"&gt;索引强一致&lt;/h2&gt;&lt;p&gt;其中一个用例是当乘客/食客使用 Uber 时处理信用卡授权流程。在优步行程开始时，乘客/乘客的信用卡上会被冻结。该保留应转换为费用或作废，具体取决于行程是进行还是取消，如下所示。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084314" height="929" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig1-e1711742633458.png" width="1539" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：强一致性指数支持的 Uber Trip 信用卡支付流程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;如果为保留服务的索引不是强一致的&lt;em&gt;，&lt;/em&gt;则读取时保留可能需要一段时间才能可见。这样做的结果是，可能会在用户的信用卡上重复收费，而原始保留仍保留在信用卡上。&lt;/p&gt;&lt;p&gt;现在，让我们深入研究如何构建强一致性索引，以确保一旦执行记录写入，任何后续读取都保证看到与该记录对应的索引。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-write-path"&gt;写入路径&lt;/h3&gt;&lt;p&gt;为了构建强一致性索引，我们使用两阶段提交来确保索引始终与记录强一致，如下所示。&lt;/p&gt;&lt;p&gt;插入操作从记录写入之前的索引意向写入开始。如果记录写入成功，这些意图将在记录写入操作后提交，并且这是异步完成的，以避免影响最终用户插入延迟。如果索引意图写入成功，但记录写入失败，则需要回滚索引意图，否则会导致未使用意图的累积，这将在读取时处理，如下所示。&lt;/p&gt;&lt;p&gt;需要注意的是，如果索引意图写入失败，则整个插入操作都会失败，因为我们无法保证索引与记录的一致性。因此，只有当用例强烈需要时才需要考虑强一致性索引。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084320" height="640" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig2-e1711742801993.png" width="1260" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 2：强一致索引的两阶段提交写入流程。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-read-path"&gt;读取路径&lt;/h3&gt;&lt;p&gt;有两种情况索引可以在插入后处于意向状态：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;索引意向提交操作在写入路径中失败或&lt;/li&gt;&lt;li&gt;如果记录写入失败&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;此类意图通过提交或删除在读取路径上进行处理。当对这些索引进行读取时，如果索引处于 Intent 状态，则会读取相应的记录。如果记录存在，则提交索引，否则回滚。这些操作异步发生，以免影响最终用户的读取延迟。一般来说，只有很小一部分索引最终处于意图状态。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084321" height="912" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig3-e1711742960693.png" width="1180" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 3：强一致索引的读取流程。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-eventually-consistent-indexes"&gt;最终一致的索引&lt;/h2&gt;&lt;p&gt;并非所有索引都需要强大的“读即写”保证。这种索引的一个例子是支付历史页面，其中，只要支付出现在页面上，几秒的延迟是可以接受的。&lt;/p&gt;&lt;p&gt;虽然强一致性索引提供了&lt;em&gt;读你所写的&lt;/em&gt;保证，但它们在某些情况下并不适合，因为它们会权衡以下属性来实现此保证：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;更高的写入延迟&lt;/strong&gt;&lt;br /&gt;由于索引意向写入操作和相应的记录写入必须是串行的，才能为记录提供索引的强一致性保证&lt;/li&gt;&lt;li&gt;&lt;strong&gt;可用性较低&lt;/strong&gt;&lt;br /&gt;任何一个索引意图的写入失败都意味着整个写入应该失败，否则索引将与相应的记录不一致&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;与强一致索引相比，最终一致索引在这方面是相反的，因为它们是由与在线写入路径完全隔离的单独进程在后台构建的。因此，它们不会遭受&lt;em&gt;较高的写入延迟&lt;/em&gt;或导致 LedgerStore 服务的潜在&lt;em&gt;可用性较低&lt;/em&gt;。我们利用我们自己开发的&lt;a href="https://www.uber.com/blog/schemaless-sql-database/" rel="noreferrer noopener" target="_blank"&gt;Docstore&lt;/a&gt;数据库中名为“物化视图”的功能来生成这些索引。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084322" height="1760" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig4-e1711743013811.png" width="1820" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：最终一致索引提供的支付历史记录。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-time-range-indexes"&gt;时间范围索引&lt;/h2&gt;&lt;p&gt;账本由于其不可变的性质，会随着时间的推移而不断增长，从而增加其存储成本。因此，在 Uber，我们将时间范围内的旧账本分批卸载到更便宜的冷存储中。&lt;/p&gt;&lt;p&gt;每个账本都与一个称为业务或事件时间戳的时间戳相关联。为了将账本卸载到冷存储（也为了密封数据），我们需要一类索引来查询事件时间范围批次中的数据。该索引的不同之处在于，数据是在确定的时间范围批次中读取的，其数量级高于上述两种索引类型。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-full is-resized"&gt;&lt;img alt="" class="wp-image-1084325" height="591" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig5-e1711743069138.png" style="width: 701px; height: auto;" width="1020" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：数据分层中使用的时间范围索引。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;以下是如何在分类账上进行时间范围查询的示例：&lt;/p&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;从 LEDGER_TABLE 中选择 *&lt;/strong&gt; &lt;strong&gt;，其中&lt;/strong&gt;LedgerTime&lt;strong&gt;介于&lt;/strong&gt;1701252000000&lt;strong&gt;和&lt;/strong&gt;1701253800000 之间&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class="wp-block-table"&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;分类帐&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;账本时间&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;{行程开始}&lt;/td&gt;&lt;td&gt;上午 10:01&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; {行程已完成且票价已调整}&lt;/td&gt;&lt;td&gt;上午 10:15&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt; {行程后更正}&lt;/td&gt;&lt;td&gt;中午 12:01&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;有几种方法可以在分布式数据库中对此进行建模。我们将深入探讨在 Amazon DynamoDB 与 Docstore 数据库之上开发时间范围索引之间的主要区别。 DynamoDB 和 Docstore 都是分布式数据库，都提供数据建模构造作为分区和排序键。前者用于根据数据的值在分区之间均匀分布数据，后者用于控制数据的排序顺序。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-design-with-dynamodb"&gt;使用 DynamoDB 进行设计&lt;/h3&gt;&lt;p&gt;Dynamodb 提供两种管理表读/写&lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html" rel="noreferrer noopener" target="_blank"&gt;容量&lt;/a&gt;的方法。我们使用预配置模式，因为流量不会太&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html" rel="noreferrer noopener" target="_blank"&gt;突发，&lt;/a&gt;不需要按需模式。预配模式配置了&lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html" rel="noreferrer noopener" target="_blank"&gt;自动缩放功能&lt;/a&gt;，可根据流量模式调整容量。&lt;/p&gt;&lt;p&gt;正如我们从上面的写入模式中注意到的，分类账时间通常与当前挂钟时间相关。因此，这些值往往聚集在当前时间附近。如果我们基于&lt;strong&gt;G&lt;/strong&gt;个时间单位粒度对数据进行分区，那么&lt;strong&gt;G 个&lt;/strong&gt;时间单位中的所有写入都将进入同一个物理分区，从而导致热分区。 DynamoDB 在&lt;a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/" rel="noreferrer noopener" target="_blank"&gt;热分区&lt;/a&gt;的情况下对吞吐量有限制，导致写入请求受到限制，这在在线写入路径中是不可接受的。假设 1K 峰值 Uber 行程/秒，即使 G=1 秒也不是一个好值，因为它对应于 1K WCU（写入容量单位），这是发生&lt;a href="https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/" rel="noreferrer noopener" target="_blank"&gt;限制&lt;/a&gt;之前允许的 QPS 峰值。&lt;/p&gt;&lt;p&gt;虽然看起来我们可以使分区更细粒度，但它仍然不是万无一失的，因为随着时间的推移，流量的增加可能会导致不稳定。这样做的另一个副作用是通过分散-聚集执行的累积读取量增加。因此，我们对 DynamoDB 所做的操作如下：&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-write-optimized-temporary-index-table-called-buffer-index"&gt; 写优化临时索引表（称为缓冲区索引）&lt;/h4&gt;&lt;p&gt;所有在线时间索引写入都会写入缓冲区索引表。插入的索引项根据相应记录的哈希模划分到&lt;strong&gt;&lt;em&gt;M 个&lt;/em&gt;&lt;/strong&gt;唯一的存储桶中，以在缓冲区索引表中的分区之间&lt;a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html" rel="noreferrer noopener" target="_blank"&gt;均匀分配&lt;/a&gt;负载，从而提高&lt;em&gt;写入效率&lt;/em&gt;。选择&lt;strong&gt;&lt;em&gt;M&lt;/em&gt;&lt;/strong&gt;的值，使其足够高，以便每个分区的负载量避免过度拆分。它也被选择得足够低，以限制分散-聚集的数量&lt;em&gt; &lt;/em&gt;在读取期间执行。&lt;/p&gt;&lt;h4 class="wp-block-heading" id="h-read-optimized-permanent-index-table"&gt;读取优化的永久索引表&lt;/h4&gt;&lt;p&gt;对缓冲表进行分散&lt;em&gt;聚集&lt;/em&gt;读取的需要使得它们的读取效率不高，并且由于读取可能发生在表的整个生命周期中，因此我们需要对其进行优化。这就带来了对读取高效的永久索引表的需求。&lt;/p&gt;&lt;p&gt;永久时间范围索引表根据与特定持续时间&lt;strong&gt;&lt;em&gt;N&lt;/em&gt;&lt;/strong&gt; （例如 10 分钟）对齐的时间戳进行分区。缓冲表中的索引定期批量写入永久索引表。由于写入是在后台批量完成的，所以这里的任何写入节流都不会影响线上流量。批处理的另一个优点是写入流量可以跨分区分布，减少热分区。缓冲区索引表在将其索引卸载到永久索引表后将被删除，因为不再需要它们。对永久索引表的读取以&lt;strong&gt;&lt;em&gt;N&lt;/em&gt;&lt;/strong&gt;分钟为间隔完成，没有任何分散-聚集，使得该表的&lt;em&gt;读取效率很高&lt;/em&gt;。&lt;br /&gt;&lt;br /&gt;以下是 DynamoDB 情况下的时间范围索引流的描述。双表设计带来了状态管理和协调的需要，因此读取也会转到正确的索引表。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084328" height="1669" src="https://blog.uber-cdn.com/cdn-cgi/image/width=920,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig6-e1711743514668.png" width="1499" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：Dynamodb 上的时间范围索引设计。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-design-with-docstore"&gt;使用 Docstore 进行设计&lt;/h3&gt;&lt;p&gt;DynamoDB 的双表设计运行良好，可以处理高吞吐量，但在操作上带来了挑战。如果临时缓冲表没有及时创建，可能会因为无法接受写入而导致写入失败，这在过去曾引起过可用性问题。作为成本效率的一部分，我们将索引存储后端从 DynamoDB 重新架构为 Uber 的&lt;a href="https://www.uber.com/blog/schemaless-sql-database/" rel="noreferrer noopener" target="_blank"&gt;Docstore&lt;/a&gt;数据库。作为此重新架构的一部分，我们还通过利用两个 Docstore 属性改进了时间范围索引设计，以克服维护两个表的缺点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt; &lt;a href="https://www.uber.com/blog/schemaless-sql-database/" rel="noreferrer noopener" target="_blank"&gt;Docstore&lt;/a&gt;是一个构建在 MySQL 之上的分布式数据库，固定数量的分片分配给可变数量的物理分区。随着数据大小的增长，物理分区的数量增加，一些现有的分片被重新分配给新的分区，从而导致物理分区的数量达到最大&lt;strong&gt;上限&lt;/strong&gt;。&lt;/li&gt;&lt;li&gt; Docstore中的数据以主键（分区+排序键）的&lt;strong&gt;排序&lt;/strong&gt;方式存储。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们只维护一张时间范围索引表，其中索引条目根据完整时间戳值进行分区。由于时间戳非常细粒度，因此不存在热分区（因此不存在写入限制），因为大多数写入均匀分布在分区之间。&lt;/p&gt;&lt;p&gt;读取涉及对表的每个分片进行前缀扫描，直至达到特定的时间粒度。前缀扫描与表的常规扫描非常相似，不同之处在于每个扫描请求的边界由应用程序控制。因此，在下面的示例中，要读取 30 分钟的数据，可以从 2023–02-03 01:00:00 到 2023–02-03 01:10:00 每隔 10 分钟间隔读取一次，类似地对接下来的两个子窗口重复此操作。由于数据是按主键排序的，因此具有给定边界的前缀扫描可确保仅读取这些时间戳内的数据。&lt;/p&gt;&lt;p&gt;然后执行分散-聚集，然后执行跨分片的排序合并，以按排序的方式获取给定窗口中的所有时间范围索引条目。由于Docstore中分片的数量是固定的，因此我们可以精确地确定（并限制）需要执行的读取请求的数量。同样的技术不适用于 DynamoDB，因为随着表大小的增加，分区数量不断增加。这显着简化了设计并降低了时间索引的运营维护成本。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084329" height="964" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig7-e1711743568767.png" width="1080" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 7：Docstore 上的时间范围索引设计。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-index-lifecycle-management"&gt;索引生命周期管理&lt;/h2&gt;&lt;p&gt;定期定义新索引，并且也可以修改某些索引以发展用例。为了以最小的努力支持这一点并且不导致任何回归，我们需要一种机制来管理索引生命周期。以下是相同的组件：&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-index-lifecycle-state-machine"&gt;索引生命周期状态机&lt;/h3&gt;&lt;p&gt;该组件协调索引的生命周期，包括创建索引表、用历史索引条目回填索引表、验证它们的完整性、将旧索引与新索引交换以进行读/写，以及停用旧索引。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084330" height="660" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig8-e1711743614440.png" width="1800" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 8：索引生命周期状态机。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-historical-index-data-backfill"&gt;历史指数数据回填&lt;/h3&gt;&lt;p&gt;根据业务用例，需要定义新索引，并且必须回填历史索引条目以使其完整。该组件根据卸载到冷数据存储的历史数据构建索引，并以可扩展的方式将其回填到存储层。考虑到数据下载速度高于数据处理速度，该组件以可重用的方式构建了可配置的速率限制和批处理，因为我们可以将实际处理逻辑作为批处理器插件插入。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084331" height="821" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig9-e1711743676584.png" width="1420" /&gt;&lt;figcaption class="wp-element-caption"&gt;图9：为回填索引而定制的历史数据处理模块。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-index-validation"&gt;索引验证&lt;/h3&gt;&lt;p&gt;索引回填后，需要验证索引的完整性。这是通过离线作业来完成的，该作业在一定的时间窗口粒度上计算顺序无关的校验和，并在真实数据源和索引表之间进行比较。此步骤可识别索引回填过程中的任何错误，因为即使丢失了一个条目，该时间窗口的聚合校验和也将导致不匹配。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1084332" height="560" src="https://blog.uber-cdn.com/cdn-cgi/image/width=1024,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/04/Fig10-e1711743725757.png" width="1640" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 10：索引完整性验证。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-highlights"&gt;强调&lt;/h2&gt;&lt;p&gt;这是我们衡量这个关键项目成功与否的方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;我们构建了超过 2 万亿个唯一索引，到目前为止，尚未检测到任何数据不一致，新架构已投入生产 6 个多月。&lt;/li&gt;&lt;li&gt;鉴于资金流动对 Uber 的重要性，在回填期间没有发现任何生产事故。&lt;/li&gt;&lt;li&gt;我们还将所有这些索引从 DynamoDB 移至 Docstore。因此该项目还实现了技术整合，减少了外部依赖。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;从业务影响的角度来看，由于 DynamoDB 支出减少，运营 LedgerStore 现在非常划算。预计每年可节省超过 600 万美元。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;结论&lt;/h1&gt;&lt;p&gt;账本是 Uber 资金流动事件的真相来源。我们构建的强大索引平台支持访问各种业务用例的真相分类账来源，我们期待将来在该平台上支持更多索引。&lt;/p&gt;&lt;p&gt;我们想总结一些关键要点： 在 OLTP 系统中维护 PB 级索引会带来一定的挑战，例如分区不平衡、读/写放大高、邻居噪声问题等。因此数据建模和隔离非常重要设计这些系统时要考虑的方面。此外，根据底层用于存储的实际数据库，设计方法可能会有很大不同，正如我们从两个不同分布式数据库上的时间范围索引的设计对比中看到的那样。&lt;/p&gt;&lt;p&gt;下周加入我们，观看 LedgerStore 系列的第二部分，我们将记录从 DynamoDB 到 LedgerStore 的迁移。&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-acknowledgments"&gt;致谢&lt;/h2&gt;&lt;p&gt;如果没有以下团队的合作，这个项目就不可能实现，这些团队体现了&lt;a href="https://www.uber.com/in/en/careers/values/" rel="noreferrer noopener" target="_blank"&gt;Uber 的多项价值观&lt;/a&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.uber.com/blog/payments-platform/" rel="noreferrer noopener" target="_blank"&gt;湾流&lt;/a&gt;团队与 LedgerStore 团队密切合作，以实现共同目标并迁移到 LedgerStore 平台，这是一个多年项目。&lt;/li&gt;&lt;li&gt; Docstore 团队，负责不断发展&lt;a href="https://www.uber.com/en-IN/blog/schemaless-sql-database/" rel="noreferrer noopener" target="_blank"&gt;Docstore&lt;/a&gt;以满足 LedgerStore 索引的大规模需求。&lt;/li&gt;&lt;li&gt; LedgerStore 团队负责领导、构建和推动分类账索引的大规模采用。&lt;/li&gt;&lt;/ul&gt;&lt;p class="has-small-font-size"&gt; &lt;em&gt;Amazon Web Services、AWS、Powered by AWS 徽标和 Amazon DynamoDB 是 Amazon.com, Inc. 或其附属公司的商标。&lt;/em&gt;&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 05:30:00 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/how-ledgerstore-supports-trillions-of-indexes/</guid></item><item><title>【Scaling AI/ML Infrastructure at Uber】扩展 Uber 的 AI/ML 基础设施</title><link>https://www.uber.com/blog/scaling-ai-ml-infrastructure-at-uber/</link><description>&lt;h1 class="wp-block-heading" id="h-introduction"&gt;&lt;strong&gt;介绍&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;自 2016 年我们首次开始为司机-乘客匹配和定价团队使用复杂的基于规则的机器学习模型以来，机器学习 (ML) 正在庆祝其在 Uber 的第八个年头。从那时起，我们取得了重大进展，转向采用深度学习学习模型是当今大多数关键业务应用程序的核心，同时积极探索生成式人工智能模型提供的可能性。随着 AI/ML 模型的复杂性和规模不断激增，对高效基础设施来有效支持这些模型的需求不断增长。在过去的几年里，我们战略性地实施了一系列以 CPU 和 GPU 为中心的基础设施解决方案，以动态扩展我们的系统并满足不断变化的 ML 用例环境。这一演变涉及定制硬件 SKU、软件库增强、各种分布式训练框架的集成以及对我们的端到端 Michaelangelo 平台的持续改进。这些迭代改进是由我们一路走来的经验教训以及对行业趋势和 Uber 发展轨迹的不断调整推动的，所有这些都是为了满足我们的合作伙伴和客户不断变化的需求。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-goal-and-key-metrics"&gt;&lt;strong&gt;目标和关键指标&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;当我们开始从内部部署向云基础设施的过渡（我们于 2023 年 2 月&lt;a href="https://www.wsj.com/articles/uber-signs-cloud-deals-with-google-and-oracle-b45a9372" rel="noreferrer noopener" target="_blank"&gt;宣布）&lt;/a&gt;时，我们跨团队的硬件/软件协同设计和协作是由以下具体目标驱动的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;最大限度地利用现有基础设施&lt;/li&gt;&lt;li&gt;为新兴工作负载建立新系统，例如生成式人工智能&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了实现这些目标，我们概述了指导我们进步的独特关键结果和指标。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;可行性和可靠性：&lt;/strong&gt;机器学习用户期望在预期的时间范围内（根据复杂性，可以是几周或几个月）成功地融合他们的训练任务，并且不会出现错误。例如，训练 Falcon 180B™ 等更大、更复杂的模型可能需要数月时间，而较长的训练持续时间会增加出现可靠性问题的可能性。因此，我们的目标是为所有培训依赖项实现 99% 的正常运行时间 SLA，以确保一致且可靠的结果。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;效率：&lt;/strong&gt;我们对效率的关注涉及对不同 GPU 配置进行彻底的基准测试，并评估针对特定工作负载定制的本地和云 SKU 的性价比。我们使用模型触发器利用率 (MFU) 等指标来衡量训练效率，以保证最佳的 GPU 利用率。我们的目标是防止 GPU 闲置，通过反应式扩展在服务非高峰时段机会性地使用训练作业，并保持高利用率以最大限度地提高资源效率。我们希望做到这一点的同时也保持不同用户之间资源共享的公平性。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;开发人员速度：&lt;/strong&gt;该指标通过我们的工程师在特定时间范围内可以进行的实验数量来量化。我们优先考虑成熟的生态系统来提高开发人员的速度，确保我们的团队高效工作以提供最佳解决方案。这种方法不仅简化了我们最先进的模型的生产过程，而且还减少了这一过渡所需的时间。&lt;/p&gt;&lt;p&gt;接下来是我们为使本地和云基础设施中的培训和服务部署高效且可扩展而采取的各种举措的结果摘要： &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-optimizing-existing-on-prem-infrastructure"&gt;&lt;strong&gt;优化现有的本地基础设施&lt;/strong&gt;&lt;/h2&gt;&lt;h3 class="wp-block-heading" id="h-federation-of-batch-jobs"&gt;&lt;br /&gt;批处理作业联合会：&lt;/h3&gt;&lt;p&gt;我们的 GPU 资产分布在各个可用区和区域的多个&lt;a href="https://kubernetes.io/" rel="noreferrer noopener" target="_blank"&gt;Kubernetes&lt;/a&gt; ™ 集群中。这种分布主要是由于 GPU 可用性和单个 Kubernetes 集群内的节点数量限制。这种安排带来了两个主要挑战：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;向机器学习工程师展示基础设施的具体细节。&lt;/li&gt;&lt;li&gt;由于静态分配，跨集群的资源利用率不一致。虽然我们在每个集群内都有有效的资源共享系统，但我们缺乏集群间调度的能力。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了解决这些问题，我们为批量工作负载创建了一个统一的联合层，包括&lt;a href="https://www.ray.io/" rel="noreferrer noopener" target="_blank"&gt;Ray&lt;/a&gt; ™ 和 Apache &lt;a href="https://spark.apache.org/" rel="noreferrer noopener" target="_blank"&gt;Spark&lt;/a&gt; ™，称为&lt;strong&gt;Michelangelo Job Controller&lt;/strong&gt; 。该组件充当所有工作负载调度的集中式接口，隐藏底层 Kubernetes 集群，并根据各种策略（负载感知、bin-pack）分配工作负载，包括计算和数据关联性考虑因素。我们计划在后续的博客文章中分享更多相关技术细节。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/BQBvxC9TSCSgAj8ZlAAL8Dkydbx3B__KT9nHfrs7eUfLEU6CVUiGo4uG6QUmWkJ0piVRdwkjSioJ-Q80JmKI7pFzlHOEssw3DTZlou544_4uJkyYHdbC55OkKSQfq7ZyL9x8yN8iuZ6a8Hv5RMg0-xk" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 1：用于 ML 工作负载分配的统一联合层。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h3 class="wp-block-heading" id="h-network-upgrade-for-llm-training-efficiency"&gt;网络升级提升LLM培训效率&lt;/h3&gt;&lt;p&gt;当扩展基础设施以适应生成式 AI 应用程序并提高分布式训练的效率，同时微调开源 LLM 时，重要的是要重点关注跨纵向扩展和横向扩展配置的网络带宽扩展。这就需要实现关键功能，例如 GPU 之间的全网状&lt;a href="https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/" rel="noreferrer noopener" target="_blank"&gt;NVlink&lt;/a&gt; ™ 连接、网络链路速度升级、熟练的拥塞控制管理、QoS 控制以及专用机架和网络拓扑的建立以及其他基本功能。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1083848" height="593" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/03/Fig2_network_upgrade-1024x593.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图2：通过网络链路容量升级提高训练效率。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;我们总结了大型语言模型 (LLM) 案例研究的结果，强调增强的网络带宽和拥塞控制机制对培训效果和性价比效率的巨大影响。我们的观察表明，与现有的网络互连相比，采用更高的网络带宽和更好的拥塞控制机制时，训练速度提高了近两倍，并且训练持续时间大幅缩短。在多节点训练期间，跨节点复制数据会增加本地内存需求并增加 IO 工作负载。我们的分析建议将每个 GPU 服务器上的网络链路容量增加 4 倍（25GB/s 至 100GB/s），从而可能使可用训练容量增加一倍。在构建这些服务的同时，我们还需要通过适当的隔离和 QoS 控制来确保大型训练运行生成的“大象流”不会对其他高优先级服务产生负面影响。&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-memory-upgrade-to-improve-gpu-allocation-rates"&gt;&lt;br /&gt;内存升级以提高 GPU 分配率&lt;/h3&gt;&lt;p&gt;较新的 AI/ML 工作负载要求每个 GPU 工作线程使用比我们设计的更多的系统内存。固有的物理限制，例如每台服务器上的内存通道数量有限，以及 NPI（新产品推出）期间部署的 DIMM 容量限制了我们扩展 GPU 分配的能力。为了提高 GPU 分配率，我们已开始努力将这些服务器上的内存量增加一倍（每个 DIMM 通道 16GB 至 32GB）。此外，我们还在构建一个框架，以便在旧机架退役时重新利用和重复使用 DIMM。这种优化使我们能够最大限度地利用现有的机器学习基础设施，并充分利用我们当前的资源。我们将在下一篇文章中详细介绍通过这一举措所实现的效率提升。与此同时，我们已开始努力帮助调整培训工作的资源需求。正如其他人[&lt;a href="https://tianyin.github.io/pub/amp.pdf" rel="noreferrer noopener" target="_blank"&gt;参考文献&lt;/a&gt;]所证明的那样，手动请求最佳资源是一个难题，而自动化将有助于提高分配效率。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h2 class="wp-block-heading" id="h-building-new-infrastructure"&gt;&lt;strong&gt;建设新型基础设施&lt;/strong&gt;&lt;/h2&gt;&lt;h3 class="wp-block-heading" id="h-price-performance-evaluations-across-various-cloud-skus"&gt;&lt;br /&gt;各种云 SKU 的性价比评估&lt;/h3&gt;&lt;p&gt;2022 年底，当我们踏上向云过渡的旅程时，我们评估了不同云提供商提供的各种 CPU 和 GPU 模型。我们的目标是使用既定基准（从基于树的深度学习到大型语言模型）以及专有数据集和 Uber 模型（例如 deepETA 和 deepCVR）来比较它们的性价比。这些针对培训和服务目的进行的评估使我们能够选择针对特定工作负载进行优化的最合适的 SKU，同时考虑可行性、成本、吞吐量和延迟等因素。整个 2023 年，我们广泛测试了 17 种不同的 GPU 和 CPU SKU，采用了各种库和优化器，包括 Nvidia 的&lt;a href="https://github.com/NVIDIA/TensorRT" rel="noreferrer noopener" target="_blank"&gt;TensorRT&lt;/a&gt; ™(TRT) 和 TRT-LLM 优化。例如，如图 4 和图 5 所示，我们发现虽然 A10 GPU 可能无法为训练任务提供最具成本效益的吞吐量，但事实证明它们是我们服务用例的最佳选择，在提供最佳吞吐量的同时保持可接受的吞吐量。使用 TRT 的 SLA。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/6lE7W5fYhVSHGudF-A9Fu6wzvEQo4-S7ZjnN8dSgy3HcTmC4pqn-RT9VvBF0kTwZYY3rCz6OU4BVQfZ5erhWA2UMlK-7VbUYQkqKF6SUb58hKjHdzbzH7GK6Do40TLNw7bg0GTxgQYgzQDDV-jbkXxg" title="图表" /&gt;&lt;figcaption class="wp-element-caption"&gt;图3：深度学习训练和服务性能价格评估。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/jnlRbG2pls4P0xjxD9Pvv7fx4BW5_ZWXtSYLQLXfN317i1lPtxJPOxT6xbzNT8OIHEtAXarcDNNFeC-YRUAI0zh4t1sTBuPUfYeppJPdIwMpPDDJ7E8ddXBnNqde1rCZZVxCJiCIbpQ1qWd4oIGWyhk" title="图表" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 4：使用和不使用 TensorRT 优化的深度学习服务延迟。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt; Uber 的众多生成式 AI 应用程序需要使用 Nvidia 最新的 H100 GPU 来满足其严格的延迟要求。这一要求源于 H100 GPU 的功能，与上一代 A100 GPU 相比，它包括高达 4 倍 TFlops 和双倍的内存带宽。在试验 Meta™ &lt;a href="https://llama.meta.com/" rel="noreferrer noopener" target="_blank"&gt;Llama2&lt;/a&gt; ™ 模型系列（涉及各种批量大小、量化和模型参数）时，我们评估了各种开源和闭源框架，以进一步优化 LLM 服务性能。在图 6 和图 7 中，我们提出了一个具体案例，其中我们采用两个指标：每个令牌延迟（毫秒/令牌）和令牌/秒/gpu，来评估和比较两个表现最好的框架（TRT）的模型性能。 -LLM 和当前保密的框架），保持所有其他参数不变并使用 FP16 量化。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/qs9gWGpDUlhVZrug3qN88E-xDt8PKhA0Rd-R_WL8ECGq-DG50lJ7rz1nT37PxgIRyM0k2ypyN2Aq4v_FTnve8_tq9ayMOkm4cRd0UoTzSYbalhD2CDKQINp5F8uxVVDS8Y1ol-1MeKZK3pGzcSfd4dw" title="图表" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 5：按框架划分的 LLM 服务延迟比较 (H100)。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/rhL0z9GvoYxR0yiwFONGfGaHXsFNo_kORbuXeb9b2c8M0NEe_jJCbeEDThJUVHc05NeSt84xDjN7m5Skd2SvbzfHTUyz-MaaEhAffQoBtuKe3k7RZPVDQFDk7ZsgMYyFXfwCX-wL2dldeo2tHYzce-A" title="图表" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 6：使用相同延迟预算和所需 GPU 最少数量 (H100) 的框架的 LLM 服务吞吐量比较。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;这些实验结果清楚地表明，与 TRT-LLM 相比，框架 B 的延迟增加了两倍，吞吐量提高了六倍。它进一步强调了硬件/软件协同设计的重要性，并且为了充分利用硬件功能，必须在整个堆栈中拥有正确的解决方案。 &lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-llm-training-efficiency-improvements-with-memory-offload"&gt;&lt;br /&gt;通过内存卸载提高 LLM 培训效率&lt;/h3&gt;&lt;p&gt;在本节中，我们概述了有关大型语言模型的优化器状态、参数和梯度从 GPU 内存到 CPU 内存或 NVMe 设备的放置的设计和实验框架。我们的目标是评估这种卸载对 GPU 可扩展性、训练效率和一系列系统指标的影响。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-1083850" height="637" src="https://blog.uber-cdn.com/cdn-cgi/image/width=2160,quality=80,onerror=redirect,format=auto/wp-content/uploads/2024/03/Fig6_memory_offload_design_space-1024x637.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 7：内存卸载实验的设计框架。&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;p&gt;我们的实验结果表明，我们训练先前因 GPU 内存有限而阻碍的扩展模型的能力已得到显着增强。将内存从 GPU 内存卸载到系统内存甚至 NVMe 设备，通过在相同数量的 GPU 上使用更大的批量大小，有助于提高训练效率。这一转变使 MFU（模型触发器利用率）提高了 2 倍，同时 GPU 使用率降低了 34%。然而，值得注意的是，这种改进伴随着网络吞吐量的相应减少。有关此主题的详细开放计算机项目 ( &lt;a href="https://www.opencompute.org/" rel="noreferrer noopener" target="_blank"&gt;OCP&lt;/a&gt; ) 会议演讲可&lt;a href="https://www.youtube.com/watch?v=Ju0r8yU1_Lw" rel="noreferrer noopener" target="_blank"&gt;在此处&lt;/a&gt;找到。 &lt;/p&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter"&gt;&lt;img alt="" src="https://lh7-us.googleusercontent.com/L8HJjzd4fJLc0rv2q8ArOqvVbN-mlHlKwGTqoYg6dPciNtvU6kjAn3NK3eVfjp1WdeqLgJu2zIhmRNxeM5vkX3E47bIRNezH-X9ytjOWzh7lRI0V9OFtZMzOtUbaZXJjf6HsXy4oznVON7V6JTQt7zc" title="图表" /&gt;&lt;figcaption class="wp-element-caption"&gt;图 8：实施 Deepspeed 内存卸载优化的训练效率。 &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;&lt;h1 class="wp-block-heading" id="h-conclusion"&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;最后，我们想向您提供三个关键见解。在快速应用和模型发展（从 XGboost 到深度学习推荐模型和大型语言模型）中设计单一的 AI/ML 系统带来了相当大的挑战。例如，虽然法学硕士需要高 TFlops，但深度学习模型可能会遇到内存限制。为了提高这些系统的成本效益，必须根据给定 SLA 内的服务成本和单位成本性能等效率指标探索工作负载优化的解决方案。最大限度地提高基础设施效率需要跨系统所有层的协作硬件和软件设计方法。在此背景下，我们在本文中展示了各种示例，说明如何有效利用现有基础设施，同时构建新功能以有效扩展基础设施。最后，我们发出促进行业合作伙伴关系的邀请，敦促参与开源优化以提高效率，并就有效扩展基础设施交换想法和经验，以满足人工智能领域不断变化的需求。&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-acknowledgments"&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;非常感谢 UBER AI 基础设施、OCI、GCP 和 Nvidia 团队成员在上述工作中的合作。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Apache®、Apache Kafka、Kafka、Apache Spark、Spark 和星形徽标是 Apache Software Foundation 在美国和/或其他国家/地区的注册商标或商标。使用这些标记并不暗示 Apache 软件基金会的认可。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Kubernetes® 及其徽标是 Linux Foundation® 在美国和其他国家/地区的注册商标。使用这些标记并不暗示 Linux 基金会的认可。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; Falcon 180B® 及其徽标是 Technology Innovation Institute™ 在美国和其他国家/地区的注册商标。使用这些标志并不暗示技术创新学院的认可。&lt;/p&gt;&lt;p class="has-small-font-size"&gt; LLaMA 2® 及其徽标是 Meta® 在美国和其他国家/地区的注册商标。使用这些标记并不暗示 Meta 的认可。&lt;/p&gt;</description><pubDate>Thu, 28 Mar 2024 07:28:34 GMT</pubDate><guid isPermaLink="true">https://www.uber.com/blog/scaling-ai-ml-infrastructure-at-uber/</guid></item></channel></rss>