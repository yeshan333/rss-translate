<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Netflix 技术博客 - Medium</title><link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link><description>了解 Netflix 世界一流的工程工作、公司文化、产品开发等。 - 中等的</description><lastBuildDate>Sun, 12 May 2024 21:40:14 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>【The Making of VES: the Cosmos Microservice for Netflix Video Encoding】VES 的制作：用于 Netflix 视频编码的 Cosmos 微服务</title><link>https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/liwei-guo/"&gt;郭立伟&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/carvalhovinicius/"&gt;Vinicius Carvalho&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/anush-moorthy-b8451142/"&gt;Anush Moorthy&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/aditya-mavlankar-7139791/"&gt;Aditya Mavlankar&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/lishan-z-51302abb/"&gt;朱立山&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;这是 Netflix 多部分系列的第二篇文章。请参阅&lt;/em&gt;&lt;a href="https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359"&gt;&lt;em&gt;此处&lt;/em&gt;&lt;/a&gt;&lt;em&gt;的第 1 部分，其中概述了我们使用微服务重建 Netflix 视频处理管道的努力。本博客深入探讨了构建视频编码服务 (VES) 的详细信息，并分享了我们的经验教训。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad"&gt;Cosmos&lt;/a&gt;是 Netflix 的下一代媒体计算平台。 Cosmos 将微服务架构与异步工作流程和无服务器功能相结合，旨在实现 Netflix 媒体处理管道的现代化，提高灵活性、效率和开发人员生产力。在过去的几年里，Encoding Technologies (ET) 的视频团队一直致力于在 Cosmos 上重建整个视频管道。&lt;/p&gt;&lt;p&gt;这个新管道由许多微服务组成，每个微服务专用于单一功能。其中一种微服务是视频编码服务 (VES)。编码是视频管道的重要组成部分。在较高层面上，它采用摄取的夹层并将其编码为适合 Netflix 流或服务于某些工作室/制作用例的视频流。就 Netflix 而言，这项服务有许多要求：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;鉴于从手机到浏览器再到智能电视的广泛设备，需要支持多种编解码器格式、分辨率和质量级别。&lt;/li&gt;&lt;li&gt;分块编码是满足我们业务需求的延迟要求的必要条件，并且需要适应具有不同延迟敏感度级别的用例。&lt;/li&gt;&lt;li&gt;持续发布的能力对于在流媒体和工作室领域实现快速产品创新至关重要。&lt;/li&gt;&lt;li&gt;每天都有大量的编码工作。该服务需要具有成本效益并充分利用可用资源。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这篇技术博客中，我们将介绍如何构建 VES 来实现上述目标，并将分享我们从构建微服务中学到的一些经验教训。请注意，为简单起见，我们选择省略某些特定于 Netflix 的细节，这些细节不属于本博文的主要信息。&lt;/p&gt;&lt;h3&gt;在 Cosmos 上构建视频编码服务&lt;/h3&gt;&lt;p&gt;Cosmos 微服务由三层组成：接收请求的 API 层 (Optimus)、编排媒体处理流的工作流层 (Plato) 以及处理媒体的无服务器计算层 (Stratum)。这三层通过名为&lt;a href="https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f"&gt;Timestone&lt;/a&gt;的本地开发的基于优先级的消息系统进行异步通信。我们选择Protobuf作为payload格式，是因为它的高效性和成熟的跨平台支持。&lt;/p&gt;&lt;p&gt;为了帮助服务开发人员抢占先机，Cosmos 平台提供了强大的服务生成器。该生成器具有直观的用户界面。只需点击几下，它就创建了一个基本但完整的 Cosmos 服务：创建了所有 3 层的代码存储库；启用所有平台功能，包括发现、日志记录、跟踪等；发布管道已设置并且仪表板易于访问。我们可以立即开始添加视频编码逻辑并将服务部署到云端进行实验。&lt;/p&gt;&lt;h4&gt;擎天柱&lt;/h4&gt;&lt;p&gt;作为 API 层，Optimus 充当 VES 的网关，这意味着服务用户只能通过 Optimus 与 VES 交互。定义的API接口是VES与外部世界之间的强有力的契约。只要 API 稳定，用户就不会受到 VES 内部更改的影响。这种解耦有助于实现 VES 内部结构的更快迭代。&lt;/p&gt;&lt;p&gt;作为单一用途的服务，VES 的 API 非常干净。我们定义了一个端点&lt;em&gt;encodeVideo&lt;/em&gt; ，它接受&lt;em&gt;EncodeRequest&lt;/em&gt;并返回&lt;em&gt;EncodeResponse&lt;/em&gt; （通过Timestone消息以异步方式）。 &lt;em&gt;EncodeRequest&lt;/em&gt;对象包含有关源视频以及编码配方的信息。编码视频的所有要求（编解码器、分辨率等）以及延迟控制（分块指令）都通过编码配方的数据模型公开。&lt;/p&gt;&lt;pre&gt; //protobuf定义&lt;br /&gt;&lt;br /&gt;消息编码请求{&lt;br /&gt; VideoSource video_source = 1;//待编码源&lt;br /&gt;菜谱配方=2； //包括编码格式、分辨率等&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;消息编码响应{&lt;br /&gt;输出视频output_video = 1; //编码后的视频&lt;br /&gt;误差误差=2； //错误信息（可选）&lt;br /&gt; }&lt;br /&gt;&lt;br /&gt;消息食谱{&lt;br /&gt;编解码器编解码器=1； //包括编解码格式、配置文件、级别等&lt;br /&gt;分辨率分辨率=2；&lt;br /&gt;分块指令 chunking_directives = 3;&lt;br /&gt; ...&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;与任何其他 Cosmos 服务一样，该平台会根据 VES API 数据模型自动生成 RPC 客户端，用户可以使用该客户端构建请求并调用 VES。收到传入请求后，Optimus 会执行验证，并（如果适用）将传入数据转换为内部数据模型，然后再将其传递到下一层 Plato。&lt;/p&gt;&lt;h3&gt;柏拉图&lt;/h3&gt;&lt;p&gt;工作流层 Plato 控制媒体处理步骤。 Cosmos 平台支持 Plato 的两种编程范例：前向链接规则引擎和有向无环图（DAG）。 VES 具有线性工作流程，因此我们选择 DAG 是因为它简单。&lt;/p&gt;&lt;p&gt;在 DAG 中，工作流由节点和边表示。节点代表工作流中的阶段，而边表示依赖关系——只有当所有依赖关系都完成时，阶段才准备好执行。 VES 需要对视频块进行并行编码，以满足其延迟和弹性目标。 DAG 通过 MapReduce 模式促进了这种工作流级并行性。可以通过对节点进行注释来指示这种关系，并且只有当所有关联的 Map 节点都准备就绪时，Reduce 节点才会被触发。&lt;/p&gt;&lt;p&gt;对于 VES 工作流程，我们定义了五个节点及其关联的边，如下图所示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分割器节点：该节点根据配方中的分块指令将视频分成块。&lt;/li&gt;&lt;li&gt;编码器节点：该节点对视频块进行编码。它是一个Map节点。&lt;/li&gt;&lt;li&gt;汇编器节点：该节点将编码块拼接在一起。它是一个Reduce节点。&lt;/li&gt;&lt;li&gt;验证器节点：该节点执行编码视频的验证。&lt;/li&gt;&lt;li&gt;通知节点：整个工作流程完成后，该节点通知 API 层。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7n3hN3lYhe89Ezk6" /&gt;&lt;/figure&gt;&lt;p&gt;在此工作流程中，Notifier 等节点执行非常轻量级的操作，可以直接在 Plato 运行时中执行。然而，资源密集型操作需要委托给计算层（Stratum）或其他服务。 Plato 调用 Stratum 函数来执行编码和组装等任务，其中节点（编码器和组装器）将消息发布到相应的消息队列。验证器节点调用另一个 Cosmos 服务（视频验证服务）来验证组装的编码视频。&lt;/p&gt;&lt;h4&gt;地层&lt;/h4&gt;&lt;p&gt;计算层 Stratum 是可以访问媒体样本的地方。 Cosmos 服务的开发人员创建 Stratum Functions 来处理媒体。他们可以自带媒体处理工具，这些工具被打包到 Functions 的 Docker 镜像中。然后，这些 Docker 镜像会发布到我们的内部 Docker 注册表（ &lt;a href="https://medium.com/p/f868c9fb5436"&gt;Titus&lt;/a&gt;的一部分）。在生产中，Titus 会根据作业队列的深度自动扩展实例。&lt;/p&gt;&lt;p&gt; VES 需要支持将源视频编码为各种编解码器格式，包括 AVC、AV1 和 VP9 等。我们针对不同的编解码器格式使用不同的编码器二进制文件（简称为“编码器”）。对于AVC这种已有20年历史的格式来说，编码器相当稳定。另一方面， &lt;a href="https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320"&gt;Netflix 流媒体的最新成员&lt;/a&gt;AV1 正在不断进行积极的改进和实验，因此需要更频繁的编码器升级。为了有效管理这种可变性，我们决定创建多个 Stratum Functions，每个 Stratum Functions 专用于特定的编解码器格式，并且可以独立发布。这种方法可确保升级一个编码器不会影响其他编解码器格式的 VES 服务，从而保持全面的稳定性和性能。&lt;/p&gt;&lt;p&gt;在 Stratum Function 中，Cosmos 平台提供了常见媒体访问模式的抽象。无论文件格式如何，源都统一呈现为本地安装的框架。同样，对于需要保存在云端的输出，平台将过程呈现为写入本地文件。所有细节（例如字节流和错误重试）都被抽象掉。由于平台考虑了基础设施的复杂性，Stratum Function 中视频编码的基本代码可以简单如下。&lt;/p&gt;&lt;pre&gt; ffmpeg -i 输入/源%08d.j2k -vf ... -c:v libx264 ... 输出/encoding.264&lt;/pre&gt;&lt;p&gt;编码是一个资源密集型过程，所需资源与编解码格式和编码配方密切相关。我们进行了基准测试，以了解不同编码方案的资源使用模式，特别是 CPU 和 RAM。根据结果​​，我们利用了 Cosmos 平台的“容器整形”功能。&lt;/p&gt;&lt;p&gt;我们定义了许多不同的“容器形状”，指定了 CPU 和 RAM 等资源的分配。&lt;/p&gt;&lt;pre&gt; # 容器形状的定义示例&lt;br /&gt;组：containerShapeExample1&lt;br /&gt;资源：&lt;br /&gt;中央处理器数量：2&lt;br /&gt;内存（MB）：4000&lt;br /&gt;网络 Mbp: 750&lt;br /&gt;磁盘大小（MB）：12000&lt;/pre&gt;&lt;p&gt;创建路由规则以根据编解码器格式和编码分辨率的组合将编码作业分配给不同的形状。这有助于平台执行“装箱”，从而最大化资源利用率。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/425/1*UZ7G9SlWnqLXd4umutzhcQ.png" /&gt;&lt;figcaption&gt; “装箱”的一个例子。圆圈代表 CPU 核心，区域代表 RAM。此 16 核 EC2 实例包含 5 个 3 种不同形状（用不同颜色表示）的编码容器（矩形）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;持续发布&lt;/h3&gt;&lt;p&gt;在我们完成所有三层的开发和测试后，VES 投入生产。然而，这并不标志着我们工作的结束。恰恰相反，我们相信并且仍然相信服务价值的很大一部分是通过迭代实现的：支持新的业务需求、增强性能和提高弹性。我们愿景的一个重要部分是让 Cosmos 服务能够以安全的方式持续将代码更改发布到生产中。&lt;/p&gt;&lt;p&gt;专注于单一功能，与 VES 中添加的单一功能相关的代码更改通常较小且具有凝聚力，因此易于审查。由于调用者只能通过其 API 与 VES 交互，因此内部代码是真正可以安全更改的“实现细节”。显式的 API 契约限制了 VES 的测试面。此外，Cosmos 平台还提供了基于&lt;a href="https://martinfowler.com/articles/practical-test-pyramid.html"&gt;金字塔&lt;/a&gt;的测试框架，指导开发人员创建不同级别的测试。&lt;/p&gt;&lt;p&gt;经过测试和代码审查后，更改将被合并并准备发布。发布管道是完全自动化的：合并后，管道会检查代码、编译、构建、按规定运行单元/集成/端到端测试，如果没有遇到问题，则继续进行完整部署。通常，从代码合并到功能落地大约需要 30 分钟（在我们的上一代平台中，这个过程需要 2-4 周！）。较短的发布周期为开发人员提供了更快的反馈，并帮助他们在上下文仍然新鲜的情况下进行必要的更新。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1020/1*fgKHQg4IZVMkWQlHGaCT2Q.png" /&gt;&lt;figcaption&gt;在我们的生产环境中运行的发布管道的屏幕截图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在生产环境中运行时，该服务会不断发出指标和日志。它们由平台收集以可视化仪表板并驱动监控/警报系统。与基线偏差太大的指标将触发警报，并可能导致服务自动回滚（启用“金丝雀”功能时）。&lt;/p&gt;&lt;h3&gt;学到的东西：&lt;/h3&gt;&lt;p&gt; VES 是我们团队构建的第一个微服务。我们从微服务的基本知识开始，并在此过程中学到了很多经验教训。这些知识加深了我们对微服务的理解，并帮助我们改进了设计选择和决策。&lt;/p&gt;&lt;h4&gt;定义适当的服务范围&lt;/h4&gt;&lt;p&gt;微服务架构的一个原则是应该为单一功能构建服务。这听起来很简单，但是什么才算是“单一功能”呢？ “编码视频”听起来不错，但“将视频编码为AVC格式”难道不是一个更具体的单一功能吗？&lt;/p&gt;&lt;p&gt;当我们开始构建 VES 时，我们采用了为每种编解码器格式创建单独的编码服务的方法。虽然这具有解耦工作流程等优点，但很快我们就被开发开销淹没了。想象一下，用户请求我们在编码中添加水印功能。我们需要对多个微服务进行更改。更糟糕的是，所有这些服务的变化都非常相似，本质上我们一次又一次地添加相同的代码（和测试）。这种重复性的工作很容易让开发人员疲惫不堪。&lt;/p&gt;&lt;p&gt;本博客中介绍的服务是我们的 VES 的第二次迭代（是的，我们已经经历了一次迭代）。在此版本中，我们将不同编解码器格式的编码合并到单个服务中。它们共享相同的 API 和工作流程，而每种编解码器格式都有自己的层函数。到目前为止，这似乎取得了良好的平衡：公共 API 和工作流程减少了代码重复，而单独的 Stratum Functions 则保证了每种编解码器格式的独立演进。&lt;/p&gt;&lt;p&gt;我们所做的改变并非不可逆转。如果将来的某一天，一种特定编解码器格式的编码演变成完全不同的工作流程，我们可以选择将其分离到自己的微服务中。&lt;/p&gt;&lt;h4&gt;对数据建模要务实&lt;/h4&gt;&lt;p&gt;一开始，我们对数据模型分离非常严格——我们坚信共享就等于耦合，而耦合可能会导致未来潜在的灾难。为了避免这种情况，对于每个服务以及服务中的三个层，我们定义了自己的数据模型并构建了转换器来在不同的数据模型之间进行转换。&lt;/p&gt;&lt;p&gt;我们最终为系统中的位深度和分辨率等方面创建了多个数据模型。公平地说，这确实有一些优点。例如，我们的编码管道支持 AVC 编码（8 位）和 AV1 编码（10 位）的不同位深度。通过定义&lt;em&gt;AVC.BitDepth&lt;/em&gt;和&lt;em&gt;AV1.BitDepth&lt;/em&gt; ，可以将位深度的约束构建到数据模型中。然而，这种差异化能力的好处是否大于缺点（即多个数据模型转换）是有争议的。&lt;/p&gt;&lt;p&gt;最终，我们创建了一个库来托管视频领域常见概念的数据模型。此类概念的示例包括帧速率、扫描类型、色彩空间等。如您所见，它们非常常见且稳定。这个“通用”数据模型库在视频团队拥有的所有服务之间共享，避免了不必要的重复和数据转换。在每个服务中，为特定于服务的对象定义了附加数据模型。&lt;/p&gt;&lt;h4&gt;拥抱服务 API 变化&lt;/h4&gt;&lt;p&gt;这听起来可能很矛盾。我们一直在说 API 是服务与其用户之间的强有力的契约，保持 API 的稳定可以保护用户免受内部变化的影响。这是绝对正确的。然而，当我们设计服务 API 的第一个版本时，我们谁都没有水晶球。不可避免的是，在某个时刻，这个 API 会变得不够用。如果我们过于坚信“API 不能改变”，那么开发人员将被迫寻找解决方法，而这几乎肯定不是最佳的。&lt;/p&gt;&lt;p&gt;有很多关于优雅地发展 API 的优秀技术文章。我们相信我们还拥有独特的优势：VES 是 Netflix Encoding Technologies (ET) 的一项内部服务。我们的两个用户（Streaming Workflow Orchestrator 和 Studio Workflow Orchestrator）由 ET 内的工作流团队拥有。我们的团队拥有相同的背景并为共同的目标而努力。如果我们认为更新 API 符合 Netflix 的最佳利益，我们就会与他们会面以寻求一致。一旦达成更新 API 的共识，团队就会协作以确保平稳过渡。&lt;/p&gt;&lt;h3&gt;敬请关注…&lt;/h3&gt;&lt;p&gt;这是我们的技术博客系列使用微服务重建 Netflix 视频管道的第二部分。在这篇文章中，我们详细描述了视频编码服务（VES）的构建过程以及我们的经验教训。我们的管道包括我们也计划分享的一些其他服务。请继续关注我们未来关于微服务主题的博客！ &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=946b9b3cd300" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300"&gt;VES 的制作：Netflix 视频编码的 Cosmos 微服务&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Tue, 09 Apr 2024 22:12:32 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300?source=rss----2615bd06b42e---4</guid></item><item><title>【Reverse Searching Netflix’s Federated Graph】反向搜索 Netflix 的联合图</title><link>https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;作者：&lt;a href="https://www.linkedin.com/in/rickygardiner/"&gt;瑞奇·加德纳&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/ahutter/"&gt;亚历克斯·哈特&lt;/a&gt;和&lt;a href="https://www.linkedin.com/in/katielefevre/"&gt;凯蒂·勒费夫尔&lt;/a&gt;&lt;/p&gt;&lt;p&gt;自从我们之前发布关于内容工程在 Netflix 联合图内启用搜索功能中的作用的文章（ &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf"&gt;第一篇文章&lt;/a&gt;，我们识别问题并详细说明索引架构， &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c"&gt;第二篇文章&lt;/a&gt;，我们详细介绍如何促进查询）以来，已经发生了重大的变化。事态发展。我们已将 Studio Search 开放给内容工程之外的 Netflix 整个工程部门，并将其重命名为 Graph Search。有超过 100 个应用程序与图搜索集成，我们支持近 50 个索引。我们继续向该服务添加功能。正如上一篇文章中所承诺的，我们将分享我们如何与 Studio 工程团队合作构建反向搜索。反向搜索反转了标准查询模式：它不是查找与查询匹配的文档，而是查找与文档匹配的查询。&lt;/p&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;p&gt;蒂芙尼 (Tiffany) 是 Netflix 的后期制作协调员，负责监督近十几部处于不同预制作、制作和后期制作状态的电影。蒂芙尼和她的团队与各种跨职能合作伙伴（包括法律、创意和影片发行管理）合作，跟踪她的电影的进展和健康状况。&lt;/p&gt;&lt;p&gt;因此，蒂芙尼订阅了特定于某些关注领域的通知和日历更新，例如“在墨西哥城拍摄但没有分配关键角色的电影”，或“有可能在发布日期前准备就绪的电影”。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;Tiffany 不是订阅特定电影的更新，而是订阅返回电影动态子集的查询。&lt;/em&gt;这给我们这些负责向她发送这些通知的人带来了一个问题。当电影发生变化时，我们不知道该通知谁，因为员工和他们感兴趣的电影之间没有关联。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们可以保存这些搜索，然后重复查询每个搜索的结果，但由于我们是大型联合图的一部分，这会对我们连接到的每个服务产生大量流量影响。我们必须决定是否需要及时通知或减少图表上的负载。&lt;/p&gt;&lt;p&gt;如果我们能够回答“此查询是否会返回这部电影”的问题，我们就可以根据变化事件以激光精度重新查询，而不会影响更广泛的生态系统。&lt;/p&gt;&lt;h3&gt;解决方案&lt;/h3&gt;&lt;p&gt;图搜索建立在 Elasticsearch 之上，它具有我们所需的确切功能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可用于索引 Elasticsearch 查询的&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/percolator.html#:~:text=The%20percolator%20field%20type%20parses,to%20be%20a%20percolator%20field."&gt;渗透器字段&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html"&gt;渗透查询&lt;/a&gt;可用于确定哪些索引查询与输入文档匹配。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GCZRoNqT8seObcUFzYthXg.png" /&gt;&lt;/figure&gt;&lt;p&gt;渗透查询不是进行搜索（例如“在墨西哥城拍摄的西班牙语电影”）并返回匹配的文档（一个用于 Roma，一个用于 Familia），而是采用一个文档（一个用于 Roma）并返回匹配的搜索。匹配该文档，例如“西班牙语电影”和“剧本戏剧”。&lt;/p&gt;&lt;p&gt;我们将此功能称为保存搜索的功能，称为 SavedSearches，它是现有索引上的持久过滤器。&lt;/p&gt;&lt;pre&gt;输入已保存的搜索{&lt;br /&gt;身份证：身份证！&lt;br /&gt;过滤器：字符串&lt;br /&gt;索引：搜索索引！&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;该过滤器以 Graph Search DSL 编写，被转换为 Elasticsearch 查询并在 percolator 字段中建立索引。要了解有关图搜索 DSL 的更多信息以及我们创建它而不是直接使用 Elasticsearch 查询语言的原因， &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c"&gt;请参阅“Netflix 内容工程如何使联合图可搜索（第 2 部分）”的查询语言部分&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们将查找匹配的已保存搜索的过程称为反向搜索。这是该产品中最直接的部分。我们向&lt;a href="https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2"&gt;域图服务&lt;/a&gt;(DGS) 添加了一个新的解析器以进行图搜索。它获取感兴趣的索引和文档，并通过发出渗透查询返回与文档匹配的所有已保存搜索。&lt;/p&gt;&lt;pre&gt; ”“”&lt;br /&gt;用于检索给定索引中所有注册的已保存搜索的查询，&lt;br /&gt;根据提供的文件。本例中的文档是 ElasticSearch&lt;br /&gt;根据索引的配置生成的文档。&lt;br /&gt; ”“”&lt;br /&gt;反向搜索(&lt;br /&gt;之后：字符串，&lt;br /&gt;文档：JSON！，&lt;br /&gt;第一：Int！，&lt;br /&gt;索引：SearchIndex！）：SavedSearchConnection&lt;/pre&gt;&lt;p&gt;持久保存 SavedSearch 是作为图搜索 DGS 上的新突变实现的。这最终会触发 Percolator 字段中 Elasticsearch 查询的索引。&lt;/p&gt;&lt;pre&gt; ”“”&lt;br /&gt;用于注册和更新已保存搜索的突变。他们需要更新&lt;br /&gt;任何时候用户调整他们的搜索条件。&lt;br /&gt; ”“”&lt;br /&gt; upsertSavedSearch（输入：UpsertSavedSearchInput！）：UpsertSavedSearchPayload&lt;/pre&gt;&lt;p&gt;支持渗透器字段从根本上改变了我们为图搜索提供索引管道的方式（ &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf"&gt;请参阅 Netflix 内容工程如何使联合图可搜索的架构部分）&lt;/a&gt; 。我们现在有两个索引管道，而不是每个图形搜索索引都有一个索引管道：一个用于索引文档，另一个用于将保存的搜索索引到渗透索引。我们选择将渗透器字段添加到单独的索引中，以便分别调整两种类型查询的性能。&lt;/p&gt;&lt;p&gt; Elasticsearch 要求渗透索引具有与其存储的查询结构相匹配的映射，因此必须与文档索引的映射相匹配。索引模板定义创建新索引时应用的​​映射。通过使用索引模板的&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates-v1.html#put-index-template-v1-api-request-body"&gt;index_patterns&lt;/a&gt;功能，我们能够在两者之间共享文档索引的映射。 index_patterns 还为我们提供了一种简单的方法来将渗透器字段添加到我们创建的每个渗透索引中。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;文档索引映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;索引模式 — application_*&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “订单”：1，&lt;br /&gt; “index_patterns”：[“application_*”]，&lt;br /&gt; “映射”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “电影标题”： {&lt;br /&gt; “类型”：“关键字”&lt;br /&gt; },&lt;br /&gt; “已存档”：{&lt;br /&gt; “类型”：“布尔值”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;渗透索引映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;索引模式 — *_percolate&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “订单”：2，&lt;br /&gt; “index_patterns”：[“*_percolate*”]，&lt;br /&gt; “映射”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “渗透查询”：{&lt;br /&gt; “类型”：“渗滤器”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;生成的映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Percolate 索引名称为 application_v1_percolate&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “application_v1_percolate”：{&lt;br /&gt; “映射”：{&lt;br /&gt; “_doc”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “电影标题”： {&lt;br /&gt; “类型”：“关键字”&lt;br /&gt; },&lt;br /&gt; “已存档”：{&lt;br /&gt; “类型”：“布尔值”&lt;br /&gt; },&lt;br /&gt; “渗透查询”：{&lt;br /&gt; “类型”：“渗滤器”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;h3&gt;渗透索引管道&lt;/h3&gt;&lt;p&gt;渗透索引并不像从 GraphQL 突变获取输入、将其转换为 Elasticsearch 查询并为其建立索引那么简单。我们稍后将详细讨论版本控制，它露出了丑陋的一面，并使事情变得更加复杂。这是渗透索引管道的设置方式。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KSZuvPeOxDOKNrPiNnNvFg.png" /&gt;&lt;figcaption&gt;&lt;em&gt;请参阅&lt;/em&gt;&lt;a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873"&gt;Netflix 的数据网格 — 数据移动和处理平台，&lt;/a&gt;了解有关数据网格的更多信息。&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;当 SavedSearches 被修改时，我们将它们存储在 CockroachDB 中，并且 Cockroach 数据库的源连接器会发出 CDC 事件。&lt;/li&gt;&lt;li&gt;共享一个表来存储所有 SavedSearches，因此下一步是使用过滤处理器过滤到仅用于 *this* 索引的表。&lt;/li&gt;&lt;li&gt;如前所述，数据库中存储的是我们自定义的图形搜索过滤器 DSL，它与 Elasticsearch DSL 不同，因此我们无法直接将事件索引到渗透索引。相反，我们对图搜索 DGS 进行了修改。图形搜索 DGS 将 DSL 转换为 Elasticsearch 查询。&lt;/li&gt;&lt;li&gt;然后，我们将 Elasticsearch 查询作为适当渗透索引中的渗透字段进行索引。&lt;/li&gt;&lt;li&gt;返回 SavedSearch 索引成功或失败。失败时，SavedSearch 事件将发送到死信队列 (DLQ)，该队列可用于解决任何故障，例如从索引中删除搜索查询中引用的字段。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;现在稍微介绍一下版本控制，以解释为什么上述内容是必要的。想象一下，我们已经开始为有动物的电影添加标签。如果我们希望用户能够创建“动物电影”的视图，我们需要将这个新字段添加到现有搜索索引中以标记电影。但是，当前索引中的映射不包含它，因此我们无法对其进行过滤。为了解决这个问题，我们有索引版本。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AnB07zkL_g4a30TUgAHGSA.jpeg" /&gt;&lt;figcaption&gt; &lt;a href="https://www.netflix.com/title/81730862"&gt;《动物宝宝》&lt;/a&gt;系列中的达莉亚和福雷斯特&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当索引定义发生更改而需要新映射时（例如添加动物标签时），图形搜索会创建新版本的 Elasticsearch 索引和新的管道来填充它。这个新的管道从数据网格中的日志压缩的 Kafka 主题中读取数据——这就是我们如何重新索引整个语料库，而无需要求数据源重新发送所有旧事件。新管道和旧管道并行运行，直到新管道处理完积压的工作，此时图搜索会切换到使用 Elasticsearch 索引别名的版本。&lt;/p&gt;&lt;p&gt;为我们的文档创建新索引意味着我们还需要为查询创建新的渗透索引，以便它们可以具有一致的索引映射。当我们更改版本时，这个新的渗透索引也需要回填。这就是管道如此工作的原因——当我们启动新的渗透索引管道时，我们可以再次利用数据网格中的日志压缩主题来重新索引 SavedSearches 语料库。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bAxTCeTOeZ_g4ueiN0cLiQ.png" /&gt;&lt;figcaption&gt;&lt;em&gt;我们将用户提供的过滤器 DSL 保留到数据库中，而不是立即将其转换为 Elasticsearch 查询语言。这使我们能够在将保存的搜索 DSL 转换为 Elasticsearch 查询时进行更改或修复。我们可以通过创建新版本的索引来部署这些更改，因为引导过程将重新翻译每个保存的搜索。&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;另一个用例&lt;/h3&gt;&lt;p&gt;我们希望反向搜索功能最终对其他工程团队有用。我们几乎立即就收到了反向搜索可以解决的问题。&lt;/p&gt;&lt;p&gt;根据电影的类型，制作电影的方式可能会非常不同。一部电影可能会经历一组不适用于另一部电影的阶段，或者可能需要安排另一部电影不需要的某些事件。我们应该能够定义电影分类的方法，并使用它来自动将它们分配给工作流程，而不是根据电影的分类手动配置电影的工作流程。但确定电影的分类具有挑战性：您可以仅根据类型来定义这些电影分类，例如“动作”或“喜剧”，但您可能需要更复杂的定义。也许它是由流派、地区、格式、语言或其某种微妙的组合来定义的。电影匹配服务提供了一种根据匹配标准的任意组合对电影进行分类的方法。在幕后，匹配标准存储为反向搜索，并且为了确定电影与哪个标准匹配，电影的文档被提交到反向搜索端点。&lt;/p&gt;&lt;p&gt;简而言之，反向搜索正在为外部化标准匹配器提供动力。它现在用于电影标准，但由于每个图形搜索索引现在都具有反向搜索功能，因此任何索引都可以使用此模式。&lt;/p&gt;&lt;h3&gt;可能的未来：订阅&lt;/h3&gt;&lt;p&gt;反向搜索看起来也像是创建响应速度更快的 UI 的有前景的基础。搜索结果可以通过 GraphQL 订阅提供，而不是作为查询获取一次结果。这些订阅可以与 SavedSearch 相关联，并且当索引发生变化时，可以使用反向搜索来确定何时更新订阅返回的键集。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=222ac5d23576" width="1" /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576"&gt;反向搜索 Netflix 的 Federated Graph&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix TechBlog&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 21:26:42 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576?source=rss----2615bd06b42e---4</guid></item><item><title>【Sequential Testing Keeps the World Streaming Netflix Part 2: Counting Processes】顺序测试让世界保持流媒体 Netflix 第 2 部分：计算进程</title><link>https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642?source=rss----2615bd06b42e---4</link><description>&lt;h3&gt;&lt;strong&gt;顺序 A/B 测试让世界保持流媒体播放 Netflix 第 2 部分：计算进程&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/michaelslindon/"&gt;迈克尔·林登&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/csanden/"&gt;克里斯·桑登&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/vshirikian/"&gt;Vache Shirikian&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/liuyanjun/"&gt;刘彦军&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/minalmishra/"&gt;米纳尔·米什拉&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/martintingley/"&gt;马丁·廷利&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7DNyGA0x7r7msS7w1Zpvpw.jpeg" /&gt;&lt;/figure&gt;&lt;p&gt;您在流式传输 Netflix 时遇到过错误吗？您的标题是否意外停止，或者根本没有开始？在有关顺序测试的博客系列的第一部分中，我们描述了&lt;a href="https://medium.com/p/cba6c7ed49df"&gt;用于连续指标（例如&lt;em&gt;播放延迟）&lt;/em&gt;的金丝雀测试方法&lt;/a&gt;。我们的一位读者评论道&lt;/p&gt;&lt;blockquote&gt;如果新版本与新的播放/流媒体功能无关怎么办？例如，如果新版本包含修改后的登录功能怎么办？您还会监控“播放延迟”指标吗？&lt;/blockquote&gt;&lt;p&gt; Netflix 监控大量指标，其中许多指标可以归类为计数。其中包括登录次数、错误次数、游戏成功开始次数，甚至客户呼叫中心联系次数等指标。在第二部分中，我们描述了用于测试计数指标的顺序方法，该方法在 NeurIPS 论文&lt;a href="https://openreview.net/forum?id=a4zg0jiuVi"&gt;&lt;em&gt;《多项计数数据的随时有效推理》&lt;/em&gt;&lt;/a&gt;中概述。&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;指出不同&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;假设我们要部署改变登录行为的新代码。为了降低软件推出的风险，我们对新代码进行 A/B 测试，也称为金丝雀测试。每当发生登录等事件时，日志就会流经我们的实时后端，并记录相应的时间戳。图 1 说明了分配给新（治疗）和现有（控制）软件版本的设备生成的时间戳序列。我们自然关心的一个问题是处理中是否减少了登录事件。你能告诉？ &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AGdASLgCaQCNo72VV7rOFg.gif" /&gt;&lt;figcaption&gt;图 1：控制和治疗中发生的事件的时间戳&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过简单检查图 1 中的点过程并不能立即明显看出这一点。当我们将观察到的&lt;a href="https://en.wikipedia.org/wiki/Counting_process#:~:text=Counting%20processes%20deal%20with%20the,be%20a%20Markov%20counting%20process."&gt;计数过程&lt;/a&gt;可视化时，差异立即变得明显，如图 2 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hAQupmPzmKi7squd4iEwig.png" /&gt;&lt;figcaption&gt;图 2：可视化计数过程 — 时间 t 观察到的事件数量&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;计数过程是每当新事件到达时就加 1 的函数。显然，治疗组中发生的事件比对照组少。如果这些是登录事件，则表明新代码包含一个错误，该错误会阻止某些用户成功登录。&lt;/p&gt;&lt;p&gt;这是处理事件时间戳时的常见情况。举另一个例子，如果事件对应于错误或崩溃，我们想知道这些事件在处理中是否比在对照组中累积得更快。此外，我们希望&lt;em&gt;尽快&lt;/em&gt;回答这个问题，以防止服务进一步中断&lt;em&gt;。&lt;/em&gt;这需要&lt;a href="https://medium.com/p/cba6c7ed49df"&gt;第 1 部分&lt;/a&gt;中介绍的顺序测试技术。&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;时间非齐次泊松过程&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;我们每个治疗组的数据是一维点过程的实现，即时间戳序列。由于事件到达的速率是随时间变化的（在处理和控制中），我们将点过程建模为时间不均匀的 &lt;a href="https://en.wikipedia.org/wiki/Poisson_point_process#Inhomogeneous_Poisson_point_process"&gt;泊松点过程&lt;/a&gt;。该点过程由强度函数 λ 定义：ℝ → [0, ∞)。区间 [0,t) 中的事件数量，表示为 N(t)，具有以下泊松分布&lt;/p&gt;&lt;p&gt;N(t) ~ Poisson(Λ(t))，其中 Λ(t) = ∫₀ᵗ λ(s) ds。&lt;/p&gt;&lt;p&gt;我们试图检验原假设 H₀：对于所有 t，即控制 (A) 和处理 (B) 的强度函数相同，λᴬ(t) = λᴮ(t)。这可以半参数地完成，无需对强度函数 λᴬ 和 λᴮ 做出任何假设。此外，该研究的新颖之处在于这可以按顺序完成，如我们论文&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;第 4 节&lt;/a&gt;所述。方便的是，在时间 t 检验该假设所需的唯一数据是 Nᴬ(t) 和 Nᴮ(t)，即迄今为止在对照和治疗中观察到的事件总数。换句话说，检验原假设所需的只是两个整数，这两个整数可以在新事件到达时轻松更新。这是模拟 A/A 测试的一个示例，其中我们通过设计知道对照组 (A) 和处理组 (B) 的强度函数是相同的，尽管是非平稳的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RDdOPECxhDmLt9FkAOuWog.png" /&gt;&lt;figcaption&gt;图 3：（左）两个非齐次泊松点过程的 A/A 模拟。 （右）强度函数对数差值的置信序列和序列 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 3 提供了 A/A 设置的图示。左图展示了原始数据和强度函数，右图展示了序贯统计分析。蓝色和红色地毯图分别表示观察到的来自处理流和控制流的事件的到达时间戳。虚线是观察到的计数过程。由于该数据是在零值下模拟的，因此强度函数是相同的并且相互重叠。右图的左轴可视化强度函数对数差值上置信序列的演变。右图的右轴可视化了序列 p 值的演变。我们可以做出以下两个观察&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在零值下，对数强度差为零，这始终被 0.95 置信度序列正确覆盖。&lt;/li&gt;&lt;li&gt;连续 p 值始终大于 0.05&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在让我们考虑 A/B 设置的示例。图 4 显示了强度函数不同时观察到的治疗和对照到达时间。由于这是模拟，因此对数强度之间的真实差异是已知的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KChDT2L5gSw3BEDvH2Pu0g.png" /&gt;&lt;figcaption&gt;图 4：（左）两个非齐次泊松点过程的 A/B 模拟。 （右）强度函数对数差异的置信序列和序列 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们可以做出以下观察&lt;/p&gt;&lt;ul&gt;&lt;li&gt;0.95 置信度序列始终涵盖真实的对数差异&lt;/li&gt;&lt;li&gt;连续 p 值降至 0.05 以下，同时 0.95 置信度序列排除零值&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在，我们提供了一些案例研究，其中该方法可以快速检测到许多计数指标中的严重问题&lt;/p&gt;&lt;h4&gt;案例研究 1：成功的标题开始下降&lt;/h4&gt;&lt;p&gt;图 2 实际上显示了真实金丝雀测试中的标题开始事件的计数。每当影片成功启动时，就会从设备向 Netflix 发送一个&lt;a href="https://netflixtechblog.com/sps-the-pulse-of-netflix-streaming-ae4db0e05f8a"&gt;事件&lt;/a&gt;。我们有来自治疗设备的标题开始事件流和来自控制设备的标题开始事件流。每当在治疗设备中观察到较少的标题开始时，新客户端中通常存在阻止播放的错误。&lt;/p&gt;&lt;p&gt;在这种情况下，金丝雀测试检测到一个错误，后来确定该错误导致大约 60% 的治疗设备无法启动其流。除了（顺序）p 值之外，置信序列如图 5 所示。虽然省略了确切的时间单位，但该错误是在&lt;em&gt;亚秒&lt;/em&gt;级检测到的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mfkXe5aNK3Y8X7aAXbxhJw.png" /&gt;&lt;figcaption&gt;图 5：对数强度与序列 p 值之差的 0.99 置信序列。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;案例研究 2：异常停机增加&lt;/h4&gt;&lt;p&gt;除了标题开始事件之外，我们还会监控 Netflix 客户端何时意外关闭。和以前一样，我们有两股异常关闭事件，一是来自处理设备，一是来自控制设备。以下屏幕截图直接取自我们的&lt;a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c"&gt;Lumen&lt;/a&gt;仪表板。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*48VrtyraTis6BFnls0hECA.png" /&gt;&lt;figcaption&gt;图 6：一段时间内的异常停机计数（累积和非累积）。处理（黑色）和对照（蓝色）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 6 说明了两个要点。异常停机事件的到来明显存在非平稳性。从非累积的角度来看，也不容易明显看出治疗和对照之间的任何差异。然而，通过观察计数过程，从累积视图中更容易看出差异。治疗中异常停机的数量有小幅但明显的增加。图 7 显示了我们的序贯统计方法如何能够识别如此小的差异。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*reK3Rxz4nybHPEHYd6LZrQ.png" /&gt;&lt;figcaption&gt;图 7：异常关闭。 （上图）λᴮ(t)/λᴬ(t)（蓝色阴影）的置信序列，以及观察到的治疗（黑色虚线）和对照（蓝色虚线）的计数过程。 （下图）连续 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;案例研究 3：错误增加&lt;/h4&gt;&lt;p&gt;Netflix 还监控治疗和控制产生的错误数量。这是一个高基数度量，因为每个错误都用指示错误类型的代码进行注释。监控按代码分段的错误有助于开发人员快速诊断问题。图 8 显示了 Netflix 在客户端推出期间监控的一组错误代码的对数标度的连续 p 值。在此示例中，我们检测到治疗设备产生的&lt;a href="https://help.netflix.com/en/node/100573?q=3.1.18"&gt;3.1.18&lt;/a&gt;错误数量较多。遇到此错误的设备会显示以下消息：&lt;/p&gt;&lt;blockquote&gt; “我们现在玩这个游戏遇到了麻烦” &lt;/blockquote&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pef_CT_X6avRIALRT8Q7MA.png" /&gt;&lt;figcaption&gt;图 8：按错误代码列出的开始播放错误的顺序 p 值&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*byxD-v65SdLe8HRaAPGB4g.png" /&gt;&lt;figcaption&gt;图 9：观察到的错误 3.1.18 时间戳和治疗（蓝色）和对照（红色）的计数过程&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;了解&lt;em&gt;哪些&lt;/em&gt;错误增加了可以简化开发人员识别错误的过程。我们立即通过 Slack 集成向开发人员发送警报，例如以下内容&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/565/1*moE4xE6_A-o0l_6Z4cAK1A.png" /&gt;&lt;figcaption&gt;图 10：通过 Slack 集成发出的通知&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下次当您观看 Netflix 时遇到错误时，请知道我们正在处理它！&lt;/p&gt;&lt;h4&gt;试试看！&lt;/h4&gt;&lt;p&gt;我们&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;论文&lt;/a&gt;中概述的统计方法在实践中非常容易实施。您所需要的只是两个整数，即迄今为止在治疗和控制中观察到的事件数量。该代码可以在这个简短的&lt;a href="https://gist.github.com/michaellindon/5ce04c744d20755c3f653fbb58c2f4dd"&gt;GitHub gist&lt;/a&gt;中找到。下面是两个使用示例：&lt;/p&gt;&lt;pre&gt; &amp;gt; 计数 = [100, 101]&lt;br /&gt; &amp;gt; 分配概率 = [0.5, 0.5]&lt;br /&gt; &amp;gt;equential_p_value（计数，分配概率）&lt;br /&gt; 1&lt;br /&gt;&lt;br /&gt; &amp;gt; 计数 = [100, 201]&lt;br /&gt; &amp;gt; 分配概率 = [0.5, 0.5]&lt;br /&gt; &amp;gt;equential_p_value（计数，分配概率）&lt;br /&gt; 5.06061172163498e-06&lt;/pre&gt;&lt;p&gt;该&lt;a href="https://gist.github.com/michaellindon/5ce04c744d20755c3f653fbb58c2f4dd"&gt;代码&lt;/a&gt;推广到的不仅仅是两个治疗组。有关完整详细信息，包括超参数调整，请参阅本文的&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;第 4 节&lt;/a&gt;。&lt;/p&gt;&lt;h4&gt;进一步阅读&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://openreview.net/forum?id=a4zg0jiuVi"&gt;多项计数数据随时有效推理&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df"&gt;顺序 A/B 测试让世界保持流媒体 Netflix 第 1 部分：连续数据&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=da6805341642" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642"&gt;顺序测试让世界保持流媒体播放 Netflix 第 2 部分：计数进程&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Mon, 18 Mar 2024 12:46:46 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642?source=rss----2615bd06b42e---4</guid></item><item><title>【Supporting Diverse ML Systems at Netflix】支持 Netflix 的多种机器学习系统</title><link>https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/david-j-berg/"&gt;&lt;em&gt;David J. Berg&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/romain-cledat-4a211a5/"&gt;&lt;em&gt;Romain Cledat&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/seeleykayla/"&gt;&lt;em&gt;Kayla Seeley&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/shashanksrikanth/"&gt;&lt;em&gt;Shashank Srikanth&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt;&lt;a href="https://www.linkedin.com/in/chaoying-wang/"&gt;&lt;em&gt;王超英&lt;/em&gt;&lt;/a&gt;&lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/zitingyu/"&gt;&lt;em&gt;Darin Yu&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; Netflix 在公司的各个方面使用数据科学和机器学习，为从&lt;a href="https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b"&gt;内部基础设施&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f"&gt;内容需求建模&lt;/a&gt;到&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;媒体理解&lt;/a&gt;的广泛业务应用程序提供支持。 Netflix 的机器学习平台 (MLP) 团队围绕&lt;a href="https://metaflow.org"&gt;Metaflow&lt;/a&gt; （我们发起的开源机器学习基础架构框架）提供了完整的工具生态系统，使数据科学家和机器学习从业者能够构建和管理各种机器学习系统。&lt;/p&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9"&gt;自诞生以来&lt;/a&gt;，Metaflow 一直致力于提供人性化的 API，用于构建数据和 ML（以及今天的 AI）应用程序，并将它们顺利地部署在我们的生产基础设施中。虽然人性化的 API 令人愉悦，但真正赋予 Metaflow 超能力的是与我们的生产系统的集成。如果没有这些集成，项目将陷入原型设计阶段，或者必须作为我们工程团队维护的系统之外的异常值进行维护，从而产生不可持续的运营开销。&lt;/p&gt;&lt;p&gt;鉴于我们支持的机器学习和人工智能用例非常多样化——今天我们在内部部署了数百个 Metaflow 项目——我们不期望所有项目都遵循从原型到生产的相同路径。相反，我们提供了一个强大的基础层，与我们公司范围内的数据、计算和编排平台集成，以及将应用程序顺利部署到生产的各种路径。除此之外，团队还构建了自己的特定领域库来支持他们的特定用例和需求。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4hoAg4FX6oeua708alTMlA.png" /&gt;&lt;/figure&gt;&lt;p&gt;在本文中，我们将介绍为 Netflix 的 Metaflow 堆栈各个层提供的一些关键集成，如上图所示。我们还将展示依赖它们的现实机器学习项目，以了解我们支持的项目的广度。请注意，所有项目都利用多种集成，但我们在它们最显着使用的集成的上下文中突出显示它们。重要的是，所有用例都是由从业者自己设计的。&lt;/p&gt;&lt;p&gt;这些集成是通过&lt;a href="https://github.com/Netflix/metaflow-extensions-template"&gt;Metaflow 的扩展机制&lt;/a&gt;实现的，该机制是公开可用的，但可能会发生变化，因此还不是 Metaflow 稳定 API 的一部分。如果您对实现自己的扩展感到好奇，请在&lt;a href="http://chat.metaflow.org"&gt;Metaflow 社区 Slack&lt;/a&gt;上与我们联系。&lt;/p&gt;&lt;p&gt;让我们从最基本的集成开始逐层浏览堆栈。&lt;/p&gt;&lt;h3&gt;数据：快速数据&lt;/h3&gt;&lt;p&gt;我们的主要数据湖&lt;a href="https://www.youtube.com/watch?v=jMFMEk8jFu8"&gt;托管在 S3 上，组织为 Apache Iceberg 表&lt;/a&gt;。对于 ETL 和其他繁重的数据处理，我们主要依赖 Apache Spark。除了 Spark 之外，我们还希望支持 Python 中的最后一英里数据处理，解决特征转换、批量推理和训练等用例。有时，这些用例涉及 TB 级的数据，因此我们必须注意性能。&lt;/p&gt;&lt;p&gt;为了实现对 Netflix 数据仓库的快速、可扩展且稳定的访问，我们为 Metaflow 开发了一个&lt;em&gt;快速&lt;/em&gt;数据库，它利用了来自 Python 数据生态系统的高性能组件： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OGn9AcNNdMXAhxLq8WugkQ.png" /&gt;&lt;/figure&gt;&lt;p&gt;如图所示，Fast Data 库由两个主要接口组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; Table 对象负责与 Netflix 数据仓库交互，其中包括解析 Iceberg（或旧版 Hive）表元数据、解析分区和 Parquet 文件以供读取。最近，我们添加了对写入路径的支持，因此也可以使用该库更新表。&lt;/li&gt;&lt;li&gt;一旦我们发现要处理的 Parquet 文件，MetaflowDataFrame 就会接管：它使用 Metaflow 的高吞吐量 S3 客户端将数据直接下载到进程的内存，这&lt;a href="https://outerbounds.com/blog/metaflow-fast-data/"&gt;通常优于读取本地文件&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们使用&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;来解码 Parquet 并托管数据的内存表示。用户可以选择最合适的工具来操作数据，例如&lt;a href="https://pandas.pydata.org"&gt;Pandas&lt;/a&gt;或&lt;a href="https://pola.rs"&gt;Polars&lt;/a&gt;来使用数据帧 API，或者使用我们的内部 C++ 库之一来进行各种高性能操作。感谢 Arrow，可以通过这些库以零拷贝的方式访问数据。&lt;/p&gt;&lt;p&gt;我们还关注依赖问题：(Py)Arrow 是许多 ML 和数据库的依赖项，因此我们不希望我们的自定义 C++ 扩展依赖于特定版本的 Arrow，这很容易导致无法解析的依赖关系图。相反，在&lt;a href="https://github.com/apache/arrow-nanoarrow"&gt;nanoarrow&lt;/a&gt;的风格中，我们的 Fast Data 库仅依赖于&lt;a href="https://arrow.apache.org/docs/format/CDataInterface.html"&gt;稳定的 Arrow C 数据接口&lt;/a&gt;，生成一个密封的库，没有外部依赖性。&lt;/p&gt;&lt;h4&gt;示例用例：内容知识图&lt;/h4&gt;&lt;p&gt;我们的娱乐世界知识图对电影或电视剧的标题、演员和其他属性之间的关系进行编码，支持 Netflix 业务的各个方面。&lt;/p&gt;&lt;p&gt;创建知识图的一个关键挑战是实体解析。关于标题的信息可能有许多不同的表示，这些表示略有不同或相互冲突，必须予以解决。这通常是通过每个实体的成对匹配过程来完成的，这在规模化时变得非常重要。&lt;/p&gt;&lt;p&gt;该项目利用快速数据和水平扩展以及&lt;a href="https://docs.metaflow.org/v/r/metaflow/basics#foreach"&gt;Metaflow 的 foreach 结构&lt;/a&gt;来加载存储在 Netflix 数据仓库中的大量标题信息（大约十亿对），因此这些对可以在许多 Metaflow 任务中并行匹配。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KG7TUCqTF3uRU6lUncCHtg.png" /&gt;&lt;/figure&gt;&lt;p&gt;我们使用 metaflow.Table 来解析分配给 Metaflow 任务的所有输入分片，这些任务负责共同处理 TB 级的数据。每个任务都使用 metaflow.MetaflowDataFrame 加载数据，使用 Pandas 执行匹配，并在输出表中填充相应的分片。最后，当所有匹配完成并写入数据时，新表将被提交，以便其他作业可以读取它。&lt;/p&gt;&lt;h3&gt;计算：提图斯&lt;/h3&gt;&lt;p&gt;Metaflow 的开源用户依赖&lt;a href="https://docs.metaflow.org/scaling/remote-tasks/introduction"&gt;AWS Batch 或 Kubernetes 作为计算后端&lt;/a&gt;，而我们则依赖&lt;a href="https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436"&gt;我们的集中式计算平台 Titus&lt;/a&gt; 。在底层，Titus&lt;a href="https://www.slideshare.net/aspyker/herding-kats-netflixs-journey-to-kubernetes-public"&gt;由 Kubernetes 提供支持&lt;/a&gt;，但它比现成的 Kubernetes 提供了一层厚厚的增强功能，&lt;a href="https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225"&gt;使其更具可观察性&lt;/a&gt;、 &lt;a href="https://netflixtechblog.com/evolving-container-security-with-linux-user-namespaces-afbe3308c082"&gt;安全性&lt;/a&gt;、&lt;a href="https://netflixtechblog.com/auto-scaling-production-services-on-titus-1f3cd49f5cd7"&gt;可扩展性&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7"&gt;成本效益&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;通过以 @titus 为目标，Metaflow 任务可以从这些久经考验的开箱即用功能中受益，无需 ML 工程师或数据科学家具备深入的技术知识或工程知识。然而，为了从可扩展的计算中受益，我们需要帮助开发人员以可重现的方式（最好是快速）在远程 Pod 中打包和补充项目的整个执行环境。具体来说，我们不想要求开发人员手动管理自己的 Docker 镜像，这很快会导致比解决的问题更多的问题。&lt;/p&gt;&lt;p&gt;这就是&lt;a href="https://docs.metaflow.org/scaling/dependencies"&gt;Metaflow 提供开箱即用的依赖管理支持的&lt;/a&gt;原因。最初，我们只支持 @conda，但基于我们在&lt;a href="https://github.com/Netflix/metaflow-nflx-extensions"&gt;可移植执行环境&lt;/a&gt;方面的工作，开源 Metaflow 几个月前也获得了对&lt;a href="https://outerbounds.com/blog/pypi-announcement/"&gt;@pypi&lt;/a&gt;&lt;a href="https://outerbounds.com/blog/pypi-announcement/"&gt;的支持&lt;/a&gt;。&lt;/p&gt;&lt;h4&gt;示例用例：构建模型解释器&lt;/h4&gt;&lt;p&gt;这是一个关于可移植执行环境有用性的有趣示例。对于我们的许多应用程序来说，模型的可解释性很重要。利益相关者喜欢了解为什么模型会产生特定的输出以及为什么它们的行为会随着时间的推移而变化。&lt;/p&gt;&lt;p&gt;有多种方法可以为模型提供可解释性，但一种方法是基于每个经过训练的模型来训练解释器模型。无需详细说明这是如何完成的，只需说 Netflix 训练了很多模型，因此我们也需要训练很多解释器。&lt;/p&gt;&lt;p&gt;借助 Metaflow，我们可以允许每个应用程序为其用例选择最佳的建模方法。相应地，每个应用程序都有自己定制的依赖项集。因此，训练解释器模型需要：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;访问原始模型及其训练环境，以及&lt;/li&gt;&lt;li&gt;特定于构建解释器模型的依赖关系。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这对依赖管理提出了一个有趣的挑战：我们需要一个高阶训练系统，即下图中的“Explainer flow”，它能够将另一个训练系统的完整执行环境作为输入，并基于它生成模型。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WoHEm2yvuo22NRp4qf0W9g.png" /&gt;&lt;/figure&gt;&lt;p&gt;解释器流由上游流事件触发，例如图中的模型 A、B、C 流。 build_environment 步骤使用&lt;a href="https://github.com/Netflix/metaflow-nflx-extensions"&gt;我们的可移植环境&lt;/a&gt;提供的元流环境命令来构建一个环境，其中包括输入模型的要求以及构建解释器模型本身所需的要求。&lt;/p&gt;&lt;p&gt;构建的环境被赋予一个唯一的名称，该名称取决于运行标识符（以提供唯一性）以及模型类型。给定这个环境，train_explainer 步骤就能够引用这个唯一命名的环境，并在既可以访问输入模型又可以训练解释器模型的环境中运行。请注意，与使用 vanilla @conda 或 @pypi 的典型流程不同，可移植环境扩展允许用户在执行时直接获取这些环境，而不是在部署时，因此允许用户（如本例所示）解析环境在下一步使用它之前。&lt;/p&gt;&lt;h3&gt;编曲：大师&lt;/h3&gt;&lt;p&gt;如果数据是机器学习的燃料，计算层是肌肉，那么神经一定是编排层。几年前，当&lt;a href="https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280"&gt;我们发布对 AWS Step Functions 的支持&lt;/a&gt;时，我们已经讨论过 Metaflow 背景下生产级工作流编排器的重要性。从那时起，开源 Metaflow 获得了对 Kubernetes 原生编排器&lt;a href="https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow/"&gt;Argo Workflows&lt;/a&gt;的支持，以及&lt;a href="https://outerbounds.com/blog/better-airflow-with-metaflow/"&gt;对至今仍被数据工程团队广泛使用的 Airflow 的支持&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在内部，我们使用&lt;a href="https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c"&gt;名为 Maestro 的生产工作流程协调器&lt;/a&gt;。 Maestro 帖子分享了有关系统如何支持可扩展性、高可用性和可用性的详细信息，这为我们生产中的所有 Metaflow 项目提供了支柱。&lt;/p&gt;&lt;p&gt;一个经常被忽视的非常重要的细节是&lt;a href="https://docs.metaflow.org/production/event-triggering"&gt;事件触发&lt;/a&gt;：它允许团队使用与整个组织，如下面的示例用例所示。&lt;/p&gt;&lt;h4&gt;示例用例：内容决策&lt;/h4&gt;&lt;p&gt;Metaflow 上运行的最关键的业务系统之一&lt;a href="https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f"&gt;支持我们的内容决策&lt;/a&gt;，即 Netflix 应该为服务带来什么内容的问题。我们为来自 190 多个国家/地区的超过 2.6 亿订阅者提供支持，他们代表着截然不同的文化和品味，我们希望我们的内容能够让他们满意。反映挑战的广度和深度，针对该问题的系统和模型已经变得非常复杂。&lt;/p&gt;&lt;p&gt;我们从多个角度解决这个问题，但我们拥有一组核心数据管道和模型，为决策提供基础。为了说明核心组件的复杂性，请考虑以下高级图表： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rp4sF-nIWgTt8kdt" /&gt;&lt;/figure&gt;&lt;p&gt;在此图中，灰色框表示与下游和上游合作伙伴团队的集成，绿色框是各种 ETL 管道，蓝色框是 Metaflow 流。这些盒子封装了数百个高级模型和复杂的业务逻辑，每天处理大量数据。&lt;/p&gt;&lt;p&gt;尽管该系统很复杂，但它是由相对较小的工程师和数据科学家团队自主管理的。 Metaflow 的一些关键功能使这成为可能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;所有的盒子都是事件触发的，由 Maestro 精心策划。 Metaflow 流之间的依赖关系通过&lt;a href="https://docs.metaflow.org/production/event-triggering/flow-events"&gt;@trigger_on_finish&lt;/a&gt;触发，通过&lt;a href="https://docs.metaflow.org/production/event-triggering/external-events"&gt;@trigger&lt;/a&gt;触发对外部系统的依赖关系。&lt;/li&gt;&lt;li&gt;通过&lt;a href="https://docs.metaflow.org/scaling/tagging"&gt;Metaflow 命名空间&lt;/a&gt;实现快速开发，因此个人开发人员可以在不干扰生产部署的情况下进行开发。&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.metaflow.org/production/coordinating-larger-metaflow-projects"&gt;分支开发和部署是通过&lt;/a&gt;&lt;a href="https://docs.metaflow.org/production/coordinating-larger-metaflow-projects"&gt;@project&lt;/a&gt;进行管理的，它还&lt;a href="https://docs.metaflow.org/production/event-triggering/project-events"&gt;隔离了不同分支之间的事件&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该团队还开发了自己的特定领域库和配置管理工具，帮助他们改进和操作系统。&lt;/p&gt;&lt;h3&gt;部署：缓存&lt;/h3&gt;&lt;p&gt;为了产生商业价值，我们所有的 Metaflow 项目都被部署为与其他生产系统配合使用。在许多情况下，集成可能是通过数据仓库中的共享表进行的。在其他情况下，通过低延迟 API 共享结果会更方便。&lt;/p&gt;&lt;p&gt;值得注意的是，并非所有基于 API 的部署都需要实时评估，我们将在下面的部分中介绍这一点。我们拥有许多关键业务应用程序，可以预先计算部分或全部预测，从而保证全球范围内尽可能低的延迟和操作简单的高可用性。&lt;/p&gt;&lt;p&gt;我们开发了一个官方支持的模式来涵盖此类用例。虽然系统依赖于我们的内部缓存基础设施，但您可以使用&lt;a href="https://aws.amazon.com/elasticache/"&gt;Amazon ElasticCache&lt;/a&gt;或&lt;a href="https://aws.amazon.com/dynamodb/"&gt;DynamoDB&lt;/a&gt;等服务遵循相同的模式。&lt;/p&gt;&lt;h4&gt;示例用例：内容性能可视化&lt;/h4&gt;&lt;p&gt;决策者利用片名的历史表现来理解和改进电影和连续剧目录。性能指标可能很复杂，并且通常最容易被人类通过可视化来理解，这些可视化可以交互式地跨感兴趣的参数分解指标。内容决策者可以通过使用metaflow.Cache构建的实时Web应用程序来实现自助可视化，该应用程序可以通过metaflow.Hosting提供的API进行访问。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*i3YtLUvobXxEYE6crwQy_w.png" /&gt;&lt;/figure&gt;&lt;p&gt;每日计划的 Metaflow 作业并行计算感兴趣的总量。该作业使用 metaflow.Cache 将大量结果写入在线键值存储。 &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;应用程序包含可视化软件和数据聚合逻辑。用户可以动态更改可视化应用程序的参数，并实时将消息发送到简单的&lt;a href="https://netflixtechblog.com/feed#a890"&gt;Metaflow 托管服务&lt;/a&gt;，该服务在缓存中查找值、执行计算，并将结果作为 JSON blob 返回到 Streamlit 应用程序。&lt;/p&gt;&lt;h3&gt;部署：Metaflow 托管&lt;/h3&gt;&lt;p&gt;对于需要 API 和实时评估的部署，我们提供集成模型托管服务 Metaflow Hosting。尽管细节已经发生了很大变化，但&lt;a href="https://www.youtube.com/watch?v=sBM5cSBGZS4"&gt;这个老话仍然很好地概述了该服务&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; Metaflow Hosting 专门用于托管 Metaflow 中生成的工件或模型。这在 Netflix 现有的微服务基础设施之上提供了一个易于使用的界面，使数据科学家能够快速将他们的工作从实验转移到生产级 Web 服务，该服务可以通过 HTTP REST API 以最小的开销使用。&lt;/p&gt;&lt;p&gt;其主要优点包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用于创建 RESTFull 端点的简单装饰器语法。&lt;/li&gt;&lt;li&gt;后端根据流量自动调整用于支持服务的实例数量。&lt;/li&gt;&lt;li&gt;如果在指定时间后没有向后端发出请求，则后端将缩放至零，从而节省成本，特别是如果您的服务需要 GPU 有效地产生响应。&lt;/li&gt;&lt;li&gt;请求记录、警报、监控和跟踪 Netflix 基础设施的挂钩&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑该服务类似于&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html"&gt;AWS Sagemaker Model Hosting&lt;/a&gt;等托管模型托管服务，但与我们的微服务基础设施紧密集成。&lt;/p&gt;&lt;h4&gt;示例用例：媒体&lt;/h4&gt;&lt;p&gt;我们在使用机器学习来处理媒体资产方面有着悠久的历史，例如，&lt;a href="https://netflixtechblog.com/artwork-personalization-c589f074ad76"&gt;个性化艺术品&lt;/a&gt;并帮助我们的 &lt;a href="https://netflixtechblog.com/new-series-creating-media-with-machine-learning-5067ac110bcd"&gt;创意人员有效地创建促销内容&lt;/a&gt;。处理大量媒体资产在技术上并不简单，而且计算成本高昂，因此多年来，我们开发了大量专门用于此目的的&lt;a href="https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359"&gt;专用基础设施&lt;/a&gt;，特别是&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;支持媒体 ML 用例的基础设施&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;为了演示 Metaflow Hosting 提供支持同步和异步查询的通用 API 层的优势，请考虑涉及&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;Amber（我们的媒体功能存储）的&lt;/a&gt;用例。&lt;/p&gt;&lt;p&gt;虽然 Amber 是一个特征&lt;em&gt;存储&lt;/em&gt;，但提前预计算和存储所有媒体特征是不可行的。相反，我们按需计算和缓存特征，如下所示： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SP7GASNee-YB_dDu35ldJA.png" /&gt;&lt;/figure&gt;&lt;p&gt;当一项服务向 Amber 请求一项功能时，它会计算功能依赖关系图，然后向 Metaflow Hosting 发送一个或多个异步请求，Metaflow Hosting 将请求放入队列中，最终在计算资源可用时触发功能计算。 Metaflow Hosting 会缓存响应，因此 Amber 可以在一段时间后获取它。我们本可以专门为此用例构建专用的微服务，但由于 Metaflow Hosting 的灵活性，我们能够更快地交付该功能，而无需额外的操作负担。&lt;/p&gt;&lt;h3&gt;未来的工作&lt;/h3&gt;&lt;p&gt;我们对在不同用例中应用机器学习的兴趣只会越来越大，因此我们的 Metaflow 平台将相应地不断扩大其足迹，并继续为 Netlfix 其他团队构建的系统提供令人愉快的集成。例如，我们计划通过为工件和模型管理提供更多选项来改进版本控制层（本文未涵盖）。&lt;/p&gt;&lt;p&gt;我们还计划与 Netflix 姐妹团队正在开发的其他系统建立更多集成。例如，Metaflow 托管模型目前尚未很好地集成到模型日志记录设施中 - 我们计划致力于改进这一点，以使使用 Metaflow 开发的模型与训练新模型至关重要的反馈循环更加集成。我们希望以可插入的方式做到这一点，允许其他用户与他们自己的日志系统集成。&lt;/p&gt;&lt;p&gt;此外，我们希望提供更多方式将 Metaflow 工件和模型集成到非 Metaflow 环境和应用程序中，例如基于 JVM 的边缘服务，以便基于 Python 的数据科学家可以轻松地为非 Python 工程系统做出贡献。这将使我们能够更好地弥合 Metaflow 提供的快速迭代（使用 Python）与服务 Netflix 会员面临的请求的基础设施所施加的要求和约束之间的差距。&lt;/p&gt;&lt;p&gt;如果您正在组织中构建业务关键型 ML 或 AI 系统，&lt;a href="http://chat.metaflow.org"&gt;请加入 Metaflow Slack 社区&lt;/a&gt;！我们很乐意分享经验、回答任何问题，并欢迎您为 Metaflow 做出贡献。&lt;/p&gt;&lt;h4&gt;致谢：&lt;/h4&gt;&lt;p&gt;感谢 Wenbing Bai、Jan Florjanczyk、Michael Li、Aliki Mavromoustaki 和 Sejal Rai 在用例和数据方面提供的帮助。感谢我们的 OSS 贡献者使 Metaflow 成为更好的产品。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d2e6b6d205d" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d"&gt;《Supporting Diverse ML Systems at Netflix》&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix TechBlog&lt;/a&gt; ，人们通过强调和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 07 Mar 2024 18:33:07 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d?source=rss----2615bd06b42e---4</guid></item><item><title>【Bending pause times to your will with Generational ZGC】通过 Generational ZGC 将暂停时间调整为您的意愿</title><link>https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;em&gt;Z 垃圾收集器中几代人的令人惊讶和不那么令人惊讶的好处。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;作者：Danny Thomas，JVM 生态系统团队&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GuEZ-RMhzNnYgLQd" /&gt;&lt;/figure&gt;&lt;p&gt;JDK 的最新长期支持版本为&lt;a href="https://docs.oracle.com/en/java/javase/21/gctuning/z-garbage-collector.html"&gt;Z 垃圾收集器&lt;/a&gt;提供了世代支持。由于并发垃圾收集的显着优势，Netflix 在 JDK 21 及更高版本上默认从 G1 切换到分代 ZGC。&lt;/p&gt;&lt;p&gt;我们超过一半的关键流视频服务现在都在带有 Generational ZGC 的 JDK 21 上运行，因此现在是谈论我们的经验和我们所看到的好处的好时机。如果您对我们如何在 Netflix 使用 Java 感兴趣，Paul Bakker 的演讲&lt;a href="https://www.infoq.com/presentations/netflix-java/"&gt;《Netflix 如何真正使用 Java&lt;/a&gt; 》是一个很好的起点。&lt;/p&gt;&lt;h3&gt;减少尾部延迟&lt;/h3&gt;&lt;p&gt;在我们的 GRPC 和&lt;a href="https://netflix.github.io/dgs/"&gt;DGS 框架&lt;/a&gt;服务中，GC 暂停是尾延迟的重要来源。对于我们的 GRPC 客户端和服务器来说尤其如此，其中由于超时而导致的请求取消与重试、对冲和回退等可靠性功能相互作用。每个错误都是取消的请求，导致重试，因此这种减少进一步按以下速率减少总体服务流量： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SCVt4VGlA517hZDi" /&gt;&lt;figcaption&gt;每秒的错误率。上周（白色）与当前取消率（紫色），因为 ZGC 已于 11 月 16 日在服务集群上启用&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;消除暂停噪音还使我们能够识别端到端延迟的实际来源，否则这些延迟来源将隐藏在噪音中，因为最大暂停时间异常值可能很重要： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rW029WscxSKDQRQ6" /&gt;&lt;figcaption&gt;对于与上述相同的服务集群，按原因划分的最大 GC 暂停时间。是的，那些 ZGC 暂停通常低于一毫秒&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;效率&lt;/h3&gt;&lt;p&gt;即使我们在评估中看到了非常有希望的结果，我们仍然期望采用 ZGC 是一种权衡：由于存储和加载障碍、在线程本地握手中执行的工作以及 GC 与应用程序竞争，应用程序吞吐量会稍微减少为了资源。我们认为这是一个可以接受的权衡，因为避免暂停所带来的好处将超过开销。&lt;/p&gt;&lt;p&gt;事实上，我们发现我们的服务和架构不存在这样的权衡。对于给定的 CPU 利用率目标，与 G1 相比，ZGC 改善了平均延迟和 P99 延迟，且 CPU 利用率相同或更高。&lt;/p&gt;&lt;p&gt;我们在许多服务中看到的请求率、请求模式、响应时间和分配率的一致性无疑对 ZGC 有帮助，但我们发现它同样能够处理不太一致的工作负载（当然有例外；更多内容见下文）。&lt;/p&gt;&lt;h3&gt;操作简单&lt;/h3&gt;&lt;p&gt;服务所有者经常向我们提出有关暂停时间过长的问题并寻求调整方面的帮助。我们有几个框架可以定期刷新大量堆上数据，以避免外部服务调用以提高效率。这些堆上数据的定期刷新非常适合让 G1 感​​到惊讶，从而导致暂停时间异常值远远超出默认的暂停时间目标。&lt;/p&gt;&lt;p&gt;这种长期存在的堆上数据是我们之前没有采用非分代 ZGC 的主要因素。在我们评估的最坏情况下，对于相同的工作负载，非分代 ZGC 的 CPU 利用率比 G1 高出 36%。一代 ZGC 的性能提升了近 10%。&lt;/p&gt;&lt;p&gt;流视频所需的所有服务中有一半使用我们的&lt;a href="https://hollow.how/)"&gt;Hollow&lt;/a&gt;库来获取堆上元数据。消除暂停这一问题使我们能够&lt;a href="https://github.com/Netflix/hollow/commit/4f21ab593543bb622d9ccea2f8e6295eae5e8080"&gt;消除阵列池缓解措施&lt;/a&gt;，从而释放数百兆字节的内存以进行分配。&lt;/p&gt;&lt;p&gt;操作简单性也源于 ZGC 的启发式和默认设置。无需显式调整即可实现这些结果。分配停滞很少见，通常与分配率的异常峰值同时发生，并且比我们在 G1 中看到的平均暂停时间短。&lt;/p&gt;&lt;h3&gt;内存开销&lt;/h3&gt;&lt;p&gt;我们预计，由于&lt;a href="https://youtu.be/YyXjC68l8mw?t=816"&gt;彩色指针需要 64 位对象指针&lt;/a&gt;，因此在小于 32G 的堆上丢失&lt;a href="https://shipilev.net/jvm/anatomy-quarks/23-compressed-references/"&gt;压缩引用&lt;/a&gt;将成为选择垃圾收集器的主要因素。&lt;/p&gt;&lt;p&gt;我们发现，虽然这是 stop-the-world GC 的一个重要考虑因素，但 ZGC 的情况并非如此，即使在小堆上，分配率的增加也会因效率和操作改进而摊销。我们感谢 Oracle 的 Erik Österlund 解释了彩色指针在并发垃圾收集器方面不太直观的好处，这使我们能够比最初计划更广泛地评估 ZGC。&lt;/p&gt;&lt;p&gt;在大多数情况下，ZGC 还能够持续为应用程序提供更多可用内存： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3eTNEdI2mHfL1Yvk" /&gt;&lt;figcaption&gt;对于与上述相同的服务集群，每个 GC 周期后的已用堆容量与可用堆容量&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;ZGC 的固定开销为堆大小的 3%，需要比 G1 更多的本机内存。除了少数情况外，无需降低最大堆大小以留出更多空间，而这些服务的本机内存需求高于平均水平。&lt;/p&gt;&lt;p&gt;参考处理也仅在 ZGC 的主要集合中执行。我们特别关注直接字节缓冲区的重新分配，但到目前为止我们还没有看到任何影响。引用处理中的这种差异确实导致了&lt;a href="https://bugs.openjdk.org/browse/JDK-8321178"&gt;JSON 线程转储支持的性能问题&lt;/a&gt;，但这是一种不寻常的情况，因为框架意外地为每个请求创建了未使用的 ExecutorService 实例。&lt;/p&gt;&lt;h3&gt;透明大页&lt;/h3&gt;&lt;p&gt;即使您不使用 ZGC，您也可能应该使用大页面，而&lt;a href="https://shipilev.net/jvm/anatomy-quarks/2-transparent-huge-pages/"&gt;透明大页面&lt;/a&gt;是使用它们的最方便的方式。&lt;/p&gt;&lt;p&gt; ZGC 对堆使用共享内存，并且许多 Linux 发行版将 shmem_enabled 配置为&lt;em&gt;never&lt;/em&gt; ，这会通过 -XX:+UseTransparentHugePages 默默地阻止 ZGC 使用大页面。&lt;/p&gt;&lt;p&gt;这里我们部署了一个服务，没有进行任何其他更改，但 shmem_enabled 从 never 变为建议，从而显着降低了 CPU 利用率： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bGoc3W9P_E2kjghe" /&gt;&lt;figcaption&gt;部署从 4k 页面变为 2m 页面。忽略差距，这是我们的不可变部署过程暂时将集群容量加倍&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们的默认配置：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将堆最小值和最大值设置为相等大小&lt;/li&gt;&lt;li&gt;配置 -XX:+UseTransparentHugePages -XX:+AlwaysPreTouch&lt;/li&gt;&lt;li&gt;使用以下透明_hugepage 配置：&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;回声疯狂 | sudo tee /sys/kernel/mm/transparent_hugepage/enabled&lt;br /&gt;回声建议| sudo tee /sys/kernel/mm/transparent_hugepage/shmem_enabled&lt;br /&gt;回显延迟| sudo tee /sys/kernel/mm/transparent_hugepage/defrag&lt;br /&gt;回声 1 | sudo tee /sys/kernel/mm/transparent_hugepage/khugpaged/defrag&lt;/pre&gt;&lt;h3&gt;哪些工作负载不适合？&lt;/h3&gt;&lt;p&gt;没有最好的垃圾收集器。每个都根据垃圾收集器的目标权衡收集吞吐量、应用程序延迟和资源利用率。&lt;/p&gt;&lt;p&gt;对于使用 G1 与 ZGC 相比表现更好的工作负载，我们发现它们往往更注重吞吐量，分配率非常高，并且长时间运行的任务在不可预测的时间内保存对象。&lt;/p&gt;&lt;p&gt;一个值得注意的例子是一个服务，其中的分配率非常高，并且存在大量长期存在的对象，这恰好非常适合 G1 的暂停时间目标和旧区域收集启发式方法。它使 G1 能够避免 GC 周期中的非生产性工作，而 ZGC 则无法做到这一点。&lt;/p&gt;&lt;p&gt;默认情况下切换到 ZGC 为应用程序所有者提供了考虑垃圾收集器选择的绝佳机会。一些批处理/预计算案例默认使用 G1，他们会看到并行收集器具有更好的吞吐量。在一个大型预计算工作负载中，我们发现应用程序吞吐量提高了 6-8%，与 G1 相比，批处理时间缩短了一个小时。&lt;/p&gt;&lt;h3&gt;亲自尝试一下！&lt;/h3&gt;&lt;p&gt;如果不加质疑，假设和期望可能会导致我们错过十年来我们对运营默认值所做的最有影响力的改变之一。我们鼓励您亲自尝试世代 ZGC。它可能会让您感到惊讶，就像我们感到惊讶一样。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=256629c9386b" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b"&gt;使用 Generational ZGC 让暂停时间随心所欲&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Wed, 06 Mar 2024 01:35:08 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b?source=rss----2615bd06b42e---4</guid></item><item><title>【Evolving from Rule-based Classifier: Machine Learning Powered Auto Remediation in Netflix Data…】从基于规则的分类器演变而来：机器学习支持 Netflix 数据中的自动修复……</title><link>https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b?source=rss----2615bd06b42e---4</link><description>&lt;h3&gt;从基于规则的分类器演变而来：Netflix 数据平台中机器学习支持的自动修复&lt;/h3&gt;&lt;p&gt;作者： &lt;a href="https://www.linkedin.com/in/binbing-hou/overlay/about-this-profile/"&gt;Binbing Hou&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/stephanievezich/overlay/about-this-profile/"&gt;Stephanie Vezich Tamayo&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/chenxiao000/overlay/about-this-profile/"&gt;Xiao Chen&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/liangtian/overlay/about-this-profile/"&gt;Liang Tian&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/troy-ristow-4899b49/overlay/about-this-profile/"&gt;Troy Ristow&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/haoyuanwang/overlay/about-this-profile/"&gt;Haoyuan Wang&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/snehalchennuru/overlay/about-this-profile/"&gt;Snehal Chennuru&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/pawan-dixit-b4307b2/overlay/about-this-profile/"&gt;Pawan Dixit&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;这是 Netflix 关于利用数据洞察和机器学习 (ML) 围绕大数据作业的性能和成本效率提高运营自动化的系列工作中的第一篇。操作自动化——包括但不限于自动诊断、自动修复、自动配置、自动调整、自动缩放、自动调试和自动测试——是现代数据平台成功的关键。在这篇博文中，我们介绍了我们的自动修复项目，该项目将当前使用的基于规则的分类器与机器学习服务集成在一起，旨在自动修复失败的作业，无需人工干预。我们在生产中部署了自动修复，用于处理 Spark 作业的内存配置错误和未分类错误，并观察了其效率和效果（例如，自动修复 56% 的内存配置错误，节省了所有错误造成的 50% 的金钱成本），效果非常好。进一步改进的潜力。&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;p&gt;在 Netflix，每天有数十万个工作流程和数百万个作业在大数据平台的多个层上运行。鉴于这种分布式大型系统固有的广泛范围和错综复杂的复杂性，即使失败的作业只占总工作负载的一小部分，诊断和修复作业失败也会造成相当大的运营负担。&lt;/p&gt;&lt;p&gt;为了高效地处理错误，Netflix 开发了一种名为 Pitive 的错误分类服务，它利用基于规则的分类器进行错误分类。基于规则的分类器根据一组预定义的规则对作业错误进行分类，并为调度程序提供见解以决定是否重试作业，并为工程师提供诊断和修复作业失败的信息。&lt;/p&gt;&lt;p&gt;然而，随着系统规模和复杂性的增加，基于规则的分类器由于对操作自动化的支持有限，特别是在处理内存配置错误和未分类错误方面一直面临挑战。因此，运营成本随着失败作业的数量线性增加。在某些情况下，例如，诊断和修复由内存不足 (OOM) 错误引起的作业失败，需要跨团队的共同努力，不仅涉及用户本身，还涉及支持工程师和领域专家。&lt;/p&gt;&lt;p&gt;为了应对这些挑战，我们开发了一项名为&lt;em&gt;“自动修复”&lt;/em&gt;的新功能，它将基于规则的分类器与机器学习服务集成在一起。基于基于规则的分类器的分类，它使用机器学习服务来预测重试成功概率和重试成本，并选择最佳候选配置作为建议；以及自动应用建议的配置服务。其主要优点如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;集成智能。&lt;/strong&gt;自动修复并没有完全弃用当前基于规则的分类器，而是将分类器与 ML 服务集成，以便它可以利用两者的优点：基于规则的分类器为每个错误类别提供静态、确定性的分类结果，该结果基于领域专家的背景； ML 服务利用 ML 的强大功能，为每个作业提供性能和成本感知建议。通过集成智能，我们可以很好地满足修复不同错误的要求。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;完全自动化。&lt;/strong&gt;错误分类、获取建议和应用建议的流程是完全自动化的。它将建议连同重试决策一起提供给调度程序，并且特别使用在线配置服务来存储和应用推荐的配置。这样，修复过程中就不需要人为干预。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;多目标优化。&lt;/strong&gt;自动修复通过考虑性能（即重试成功概率）和计算成本效率（即运行作业的货币成本）来生成建议，以避免盲目推荐资源消耗过多的配置。例如，对于内存配置错误，它会搜索与作业执行的内存使用相关的多个参数，并推荐最小化故障概率和计算成本的线性组合的组合。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些优势已通过修复 Spark 作业故障的生产部署得到验证。我们的观察表明，自动修复&lt;em&gt; &lt;/em&gt;通过在线应用推荐的内存配置，无需人工干预即可成功修复约 56% 的内存配置错误；同时，由于它能够推荐新配置以使内存配置成功并禁止对未分类错误进行不必要的重试，因此可降低约 50% 的成本。我们还注意到通过模型调整进一步改进的巨大潜力（请参阅生产中的推出部分）。&lt;/p&gt;&lt;h3&gt;基于规则的分类器：基础知识和挑战&lt;/h3&gt;&lt;h4&gt;基本&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pnViNRB4q-LX7rcdn6MgHA.png" /&gt;&lt;/figure&gt;&lt;p&gt;图1展示了数据平台中的错误分类服务，即Pitive。它利用基于规则的分类器，由三个组件组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;日志收集器&lt;/strong&gt;负责从不同平台层（例如调度程序、作业编排器和计算集群）提取日志以进行错误分类。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;规则执行引擎&lt;/strong&gt;负责将收集的日志与一组预定义的规则进行匹配。规则包括（1）错误的名称、来源、日志和摘要以及错误是否可重新启动； (2) 用于从日志中识别错误的正则表达式。例如，名称为 SparkDriverOOM 的规则包含的信息指示，如果 Spark 作业的 stdout 日志可以匹配正则表达式&lt;em&gt;SparkOutOfMemoryError:&lt;/em&gt; ，则此错误被分类为用户错误，不可重新启动。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;结果终结器&lt;/strong&gt;负责根据匹配的规则最终确定错误分类结果。如果匹配一条或多条规则，则第一个匹配的规则的分类决定最终的分类结果（规则优先级由规则排序决定，第一个规则优先级最高）。另一方面，如果没有匹配的规则，则该错误将被视为未分类。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;挑战&lt;/h4&gt;&lt;p&gt;虽然基于规则的分类器简单且有效，但由于其处理错误配置引起的错误和对新错误进行分类的能力有限，因此面临着挑战：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;内存配置错误。&lt;/strong&gt;基于规则的分类器提供错误分类结果，指示是否重新启动作业；然而，对于非暂时性错误，仍然需要工程师手动修复。最值得注意的例子是内存配置错误。此类错误通常是由于作业内存配置错误引起的。设置过小的内存可能会导致内存不足 (OOM) 错误，而设置过大的内存会浪费集群内存资源。更具挑战性的是，一些内存配置错误需要更改多个参数的配置。因此，设置合适的内存配置不仅需要手动操作，还需要Spark作业执行的专业知识。此外，即使作业的内存配置最初调整得很好，数据大小和作业定义等更改也可能导致性能下降。鉴于数据平台每月观察到约 600 个内存配置错误，仅及时修复内存配置错误就需要付出不小的工程努力。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未分类的错误。&lt;/strong&gt;基于规则的分类器依靠数据平台工程师根据已知的上下文手动添加识别错误的规则；否则，错误将无法分类。由于数据平台不同层的迁移和应用的多样性，现有规则可能无效，添加新规则需要工程工作，也取决于部署周期。分类器中已添加 300 多个规则，但所有故障中约有 50% 仍未分类。对于未分类的错误，作业可能会使用默认重试策略多次重试。如果错误是非暂时性的，这些失败的重试会产生不必要的作业运行成本。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;演变为自动修复：服务架构&lt;/h3&gt;&lt;h4&gt;方法&lt;/h4&gt;&lt;p&gt;为了解决上述挑战，我们的基本方法是将基于规则的分类器与机器学习服务集成以生成建议，并使用配置服务自动应用建议：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;生成建议。&lt;/strong&gt;我们使用基于规则的分类器作为第一遍，根据预定义的规则对所有错误进行分类，并使用 ML 服务作为第二遍，为内存配置错误和未分类错误提供建议。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;应用建议。&lt;/strong&gt;我们使用在线配置服务来存储和应用推荐的配置。该管道是完全自动化的，并且用于生成和应用建议的服务是解耦的。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;服务整合&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2eENd1mhwyGpMWNccEwqlQ.png" /&gt;&lt;/figure&gt;&lt;p&gt;图 2 说明了数据平台中生成和应用建议的服务的集成。主要服务内容如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;Nightingale&lt;/strong&gt;是一项运行使用&lt;a href="https://metaflow.org/"&gt;Metaflow&lt;/a&gt;训练的 ML 模型的服务，负责生成重试建议。建议包括 (1) 错误是否可重新启动； (2) 如果是，则重新启动作业的推荐配置。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ConfigService&lt;/strong&gt;是一个在线配置服务。推荐配置作为 JSON 补丁保存在&lt;strong&gt;ConfigService&lt;/strong&gt;中，其范围定义为指定可以使用推荐配置的作业。当&lt;strong&gt;Scheduler&lt;/strong&gt;调用&lt;strong&gt;ConfigService&lt;/strong&gt;获取推荐配置时， &lt;strong&gt;Scheduler&lt;/strong&gt;将原始配置传递给&lt;strong&gt;ConfigService&lt;/strong&gt; ， &lt;strong&gt;ConfigService&lt;/strong&gt;通过将 JSON 补丁应用于原始配置来返回变异的配置。然后，&lt;strong&gt;调度程序&lt;/strong&gt;可以使用变异的配置（包括推荐的配置）重新启动作业。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Pitive&lt;/strong&gt;是一种利用基于规则的分类器的错误分类服务。它调用&lt;strong&gt;Nightingale&lt;/strong&gt;来获取建议，并将建议存储到&lt;strong&gt;ConfigService&lt;/strong&gt; ，以便&lt;strong&gt;Scheduler&lt;/strong&gt;可以选择它来重新启动作业。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Scheduler&lt;/strong&gt;是服务调度作业（我们当前的实现是使用&lt;a href="https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c"&gt;Netflix Maestro&lt;/a&gt; ）。每次作业失败时，都会调用&lt;strong&gt;Pective&lt;/strong&gt;获取错误分类来决定是否重启作业，并调用&lt;strong&gt;ConfigServices&lt;/strong&gt;获取重启作业的推荐配置。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gyXv3JyvhUODQWecQqy1zg.png" /&gt;&lt;/figure&gt;&lt;p&gt;图 3 说明了使用自动修复的服务调用顺序：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;当作业失败时， &lt;strong&gt;Scheduler&lt;/strong&gt;会调用&lt;strong&gt;Pively&lt;/strong&gt;来获取错误分类。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Plenty&lt;/strong&gt;根据基于规则的分类器对错误进行分类。如果错误被识别为内存配置错误或未分类错误，它会调用&lt;strong&gt;Nightingale&lt;/strong&gt;来获取建议。&lt;/li&gt;&lt;li&gt;根据获得的推荐， &lt;strong&gt;Pitive&lt;/strong&gt;更新错误分类结果，并将推荐配置保存到&lt;strong&gt;ConfigService&lt;/strong&gt;中；然后将错误分类结果返回给&lt;strong&gt;Scheduler&lt;/strong&gt; 。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Scheduler根据从Pective&lt;/strong&gt;收到的错误分类结果来决定是否重新启动作业。&lt;/li&gt;&lt;li&gt;在重新启动作业之前，&lt;strong&gt;调度程序&lt;/strong&gt;会调用&lt;strong&gt;ConfigService&lt;/strong&gt;来获取推荐的配置，并使用新配置重试作业。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;演变为自动修复：ML 服务&lt;/h3&gt;&lt;h4&gt;概述&lt;/h4&gt;&lt;p&gt;ML 服务，即 Nightingale，旨在为失败的作业生成重试策略，在重试成功概率和作业运行成本之间进行权衡。它由两个主要部分组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;一种预测模型&lt;/strong&gt;，根据重试的属性，联合估计 a) 重试成功的概率和 b) 以美元为单位的重试成本。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;一个优化器&lt;/strong&gt;，它探索 Spark 配置参数空间来推荐一个配置，该配置可以最小化重试失败概率和成本的线性组合。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;预测模型每天都会离线重新训练，并由优化器调用来评估每个候选配置参数值集。优化器在 RESTful 服务中运行，该服务在作业失败时调用。如果优化有可行的配置解决方案，响应将包含此建议，ConfigService 使用该建议来更改配置以进行重试。如果没有可行的解决方案（换句话说，仅通过更改 Spark 配置参数重试不太可能成功），响应中会包含一个禁用重试的标志，从而消除浪费的计算成本。&lt;/p&gt;&lt;h4&gt;预测模型&lt;/h4&gt;&lt;p&gt;鉴于我们想要探索重试成功和重试成本在不同配置场景下可能如何变化，我们需要某种方法使用我们拥有的有关作业的信息来预测这两个值。数据平台记录重试成功结果和执行成本，为我们提供可靠的标签。由于我们使用共享特征集来预测两个目标，具有良好的标签，并且需要快速在线运行推理以满足 SLO，因此我们决定将问题表述为多输出监督学习任务。特别是，我们使用一个简单的前馈多层感知器（MLP），它有两个头，一个用于预测每个结果。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练：&lt;/strong&gt;训练集中的每条记录都代表一次可能的重试，该重试之前由于内存配置错误或未分类错误而失败。标签是：a）重试失败，b）重试成本。原始功能输入主要是有关作业的非结构化元数据，例如 Spark 执行计划、运行该作业的用户以及 Spark 配置参数和其他作业属性。我们将这些特征分为可以解析为数值的特征（例如，Spark 执行器内存参数）和不能解析为数值的特征（例如，用户名）。我们使用特征哈希来处理非数字值，因为它们来自高基数和动态值集。然后，我们创建一个较低维度的嵌入，该嵌入与归一化的数值连接并通过更多层。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推论：&lt;/strong&gt;通过验证审核后，每个新模型版本都存储在&lt;a href="https://metaflow.org/"&gt;Metaflow&lt;/a&gt; Hosting 中，这是我们内部 ML 平台提供的服务。优化器针对每个传入的配置推荐请求对模型预测函数进行多次调用，如下文更详细地描述。&lt;/p&gt;&lt;h4&gt;优化器&lt;/h4&gt;&lt;p&gt;当作业尝试失败时，它会向 Nightingale 发送带有作业标识符的请求。服务根据该标识符构建要在推理调用中使用的特征向量。如前所述，其中一些功能是 Spark 配置参数，它们是要变异的候选者（例如，spark.executor.memory、spark.executor.cores）。 Spark 配置参数集基于广泛从事 Spark 性能调优的领域专家的提炼知识。我们使用贝叶斯优化（通过 Meta 的&lt;a href="https://ax.dev/"&gt;Axe 库&lt;/a&gt;实现）来探索配置空间并生成推荐。在每次迭代中，优化器都会生成候选参数值组合（例如，spark.executor.memory=7192 mb、spark.executor.cores=8），然后通过调用预测模型来评估该候选参数，以估计重试失败概率和成本：候选配置（即，改变特征向量中的值）。在用尽固定次数的迭代后，优化器会返回“最佳”配置解决方案（即，最小化组合重试失败和成本目标的解决方案）供 ConfigService 使用（如果可行）。如果找不到可行的解决方案，我们将禁用重试。&lt;/p&gt;&lt;p&gt;优化器迭代设计的一个缺点是任何瓶颈都可能阻止完成并导致超时，我们最初在大量案例中观察到这种情况。经过进一步分析，我们发现大部分延迟来自候选生成步骤（即，在上一次迭代的评估结果之后确定在配置空间中步进的方向）。我们发现这个问题已向 Ax 库所有者提出，他们&lt;a href="https://github.com/facebook/Ax/issues/810"&gt;在 API 中添加了 GPU 加速选项&lt;/a&gt;。利用这个选项大大降低了我们的超时率。&lt;/p&gt;&lt;h3&gt;投入生产&lt;/h3&gt;&lt;p&gt;我们在生产中部署了自动修复来处理 Spark 作业的内存配置错误和未分类错误。除了重试成功概率和成本效率外，对用户体验的影响也是主要关注点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;对于内存配置错误：&lt;/strong&gt;自动修复可改善用户体验，因为如果没有针对内存配置错误的新配置，作业重试很少会成功。这意味着使用推荐配置重试成功可以减少运行负载并节省作业运行成本，而重试失败并不会让用户体验变得更差。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对于未分类的错误：&lt;/strong&gt;如果错误无法通过基于规则的分类器中的现有规则进行分类，自动修复会建议是否重新启动作业。特别是，如果 ML 模型预测重试很可能会失败，则会建议禁用重试，这可以节省不必要的重试的作业运行成本。对于业务关键型作业，并且即使重试成功概率很低，用户也喜欢总是重试作业的情况，我们可以向基于规则的分类器添加新规则，以便通过该规则对相同的错误进行分类下次基于 - 的分类器，跳过 ML 服务的建议。这体现了基于规则的分类器和机器学习服务的集成智能的优势。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;生产中的部署表明自动修复&lt;em&gt; &lt;/em&gt;可以针对内存配置错误提供有效的配置，无需人工干预即可成功修复约 56% 的内存配置。它还可以将这些作业的计算成本降低约 50%，因为它可以推荐新配置以使重试成功或禁用不必要的重试。由于性能和成本效率之间的权衡是可调的，因此我们可以决定通过调整 ML 服务来实现更高的成功率或更多的成本节省。&lt;/p&gt;&lt;p&gt;值得注意的是，ML 服务目前采用保守的策略来禁用重试。如上所述，这是为了避免对用户喜欢在作业失败时总是重试作业的情况产生影响。尽管这些情况是预期的，并且可以通过向基于规则的分类器添加新规则来解决，但我们认为以增量方式调整目标函数以逐渐禁用更多重试有助于提供理想的用户体验。鉴于当前禁用重试的政策较为保守，自动修复具有巨大的潜力，最终可以在不影响用户体验的情况下节省更多成本。&lt;/p&gt;&lt;h3&gt;超越错误处理：走向正确的规模调整&lt;/h3&gt;&lt;p&gt;自动修复是我们利用数据洞察和机器学习 (ML) 来改善用户体验、减轻运营负担并提高数据平台成本效率的第一步。它专注于自动修复失败的作业，但也为自动化除错误处理之外的操作铺平了道路。&lt;/p&gt;&lt;p&gt;我们正在采取的举措之一，称为&lt;em&gt;Right Sizing&lt;/em&gt; ，是重新配置计划的大数据作业，以请求适当的资源来执行作业。例如，我们注意到 Spark 作业的平均请求执行程序内存约为其最大已用内存的四倍，表明存在严重的过度配置。除了作业本身的配置之外，还可以减少请求执行作业的容器的资源过剩，以节省成本。通过基于启发式和机器学习的方法，我们可以推断作业执行的正确配置，以最大限度地减少资源过度配置，并在不影响性能的情况下每年节省数百万美元。与自动修复类似，这些配置可以通过 ConfigService 自动应用，无需人工干预。 Right Sizing 正在进行中，稍后将在专门的技术博客文章中介绍更多详细信息。敬请关注。&lt;/p&gt;&lt;h3&gt;致谢&lt;/h3&gt;&lt;p&gt;自动修复是来自不同团队和组织的工程师的共同工作。如果没有扎实、深入的合作，这项工作是不可能完成的。我们衷心感谢所有人，包括 Spark 专家、数据科学家、机器学习工程师、调度程序和作业编排工程师、数据工程师和支持工程师，感谢他们分享背景信息并提供建设性建议和宝贵的反馈（例如&lt;a href="https://www.linkedin.com/in/jzhuge/"&gt;John Zhuge&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/jheua/"&gt;Jun他&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/holdenkarau/"&gt;霍尔顿·卡劳&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/samarthjain11/"&gt;萨玛斯·杰恩&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/julianjaffe/"&gt;朱利安·贾菲&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/batul-shajapurwala-3274b863/"&gt;巴图尔·沙贾普尔瓦拉&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/michael-sachs-b2453b/overlay/about-this-profile/"&gt;迈克尔·萨克斯&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/fzsiddiqi/overlay/about-this-profile/"&gt;费萨尔·西迪奇&lt;/a&gt;）。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=039d5efd115b" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b"&gt;从基于规则的分类器演变而来：Netflix 数据中的机器学习支持的自动修复……&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Mon, 04 Mar 2024 18:01:55 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b?source=rss----2615bd06b42e---4</guid></item><item><title>【Announcing bpftop: Streamlining eBPF performance optimization】宣布 bpftop：简化 eBPF 性能优化</title><link>https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;em&gt;何塞&lt;/em&gt;&lt;a href="https://www.linkedin.com/in/josefernandezmn/"&gt;&lt;em&gt;·费尔南德斯&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;今天，我们很高兴地宣布发布&lt;a href="https://github.com/Netflix/bpftop"&gt;bpftop&lt;/a&gt; ，这是一个命令行工具，旨在简化 eBPF 程序的性能优化和监控。随着 Netflix 越来越多地采用 eBPF [ &lt;a href="https://netflixtechblog.com/extending-vector-with-ebpf-to-inspect-host-and-container-performance-5da3af4c584b"&gt;1&lt;/a&gt; , &lt;a href="https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96"&gt;2&lt;/a&gt; ]，我们必须像对待其他托管服务一样严格地对待这些应用程序。在 eBPF 的优势和系统负载之间取得平衡至关重要，确保它提高而不是阻碍我们的运营效率。该工具使 Netflix 能够发挥 eBPF 的潜力。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/135/1*pum8HfOu5XkB9XEwsKl_hw.png" /&gt;&lt;/figure&gt;&lt;h3&gt;介绍bpftop&lt;/h3&gt;&lt;p&gt; bpftop 提供运行 eBPF 程序的动态实时视图。它显示每个程序的平均执行运行时间、每秒事件数以及估计的总 CPU 百分比。该工具仅在其处于活动状态时才启用性能统计信息，从而最大限度地减少开销。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hLvLcNDb6RljixhN8la-lg.gif" /&gt;&lt;/figure&gt;&lt;p&gt; bpftop 通过实现基准测试、代码优化和即时反馈的有效循环，简化了 eBPF 程序的性能优化过程。如果没有 bpftop，优化工作将需要手动计算，从而给流程增加不必要的复杂性。借助 bpftop，用户可以快速建立基线、实施改进并验证增强功能，从而简化流程。&lt;/p&gt;&lt;p&gt;该工具的一个突出特点是它能够以时间序列图表显示统计数据。这种方法可以发现否则可能会错过的模式和趋势。&lt;/p&gt;&lt;h3&gt;怎么运行的&lt;/h3&gt;&lt;p&gt;bpftop 使用&lt;a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/uapi/linux/bpf.h#L792"&gt;BPF_ENABLE_STATS&lt;/a&gt;系统调用命令启用全局 eBPF 运行时统计信息收集，默认情况下禁用该功能以减少性能开销。它每秒收集这些统计数据，计算该样本周期内每个 eBPF 程序的平均运行时间、每秒事件数以及估计的 CPU 利用率。该信息以类似顶部的表格格式或 10 秒移动窗口的时间序列图显示。一旦 bpftop 终止，它就会关闭统计信息收集功能。该工具是用 Rust 编写的，利用了&lt;a href="https://github.com/libbpf/libbpf-rs"&gt;libbpf-rs&lt;/a&gt;和&lt;a href="https://github.com/ratatui-org/ratatui"&gt;ratatui&lt;/a&gt; crates。&lt;/p&gt;&lt;h3&gt;入门&lt;/h3&gt;&lt;p&gt;访问该项目的&lt;a href="https://github.com/Netflix/bpftop"&gt;GitHub 页面&lt;/a&gt;以了解有关使用该工具的更多信息。我们已在 Apache 2 许可证下开源了 bpftop，并期待社区的贡献。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6a727c1ae2e5" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5"&gt;宣布 bpftop：简化 eBPF 性能优化&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Mon, 26 Feb 2024 16:43:30 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?source=rss----2615bd06b42e---4</guid></item><item><title>【Sequential A/B Testing Keeps the World Streaming Netflix
Part 1: Continuous Data】顺序 A/B 测试让世界保持流媒体 Netflix 第 1 部分：连续数据</title><link>https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/michaelslindon/"&gt;迈克尔·林登&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/csanden/"&gt;克里斯·桑登&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/vshirikian/"&gt;Vache Shirikian&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/liuyanjun/"&gt;刘彦军&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/minalmishra/"&gt;米纳尔·米什拉&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/martintingley/"&gt;马丁·廷利&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="使用连续的、随时有效的假设检验程序来安全地发布软件" src="https://cdn-images-1.medium.com/max/1024/0*mK01JWbQB9QlCEsL" /&gt;&lt;/figure&gt;&lt;p&gt; &lt;strong&gt;1. 找出差异&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;您能发现下面两个数据流之间有什么区别吗？每个观察值都是 Netflix 会员点击播放按钮和播放开始之间的时间间隔，即&lt;em&gt;play-delay&lt;/em&gt; 。这些观察结果来自 Netflix 运行的一种特定类型的 A/B 测试，称为软件金丝雀或回归驱动实验。下面将详细介绍 - 目前，重要的是我们希望&lt;strong&gt;快速&lt;/strong&gt;、&lt;strong&gt;自信地&lt;/strong&gt;识别播放延迟分布中的任何差异 - 或者得出结论，在一定的容差范围内，没有差异。&lt;/p&gt;&lt;p&gt;在这篇博文中，我们将开发一个统计程序来做到这一点，并描述这些发展对 Netflix 的影响。关键思想是将问题的“固定时间范围”转变为“任何时间有效”的框架。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="依次比较治疗和对照的两个测量值流" src="https://cdn-images-1.medium.com/max/1024/1*yDCF303-R9uqqH_zo7F4ug.gif" /&gt;&lt;figcaption&gt;图 1. A/B 测试的示例数据流，其中每个观察值代表对照（左）和处理（右）的播放延迟。您能发现两个数据流之间的统计分布有什么差异吗？&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;strong&gt;2. 安全的软件部署、金丝雀测试和播放延迟&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;本博客的软件工程读者可能熟悉单元测试、集成测试和负载测试，以及旨在防止错误到达生产系统的其他测试实践。 Netflix 还执行金丝雀测试——当前软件版本和较新软件版本之间的软件 A/B 测试。要了解更多信息，请参阅我们之前 &lt;a href="https://netflixtechblog.com/safe-updates-of-client-applications-at-netflix-1d01c71a930c"&gt;有关客户端应用程序安全更新的&lt;/a&gt;博客文章。&lt;/p&gt;&lt;p&gt;金丝雀测试的目的有两个：充当质量控制门，在完全发布之前捕获错误，并衡量新软件的实际性能。这是通过对一小部分用户进行随机对照实验来实现的，其中治疗组接收新的软件更新，而对照组继续运行现有软件。如果在治疗组中观察到任何错误或性能下降，则可以阻止全面发布，从而限制用户群中的“影响半径”。&lt;/p&gt;&lt;p&gt; Netflix 在金丝雀测试中监控的指标之一是当用户请求某个标题时视频流启动所需的时间。在整个版本中监控此“播放延迟”指标可确保 Netflix 的流媒体性能只会随着我们发布新版本的 Netflix 客户端而不断提高。在图 1 中，左侧显示了运行现有版本 Netflix 客户端的用户的实时播放延迟测量数据流，右侧显示了运行更新版本的用户的播放延迟测量数据。我们问自己：更新客户端的用户是否会遇到更长的游戏延迟？&lt;/p&gt;&lt;p&gt;我们认为播放延迟的任何增加都是严重的性能下降，如果我们检测到增加，将阻止发布。至关重要的是，测试平均值或中位数的差异是不够的，也不能提供完整的情况。例如，我们可能面临的一种情况是，治疗组和对照组的中位或平均游戏延迟相同，但治疗组的游戏延迟上分位数有所增加。这对应于那些已经经历过较高播放延迟的用户（可能是我们的互联网连接速度慢或不稳定的会员）的 Netflix 体验下降。我们的测试程序不应忽略此类更改。&lt;/p&gt;&lt;p&gt;为了获得完整的图像，我们需要能够可靠且快速地检测&lt;em&gt;播放延迟分布的任何部分的&lt;/em&gt;向上移动。也就是说，我们必须对治疗和控制中的游戏延迟分布之间的差异进行推断和测试。&lt;/p&gt;&lt;p&gt;总而言之，以下是我们的金丝雀测试系统的设计要求：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;尽快识别错误和性能退化（通过播放延迟来衡量）。&lt;strong&gt;&lt;em&gt;理由&lt;/em&gt;&lt;/strong&gt;：为了最大限度地减少对成员的伤害，如果治疗组中的用户遇到的流媒体质量有任何问题，我们需要中止金丝雀并尽快回滚软件更改。&lt;/li&gt;&lt;li&gt;严格控制误报（误报）概率。&lt;strong&gt;&lt;em&gt;理由&lt;/em&gt;&lt;/strong&gt;：该系统是所有客户端部署的半自动化流程的一部分。误报测试不必要地中断软件发布过程，降低软件交付速度，并使开发人员寻找不存在的错误。&lt;/li&gt;&lt;li&gt;该系统应该能够检测到分布中的任何变化。&lt;strong&gt;&lt;em&gt;理由&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;：&lt;/em&gt;我们不仅关心平均值或中位数的变化，还关心尾部行为和其他分位数的变化。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们现在构建了满足这些设计要求的顺序测试程序。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;3. 顺序测试：基础知识&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;标准统计测试是固定 n 或固定时间范围：分析人员等待收集到一些预设数量的数据，然后执行一次分析。经典的 t 检验、Kolmogorov-Smirnov 检验和 Mann-Whitney 检验都是固定 n 检验的示例。固定 n 测试的限制是它们只能执行一次 - 但在上述情况下，我们希望频繁测试以尽快检测差异。如果您多次应用固定 n 测试，那么您将丧失 I 类错误或误报保证。&lt;/p&gt;&lt;p&gt;以下是固定 n 测试在重复分析下如何失败的快速说明。在下图中，每条红线描绘了当将 Mann-Whitney 检验重复应用于数据集时的 p 值，因为在处理和对照中都会产生 10,000 个观测值。每条红线都显示一个独立的模拟，在每种情况下，治疗和对照之间没有区别：这些是模拟的 A/A 测试。&lt;/p&gt;&lt;p&gt;黑点标记 p 值低于标准 0.05 拒绝阈值的位置。令人震惊的是， &lt;strong&gt;70% 的模拟&lt;/strong&gt;在某个时间点宣称存在显着差异，尽管从构造上看，没有差异：实际误报率远高于标称的 0.05。在柯尔莫哥洛夫-斯米尔诺夫检验中会观察到完全相同的行为。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="偷看曼惠特尼检验时误报增加" src="https://cdn-images-1.medium.com/max/1024/1*fhHzjEOV5Iak564vOSLq7g.png" /&gt;&lt;figcaption&gt;图 2. 在原假设下模拟的 p 值过程的 100 个样本路径（以红色显示）。黑色虚线表示标称 alpha=0.05 水平。黑点表示 p 值过程低于 alpha=0.05 阈值的位置，表示对原假设的错误拒绝。 100 次 A/A 模拟中总共有 66 次错误地拒绝了原假设。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这是“偷看”的一种表现，并且已经有很多关于这种做法的负面风险的文章（例如，参见&lt;a href="https://dl.acm.org/doi/abs/10.1145/3097983.3097992"&gt;Johari&lt;em&gt;等人，&lt;/em&gt; 2017 年&lt;/a&gt;）。如果我们限制自己正确应用固定 n 统计测试，即只分析一次数据，我们将面临一个困难的权衡：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在收集少量数据后尽早进行测试。在这种情况下，我们只能检测更大的回归。较小的性能回归将不会被检测到，并且随着小回归的累积，我们面临着逐渐侵蚀会员体验的风险。&lt;/li&gt;&lt;li&gt;收集大量数据后，稍后再进行测试。在这种情况下，我们有能力检测小回归，但在大回归的情况下，我们会让会员在不必要的长时间内遭受不良体验。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;顺序或“任何时间有效”的统计测试克服了这些限制。它们允许窥视——事实上，它们可以在每个新数据点到达后应用——同时提供误报或 I 类错误，保证在整个时间内保持不变。因此，我们可以使用&lt;em&gt;置信序列&lt;/em&gt;或&lt;em&gt;连续 p 值&lt;/em&gt;连续监控数据流，如上图所示，并快速检测大回归，同时最终检测小回归。&lt;/p&gt;&lt;p&gt;尽管在数字实验的背景下采用的时间相对较晚，但这些方法具有悠久的学术历史，其最初的想法可以追溯到亚伯拉罕·沃尔德（Abraham Wald）的&lt;a href="https://www.jstor.org/stable/2235829"&gt;&lt;em&gt;统计假设的顺序检验&lt;/em&gt;&lt;/a&gt;&lt;em&gt; &lt;/em&gt;从 1945 年开始。这一领域的研究仍然很活跃，Netflix 在过去几年中做出了许多贡献（请参阅这些论文中的参考文献以获取更完整的文献综述）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539099"&gt;软件部署中的快速回归检测&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://openreview.net/forum?id=a4zg0jiuVi"&gt;多项计数数据的随时有效推理&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2210.08589"&gt;随机实验中随时有效的线性模型和回归调整因果推断&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2210.08639"&gt;基于设计的置信序列：在线实验中风险缓解的通用方法&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/2212.14411"&gt;具有可能相关观测值的近最优非参数序贯检验和置信序列&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本博客和后续博客中，我们将描述我们开发的方法及其在 Netflix 的应用。本文的其余部分讨论了上述第一篇论文，该论文发表于 KDD &amp;#39;22（可在&lt;a href="https://arxiv.org/abs/2205.14762"&gt;ArXiV&lt;/a&gt;上获取）。我们将保持高水平——对技术细节感兴趣的读者可以查阅该论文。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;4. 顺序测试解决方案&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;分布差异&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在任何时间点，我们都可以根据迄今为止观察到的数据来估计治疗和控制的经验分位数函数。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="处理和控制数据的经验分位数函数" src="https://cdn-images-1.medium.com/max/1024/0*tKe66EiIrN9R8SST" /&gt;&lt;figcaption&gt;图 3：开始金丝雀实验后的快照中对照（左）和治疗（右）的经验分位数函数。这是来自实际的 Netflix 数据，因此我们抑制了 y 轴上的数值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这两个图看起来非常接近，但我们可以做得比肉眼比较更好——而且我们希望计算机能够持续评估分布之间是否存在显着差异。根据设计要求，我们还希望尽早检测到大的影响，同时保留最终检测小影响的能力——并且我们希望将误报概率保持在标称水平，同时允许连续分析（又称窥视）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;也就是说，我们需要对分布差异进行顺序检验&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;使用&lt;a href="https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality"&gt;DKWM 不等式&lt;/a&gt;可以获得分位数函数的“固定范围”置信带。然而，为了获得时间均匀的置信带，我们使用&lt;a href="https://projecteuclid.org/journals/bernoulli/volume-28/issue-3/Sequential-estimation-of-quantiles-with-applications-to-A-B-testing/10.3150/21-BEJ1388.short"&gt;Howard 和 Ramdas (2022)&lt;/a&gt; [ &lt;a href="https://arxiv.org/abs/1906.09712"&gt;arxiv 版本&lt;/a&gt;] 的任何时间有效的置信序列。由于这些置信区间的覆盖保证在不同时间范围内保持一致，因此我们可以看到它们变得更紧，而不必担心被&lt;a href="https://www.kdd.org/kdd2017/papers/view/peeking-at-ab-tests-why-it-matters-and-what-to-do-about-it"&gt;窥视&lt;/a&gt;。随着越来越多的数据点涌入，这些连续置信带的宽度继续缩小，这意味着分布函数中的任何差异（如果存在）最终都会变得明显。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="治疗和控制分位数函数的任何时间有效的置信带" src="https://cdn-images-1.medium.com/max/1024/1*kUcLygkzrpSiHcQI9iA-qw.gif" /&gt;&lt;figcaption&gt;图 4：对照（左）和治疗（右）分位数函数的 97.5% 时间均匀置信带&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;请注意，每一帧对应于实验开始后的一个时间点，而不是样本大小。事实上，并不要求每个治疗组具有相同的样本量。&lt;/p&gt;&lt;p&gt;通过可视化处理和对照分位数函数之间的差异，可以更容易地看到差异。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="分位数差异和序列 p 值的置信序列" src="https://cdn-images-1.medium.com/max/1024/1*FBi_sDHmfhXFp3p1ZOcodw.gif" /&gt;&lt;figcaption&gt;图 5：分位数差值函数 Q_b(p) — Q_a(p) 上的 95% 时间均匀置信带（左）。顺序 p 值（右）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;由于治疗效果分位数函数的顺序置信带在任何时候都有效，因此推理过程变得相当直观。我们可以继续观察这些置信带收紧，如果在任何时候置信带在任何分位数不再覆盖零，我们就可以得出分布不同并停止测试的结论。除了顺序置信带之外，我们还可以构建顺序 p 值来测试分布的差异。从动画中请注意，分位数处理效果的 95% 置信带排除零的时刻与连续 p 值降至 0.05 以下的时刻相同：与固定 n 检验一样，置信区间和 p 值之间存在一致性。&lt;/p&gt;&lt;p&gt;此应用程序中有许多多重测试问题。我们的解决方案同时控制所有分位数、所有治疗组和所有联合样本量的 I 型误差（请参阅&lt;a href="https://arxiv.org/pdf/2205.14762.pdf"&gt;我们的论文&lt;/a&gt;，或&lt;a href="https://projecteuclid.org/journals/bernoulli/volume-28/issue-3/Sequential-estimation-of-quantiles-with-applications-to-A-B-testing/10.3150/21-BEJ1388.short"&gt;Howard 和 Ramdas&lt;/a&gt;了解详细信息）。结果适用于所有分位数和所有时间。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;5. 对 Netflix 的影响&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;发布新软件总是会带来风险，我们始终希望降低服务中断或会员体验下降的风险。我们的金丝雀测试方法是防止错误和性能下降进入生产的另一层保护。它是完全自动化的，并已成为 Netflix 软件交付流程中不可或缺的一部分。开发人员可以安心地投入生产，因为他们知道错误和性能下降将被迅速发现。额外的信心使开发人员能够更频繁地投入生产，缩短 Netflix 客户端升级的上市时间，并提高我们的软件交付率。&lt;/p&gt;&lt;p&gt;到目前为止，该系统已成功阻止了许多严重错误影响我们的最终用户。我们详细介绍一个例子。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;案例研究：Netflix 客户端应用程序的安全部署&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;图 3-5 取自金丝雀测试，其中客户端应用程序的行为被修改（播放延迟的实际数值已被抑制）。正如我们所看到的，金丝雀测试显示，新版本的客户端增加了多个播放延迟分位数，其中中位数和 75% 百分位数的播放延迟分别相对增加了至少 0.5% 和 1%。连续 p 值的时间序列表明，在这种情况下，我们能够在大约 60 秒后拒绝 0.05 水平上分布无变化的零值。这在软件交付过程中提供了快速反馈，使开发人员能够测试新软件的性能并快速迭代。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;6.接下来怎么办？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果您对此处开发的分位数顺序测试的技术细节感到好奇，您可以在我们的&lt;a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539099"&gt;KDD 论文&lt;/a&gt;（&lt;a href="https://arxiv.org/pdf/2205.14762.pdf"&gt;也可在 arxiv 上找到&lt;/a&gt;）中了解所有数学知识。&lt;/p&gt;&lt;p&gt;您可能还想知道如果数据不是连续测量会发生什么。错误和异常是部署软件时要记录的关键指标，就像许多其他最好根据计数定义的指标一样。请继续关注——我们的下一篇文章将开发计数数据的顺序测试程序。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;更新：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642"&gt;第 2 部分：计数数据&lt;/a&gt;&lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=cba6c7ed49df" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df"&gt;顺序 A/B 测试让世界保持流媒体播放 Netflix 第 1 部分：连续数据&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Tue, 13 Feb 2024 19:10:28 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df?source=rss----2615bd06b42e---4</guid></item><item><title>【Introducing SafeTest: A Novel Approach to Front End Testing】SafeTest 简介：一种前端测试的新方法</title><link>https://netflixtechblog.com/introducing-safetest-a-novel-approach-to-front-end-testing-37f9f88c152d?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;作者：&lt;a href="https://medium.com/u/a155da075195"&gt;摩西·科洛德尼&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在这篇文章中，我们很高兴地介绍 SafeTest，这是一个革命性的库，它为基于 Web 的用户界面 (UI) 应用程序的端到端 (E2E) 测试提供了全新的视角。&lt;/p&gt;&lt;h3&gt;传统 UI 测试的挑战&lt;/h3&gt;&lt;p&gt;传统上，UI 测试是通过单元测试或集成测试（也称为端到端 (E2E) 测试）进行的。然而，这些方法中的每一种都提出了独特的权衡：您必须在控制测试夹具和设置或控制测试驱动器之间进行选择。&lt;/p&gt;&lt;p&gt;例如，当使用&lt;a href="https://testing-library.com/docs/react-testing-library/intro/"&gt;react-testing-library&lt;/a&gt; （一个单元测试解决方案）时，您可以完全控制要呈现的内容以及底层服务和导入的行为方式。但是，您失去了与实际页面交互的能力，这可能会导致无数的痛点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;难以与复杂的 UI 元素（例如 &amp;lt;Dropdown /&amp;gt; 组件）进行交互。&lt;/li&gt;&lt;li&gt;无法测试 CORS 设置或 GraphQL 调用。&lt;/li&gt;&lt;li&gt;缺乏对影响按钮可点击性的 z-index 问题的可见性。&lt;/li&gt;&lt;li&gt;复杂且不直观的测试编写和调试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;相反，使用 Cypress 或 Playwright 等集成测试工具可以提供对页面的控制，但会牺牲为应用程序检测引导代码的能力。这些工具通过远程控制浏览器访问 URL 并与页面交互来进行操作。这种方法有其自身的一系列挑战：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果不实施自定义网络层 API 重写规则，则很难调用替代 API 端点。&lt;/li&gt;&lt;li&gt;无法对间谍/模拟进行断言或在应用程序内执行代码。&lt;/li&gt;&lt;li&gt;测试诸如黑暗模式之类的内容需要单击主题切换器或了解要覆盖的本地存储机制。&lt;/li&gt;&lt;li&gt;无法测试应用程序的各个部分，例如，如果某个组件仅在单击按钮并等待 60 秒计时器倒计时后才可见，则测试将需要运行这些操作，并且至少需要一分钟长。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;认识到这些挑战后，&lt;a href="https://docs.cypress.io/guides/component-testing/overview"&gt;赛普拉斯&lt;/a&gt;和&lt;a href="https://playwright.dev/docs/test-components"&gt;Playwright&lt;/a&gt;提供的端到端组件测试等解决方案应运而生。虽然这些工具试图纠正传统集成测试方法的缺点，但由于其架构，它们还存在其他限制。他们使用引导代码启动开发服务器来加载所需的组件和/或设置代码，这限制了他们处理可能具有 OAuth 或复杂构建管道的复杂企业应用程序的能力。此外，更新 TypeScript 的使用可能会破坏您的测试，直到 Cypress/Playwright 团队更新他们的运行程序。&lt;/p&gt;&lt;h3&gt;欢迎来到安全测试&lt;/h3&gt;&lt;p&gt;SafeTest 旨在通过一种新颖的 UI 测试方法来解决这些问题。主要思想是&lt;a href="https://www.npmjs.com/package/safetest#bootstrapping-your-application"&gt;在我们的应用程序引导阶段添加一段代码，注入钩子来运行我们的测试&lt;/a&gt;（有关此操作的更多信息，请参阅&lt;a href="https://www.npmjs.com/package/safetest#how-safetest-works"&gt;安全测试的工作原理&lt;/a&gt;部分）。&lt;strong&gt;请注意，这种工作方式对应用程序的常规使用没有可衡量的影响，因为 SafeTest 仅在运行测试时利用延迟加载来动态加载测试（在 README 示例中，测试根本不在生产包中）。&lt;/strong&gt;一旦到位，我们就可以使用 Playwright 运行常规测试，从而实现我们测试所需的理想浏览器控制。&lt;/p&gt;&lt;p&gt;这种方法还解锁了一些令人兴奋的功能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;深层链接到特定测试，无需运行节点测试服务器。&lt;/li&gt;&lt;li&gt;浏览器和测试（节点）上下文之间的双向通信。&lt;/li&gt;&lt;li&gt;访问 Playwright 附带的所有 DX 功能（不包括 @playwright/test 附带的功能）。&lt;/li&gt;&lt;li&gt;测试视频录制、跟踪查看和暂停页面功能以尝试不同的页面选择器/操作。&lt;/li&gt;&lt;li&gt;能够对节点中浏览器中的间谍进行断言，匹配浏览器中调用的快照。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;使用 SafeTest 测试示例&lt;/h3&gt;&lt;p&gt;SafeTest 的设计让之前进行过 UI 测试的任何人都感到熟悉，因为它利用了现有解决方案的最佳部分。以下是如何测试整个应用程序的示例：&lt;/p&gt;&lt;pre&gt;从“safetest/jest”导入{描述、它、期望}；&lt;br /&gt;从 &amp;#39;safetest/react&amp;#39; 导入 { render } ；&lt;br /&gt;&lt;br /&gt;描述（&amp;#39;我的应用程序&amp;#39;，（）=&amp;gt; {&lt;br /&gt; it(&amp;#39;加载主页&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染();&lt;br /&gt;&lt;br /&gt;等待期望（page.getByText（&amp;#39;欢迎使用应用程序&amp;#39;））.toBeVisible（）;&lt;br /&gt;期望（等待页面.screenshot（））.toMatchImageSnapshot（）；&lt;br /&gt; });&lt;br /&gt; });&lt;/pre&gt;&lt;p&gt;我们可以轻松地测试特定组件&lt;/p&gt;&lt;pre&gt;从“safetest/jest”导入{描述、it、expect、browserMock}；&lt;br /&gt;从 &amp;#39;safetest/react&amp;#39; 导入 { render } ；&lt;br /&gt;&lt;br /&gt;描述(&amp;#39;标头组件&amp;#39;, () =&amp;gt; {&lt;br /&gt; it(&amp;#39;有正常模式&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染(&amp;lt;Header /&amp;gt;);&lt;br /&gt;&lt;br /&gt;等待期望(page.getByText(&amp;#39;Admin&amp;#39;)).not.toBeVisible();&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt; it(&amp;#39;有管理模式&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { page } = wait render(&amp;lt;Header admin={true} /&amp;gt;);&lt;br /&gt;&lt;br /&gt;等待期望(page.getByText(&amp;#39;Admin&amp;#39;)).toBeVisible();&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt; it(&amp;#39;注销时调用注销处理程序&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const 间谍 = browserMock.fn();&lt;br /&gt; const { page } = wait render(&amp;lt;Header handleLogout={spy} /&amp;gt;);&lt;br /&gt;&lt;br /&gt;等待 page.getByText(&amp;#39;注销&amp;#39;).click();&lt;br /&gt;期待（等待间谍）。toHaveBeenCalledWith（）；&lt;br /&gt; });&lt;br /&gt; });&lt;/pre&gt;&lt;h3&gt;利用覆盖&lt;/h3&gt;&lt;p&gt;SafeTest 利用 React Context 允许在测试期间覆盖值。作为其工作原理的示例，假设我们在组件中使用了 fetchPeople 函数：&lt;/p&gt;&lt;pre&gt;从&amp;#39;react-use&amp;#39;导入{useAsync}；&lt;br /&gt;从 &amp;#39;./api/person&amp;#39; 导入 { fetchPerson }；&lt;br /&gt;&lt;br /&gt;导出 const People: React.FC = () =&amp;gt; {&lt;br /&gt; const { 数据：人员、加载、错误 } = useAsync(fetchPeople);&lt;br /&gt;&lt;br /&gt;如果（正在加载）返回&amp;lt;Loader /&amp;gt;;&lt;br /&gt; if (错误) return &amp;lt;ErrorPage error={error} /&amp;gt;;&lt;br /&gt;返回&amp;lt;表数据={数据}行=[...]/&amp;gt;;&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;我们可以修改 People 组件以使用 Override：&lt;/p&gt;&lt;pre&gt;从 &amp;#39;./api/person&amp;#39; 导入 { fetchPerson }；&lt;br /&gt; +从“safetest/react”导入{createOverride}；&lt;br /&gt;&lt;br /&gt; +const FetchPerson = createOverride(fetchPerson);&lt;br /&gt;&lt;br /&gt;导出 const People: React.FC = () =&amp;gt; {&lt;br /&gt; + const fetchPeople = FetchPerson.useValue();&lt;br /&gt; const { 数据：人员、加载、错误 } = useAsync(fetchPeople);&lt;br /&gt;&lt;br /&gt;如果（正在加载）返回&amp;lt;Loader /&amp;gt;;&lt;br /&gt; if (错误) return &amp;lt;ErrorPage error={error} /&amp;gt;;&lt;br /&gt;返回&amp;lt;表数据={数据}行=[...]/&amp;gt;;&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;现在，在我们的测试中，我们可以覆盖此调用的响应：&lt;/p&gt;&lt;pre&gt; constending = new Promise(r =&amp;gt; { /* 不执行任何操作 */ });&lt;br /&gt; const已解决= [{名称：&amp;#39;Foo&amp;#39;，年龄：23​​]，{名称：&amp;#39;酒吧&amp;#39;，年龄：32]}];&lt;br /&gt; const error = new Error(&amp;#39;哎呀&amp;#39;);&lt;br /&gt;&lt;br /&gt;描述（&amp;#39;人&amp;#39;，（）=&amp;gt; {&lt;br /&gt; it(&amp;#39;有加载状态&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（&lt;br /&gt; &amp;lt;FetchPerson.Override with={() =&amp;gt; () =&amp;gt; 待处理}&amp;gt;&lt;br /&gt; &amp;lt;人物/&amp;gt;&lt;br /&gt; &amp;lt;/FetchPerson.Override&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;&lt;br /&gt;等待期望(page.getByText(&amp;#39;正在加载&amp;#39;)).toBeVisible();&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt; it(&amp;#39;已加载状态&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（&lt;br /&gt; &amp;lt;FetchPerson.Override with={() =&amp;gt; async () =&amp;gt; 已解决}&amp;gt;&lt;br /&gt; &amp;lt;人物/&amp;gt;&lt;br /&gt; &amp;lt;/FetchPerson.Override&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;&lt;br /&gt;等待期望（page.getByText（&amp;#39;用户：Foo，名称：23&amp;#39;））.toBeVisible（）;&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt; it(&amp;#39;有错误状态&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（&lt;br /&gt; &amp;lt;FetchPerson.Override with={() =&amp;gt; async () =&amp;gt; { 抛出错误 }}&amp;gt;&lt;br /&gt; &amp;lt;人物/&amp;gt;&lt;br /&gt; &amp;lt;/FetchPerson.Override&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;&lt;br /&gt;等待期望（page.getByText（&amp;#39;获取用户时出错：“哎呀”&amp;#39;））.toBeVisible（）;&lt;br /&gt; });&lt;br /&gt; });&lt;/pre&gt;&lt;p&gt;渲染函数还接受一个将传递给初始应用程序组件的函数，允许在应用程序中的任何位置注入任何所需的元素：&lt;/p&gt;&lt;pre&gt; it(&amp;#39;有一个人已加载状态&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（应用程序 =&amp;gt;&lt;br /&gt; &amp;lt;FetchPerson.Override with={() =&amp;gt; async () =&amp;gt; 已解决}&amp;gt;&lt;br /&gt; {应用程序}&lt;br /&gt; &amp;lt;/FetchPerson.Override&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;等待期望（page.getByText（&amp;#39;用户：Foo，名称：23&amp;#39;））.toBeVisible（）;&lt;br /&gt; });&lt;/pre&gt;&lt;p&gt;通过覆盖，我们可以编写复杂的测试用例，例如确保组合来自 /foo、/bar 和 /baz 的 API 请求的服务方法，仅针对失败的 API 请求具有正确的重试机制，并且仍然正确映射返回值。因此，如果 /bar 尝试解析 3 次，该方法将总共进行 5 次 API 调用。&lt;/p&gt;&lt;p&gt;覆盖不仅限于 API 调用（因为我们也可以使用&lt;a href="https://playwright.dev/docs/api/class-page#page-route"&gt;page.route&lt;/a&gt; ），我们还可以覆盖特定的应用程序级别值，例如功能标志或更改某些静态值：&lt;/p&gt;&lt;pre&gt; +const UseFlags = createOverride(useFlags);&lt;br /&gt;导出常量管理员 = () =&amp;gt; {&lt;br /&gt; + const useFlags = UseFlags.useValue();&lt;br /&gt; const { isAdmin } = useFlags();&lt;br /&gt; if (!isAdmin) return &amp;lt;div&amp;gt;权限错误&amp;lt;/div&amp;gt;;&lt;br /&gt; // ...&lt;br /&gt; }&lt;br /&gt;&lt;br /&gt; +const Language = createOverride(navigator.language);&lt;br /&gt;导出 const LanguageChanger = () =&amp;gt; {&lt;br /&gt; - const 语言 = navigator.language;&lt;br /&gt; + const 语言 = Language.useValue();&lt;br /&gt; return &amp;lt;div&amp;gt;当前语言是 { language } &amp;lt;/div&amp;gt;;&lt;br /&gt; }&lt;br /&gt;&lt;br /&gt;描述（&amp;#39;管理员&amp;#39;，（）=&amp;gt; {&lt;br /&gt; it(&amp;#39;与管理标志一起使用&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（&lt;br /&gt; &amp;lt;UseIsAdmin.Override with={oldHook =&amp;gt; {&lt;br /&gt; const oldFlags = oldHook();&lt;br /&gt;返回 { ...oldFlags, isAdmin: true };&lt;br /&gt; }}&amp;gt;&lt;br /&gt; &amp;lt;我的组件/&amp;gt;&lt;br /&gt; &amp;lt;/UseIsAdmin.Override&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;&lt;br /&gt;等待expect(page.getByText(&amp;#39;权限错误&amp;#39;)).not.toBeVisible();&lt;br /&gt; });&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt;描述(&amp;#39;语言&amp;#39;, () =&amp;gt; {&lt;br /&gt; it(&amp;#39;显示&amp;#39;, async () =&amp;gt; {&lt;br /&gt; const { 页面 } = 等待渲染（&lt;br /&gt; &amp;lt;Language.Override with={old =&amp;gt; &amp;#39;abc&amp;#39;}&amp;gt;&lt;br /&gt; &amp;lt;我的组件/&amp;gt;&lt;br /&gt; &amp;lt;/语言.覆盖&amp;gt;&lt;br /&gt; ）；&lt;br /&gt;&lt;br /&gt; wait Expect(page.getByText(&amp;#39;当前语言是 abc&amp;#39;)).toBeVisible();&lt;br /&gt; });&lt;br /&gt; });&lt;/pre&gt;&lt;p&gt;覆盖是 SafeTest 的一项强大功能，此处的示例仅涉及表面。有关更多信息和示例，请参阅&lt;a href="https://github.com/kolodny/safetest/blob/main/README.md"&gt;README&lt;/a&gt;中的&lt;a href="https://www.npmjs.com/package/safetest#overrides"&gt;Overrides 部分&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;报告&lt;/h3&gt;&lt;p&gt;SafeTest 开箱即用，具有强大的报告功能，例如视频重播的自动链接、Playwright 跟踪查看器，甚至&lt;a href="https://safetest-two.vercel.app/vite-react-ts/?test_path=./Another.safetest&amp;amp;test_name=Main2+can+do+many+interactions+fast"&gt;直接深度链接到已安装的测试组件&lt;/a&gt;。 SafeTest 存储库&lt;a href="https://github.com/kolodny/safetest/blob/main/README.md"&gt;自述文件&lt;/a&gt;链接到所有&lt;a href="https://safetest-two.vercel.app/"&gt;示例应用程序&lt;/a&gt;以及&lt;a href="https://safetest-two.vercel.app/report.html#results=vite-react-ts/artifacts/results.json&amp;amp;url=vite-react-ts/"&gt;报告&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="SafeTest 报告的图像显示了测试运行的视频" src="https://cdn-images-1.medium.com/max/995/1*OFmV3PX7Is8X48-V9ryeig.png" /&gt;&lt;/figure&gt;&lt;h3&gt;企业环境中的安全测试&lt;/h3&gt;&lt;p&gt;许多大公司需要某种形式的身份验证才能使用该应用程序。通常，导航到 localhost:3000 只会导致永久加载页面。您需要转到不同的端口，例如 localhost:8000，它有一个代理服务器来检查和/或将身份验证凭据注入到底层服务调用中。这一限制是 Cypress/Playwright 组件测试不适合在 Netflix 使用的主要原因之一。&lt;/p&gt;&lt;p&gt;但是，通常有一个服务可以生成测试用户，我们可以使用其凭据登录应用程序并与应用程序交互。这有助于围绕 SafeTest 创建一个轻量级包装器，以自动生成并假设该测试用户。例如，我们在 Netflix 基本上是这样做的：&lt;/p&gt;&lt;pre&gt;从“safetest/setup”导入{setup}；&lt;br /&gt;从 &amp;#39;netflix-test-helper&amp;#39; 导入 { createTestUser, addCookies };&lt;br /&gt;&lt;br /&gt;设置类型 = 参数&amp;lt;设置类型&amp;gt;[0] &amp;amp; {&lt;br /&gt; extraUserOptions？：用户选项；&lt;br /&gt; };&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;导出常量设置Netflix =（选项：设置）=&amp;gt; {&lt;br /&gt;设置（{&lt;br /&gt; ...选项，&lt;br /&gt; hooks: { beforeNavigate: [异步页面 =&amp;gt; addCookies(page)] },&lt;br /&gt; });&lt;br /&gt;&lt;br /&gt; beforeAll(异步() =&amp;gt; {&lt;br /&gt;创建测试用户（选项.extraUserOptions）&lt;br /&gt; });&lt;br /&gt; };&lt;/pre&gt;&lt;p&gt;设置完成后，我们只需导入上述包来代替我们使用 safetest/setup 的位置。&lt;/p&gt;&lt;h3&gt;超越反应&lt;/h3&gt;&lt;p&gt;虽然这篇文章重点介绍了 SafeTest 如何与 React 配合使用，但它并不仅限于 React。 SafeTest 还可以与 Vue、Svelte、Angular 配合使用，甚至可以在 NextJS 或 Gatsby 上运行。它还使用 Jest 或 Vitest 运行，具体取决于您的脚手架开始时使用的测试运行程序。&lt;a href="https://github.com/kolodny/safetest/tree/main/examples"&gt;示例文件夹&lt;/a&gt;演示了如何将 SafeTest 与不同的工具组合一起使用，我们鼓励贡献以添加更多案例。&lt;/p&gt;&lt;p&gt; SafeTest 的核心是测试运行器、UI 库和浏览器运行器的智能粘合剂。尽管 Netflix 最常见的用法是使用 Jest/React/Playwright，但为其他选项添加更多适配器也很容易。&lt;/p&gt;&lt;h3&gt;结论&lt;/h3&gt;&lt;p&gt;SafeTest 是 Netflix 内部采用的一个强大的测试框架。它可以轻松编写测试，并提供故障发生时间和方式的全面报告，并提供用于查看回放视频或手动运行测试步骤以查看故障原因的链接。我们很高兴看到它将如何彻底改变 UI 测试，并期待您的反馈和贡献。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=37f9f88c152d" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/introducing-safetest-a-novel-approach-to-front-end-testing-37f9f88c152d"&gt;SafeTest 简介：前端测试的一种新颖方法&lt;/a&gt;最初发表在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Tue, 13 Feb 2024 16:07:48 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/introducing-safetest-a-novel-approach-to-front-end-testing-37f9f88c152d?source=rss----2615bd06b42e---4</guid></item><item><title>【Rebuilding Netflix Video Processing Pipeline with Microservices】使用微服务重建 Netflix 视频处理管道</title><link>https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/liwei-guo-a5aa6311/"&gt;郭立伟&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/anush-moorthy-b8451142/"&gt;阿努什·穆尔西&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/li-heng-chen-a75458a2/"&gt;陈立恒&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/carvalhovinicius/"&gt;Vinicius Carvalho&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/aditya-mavlankar-7139791/"&gt;Aditya Mavlankar&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/agataopalach/"&gt;Agata Opalach&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/adithyaprakash/"&gt;Adithya Prakash&lt;/a&gt; 、Kyle Swanson、 &lt;a href="https://www.linkedin.com/in/jessicatweneboah/"&gt;Jessica Tweneboah&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/subbu-venkatrav-126172a/"&gt;Subbu Venkatrav&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/lishan-z-51302abb/"&gt;朱立山&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;这是关于 Netflix 如何使用微服务重建视频处理管道的多部分系列中的第一篇博客，以便我们能够保持快速的创新步伐，并不断改进会员流媒体和工作室运营系统。这个介绍性博客重点概述我们的旅程。未来的博客将更深入地探讨每项服务，分享从这个过程中吸取的见解和经验教训。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; Netflix 视频处理管道随着 2007 年流媒体服务的推出而上线。从那时起，视频管道经历了实质性改进和广泛扩展：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;从&lt;a href="https://en.wikipedia.org/wiki/Display_resolution"&gt;标准清晰度&lt;/a&gt;的标准动态范围 (SDR) 开始，我们将编码管道扩展到 4K 和高动态范围 (HDR)，从而支持我们的优质产品。&lt;/li&gt;&lt;li&gt;我们从集中式线性编码转向&lt;a href="https://netflixtechblog.com/high-quality-video-encoding-at-scale-d159db052746"&gt;分布式基于块的编码&lt;/a&gt;。这种架构转变极大地减少了处理延迟并提高了系统弹性。&lt;/li&gt;&lt;li&gt;我们不再使用数量有限的专用实例，而是利用了 Netflix 由于自动缩放微服务而产生的&lt;a href="https://netflixtechblog.com/creating-your-own-ec2-spot-market-6dd001875f5"&gt;内部低谷&lt;/a&gt;，从而显着提高了计算弹性和资源利用效率。&lt;/li&gt;&lt;li&gt;我们推出了编码创新，例如针对&lt;a href="https://medium.com/netflix-techblog/per-title-encode-optimization-7e99442b62a2"&gt;每个标题&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/optimized-shot-based-encodes-now-streaming-4b9464204830"&gt;每个镜头的&lt;/a&gt;优化，为 Netflix 会员提供了显着的体验质量 (QoE) 改进。&lt;/li&gt;&lt;li&gt;通过与工作室内容系统集成，我们使管道能够利用创意方面的丰富元数据，并创造更具吸引力的会员体验，例如&lt;a href="https://en.wikipedia.org/wiki/Black_Mirror:_Bandersnatch"&gt;交互式讲故事&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;我们扩展了管道支持来服务我们的工作室/内容开发用例，与传统的流媒体用例相比，这些用例具有不同的延迟和弹性要求。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;过去十五年的经验让我们更加坚信，高效、灵活的视频处理管道使我们能够创新并支持我们的流媒体服务以及我们的工作室合作伙伴，这对于 Netflix 的持续成功至关重要。为此，Encoding Technologies (ET) 的视频和图像编码团队在过去几年中一直在我们的下一代基于微服务的计算平台&lt;a href="https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad"&gt;Cosmos&lt;/a&gt;上重建视频处理管道。&lt;/p&gt;&lt;h3&gt;从《重装上阵》到《宇宙》&lt;/h3&gt;&lt;h4&gt;重装上阵&lt;/h4&gt;&lt;p&gt;从2014年开始，我们在第三代平台&lt;a href="https://www.youtube.com/watch?v=JouA10QJiNc"&gt;Reloaded&lt;/a&gt;上开发并运营视频处理管道。 Reloaded 的架构良好，提供了良好的稳定性、可扩展性和合理的灵活性。它是我们团队开发的众多编码创新的基础。&lt;/p&gt;&lt;p&gt;在设计 Reloaded 时，我们专注于一个用例：将从工作室收到的高质量媒体文件（也称为夹层）转换为用于 Netflix 流媒体的压缩资产。 Reloaded 被创建为一个单一的整体系统，来自 ET 各个媒体团队的开发人员以及我们的平台合作伙伴团队内容基础设施和解决方案 (CIS)1 在同一代码库上工作，构建了一个处理所有媒体资产的单一系统。多年来，该系统不断扩展以支持各种新的用例。这导致系统复杂度显着增加，Reloaded的局限性开始显现：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;耦合功能：&lt;/em&gt; Reloaded 由多个工作模块和一个编排模块组成。新的 Reloaded 模块的设置及其与编排的集成需要大量的工作，这导致在开发新功能时偏向于增强而不是创建。例如，在《重装上阵》中&lt;a href="https://netflixtechblog.com/netflix-video-quality-at-scale-with-cosmos-microservices-552be631c113"&gt;，视频质量计算是在视频编码器模块内部实现的&lt;/a&gt;。通过这种实现，在不重新编码的情况下重新计算视频质量是极其困难的。&lt;/li&gt;&lt;li&gt;&lt;em&gt;整体结构&lt;/em&gt;：由于重新加载的模块通常位于同一个存储库中，因此很容易忽视代码隔离规则，并且在本应牢固的边界上存在相当多的无意的代码重用。这种重用造成了紧密耦合并降低了开发速度。模块之间的紧密耦合进一步迫使我们将所有模块部署在一起。&lt;/li&gt;&lt;li&gt;&lt;em&gt;发布周期长&lt;/em&gt;：联合部署意味着人们更加担心意外的生产中断，因为对于这种规模的部署来说，调试和回滚可能很困难。这带动了“释放列车”的临近。每两周，对所有模块进行一次“快照”，并提升为“候选版本”。然后，该候选版本经过了详尽的测试，试图覆盖尽可能大的表面积。这个测试阶段大约花了两周时间。因此，根据代码更改的合并时间，可能需要两到四个星期的时间才能投入生产。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;随着时间的推移和功能的增长，《重装上阵》中新功能的贡献率下降了。由于克服架构限制所需的工作量过大，一些有希望的想法被放弃了。曾经为我们提供良好服务的平台现在正在成为发展的拖累。&lt;/p&gt;&lt;h4&gt;宇宙&lt;/h4&gt;&lt;p&gt;作为回应，CIS 和 ET 团队于 2018 年开始开发下一代平台 Cosmos。除了开发人员在《重装上阵》中已经享有的可扩展性和稳定性之外，Cosmos 还旨在显着提高系统灵活性和功能开发速度。为了实现这一目标，Cosmos 被开发为一个计算平台，用于工作流驱动、以媒体为中心的微服务。&lt;/p&gt;&lt;p&gt;微服务架构提供了服务之间的强解耦。每个微服务工作流支持减轻了实现复杂媒体工作流逻辑的负担。最后，相关抽象允许媒体算法开发人员专注于视频和音频信号的操作，而不是基础设施问题。 Cosmos 提供的优势的完整列表可以在链接的&lt;a href="https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad"&gt;博客&lt;/a&gt;中找到。&lt;/p&gt;&lt;h3&gt;在 Cosmos 中构建视频处理管道&lt;/h3&gt;&lt;h4&gt;服务边界&lt;/h4&gt;&lt;p&gt;在微服务架构中，系统由许多细粒度的服务组成，每个服务专注于单一功能。因此，第一件事（可以说是最重要的）是确定边界并定义服务。&lt;/p&gt;&lt;p&gt;在我们的管道中，随着媒体资产从创建到摄取再到交付，它们会经历许多处理步骤，例如分析和转换。我们分析了这些处理步骤以识别“边界”，并将它们分组到不同的域中，这些域反过来又成为我们设计的微服务的构建块。&lt;/p&gt;&lt;p&gt;例如，在《Reloaded》中，视频编码模块捆绑了 5 个步骤：&lt;/p&gt;&lt;p&gt; 1.将输入视频分成小块&lt;/p&gt;&lt;p&gt;2. 对每个块独立编码&lt;/p&gt;&lt;p&gt;3.计算每个块的质量分数（ &lt;a href="https://netflixtechblog.com/vmaf-the-journey-continues-44b51ee9ed12"&gt;VMAF&lt;/a&gt; ）&lt;/p&gt;&lt;p&gt; 4. 将所有编码块组装成单个编码视频&lt;/p&gt;&lt;p&gt;5. 汇总所有块的质量分数&lt;/p&gt;&lt;p&gt;从系统角度来看，组装的编码视频是首要关注的问题，而内部分块和单独的块编码的存在是为了满足某些延迟和弹性要求。此外，如上所述，与编码服务相比，视频质量计算提供了完全独立的功能。&lt;/p&gt;&lt;p&gt;因此，在 Cosmos 中，我们创建了两个独立的微服务：视频编码服务（VES）和&lt;a href="https://netflixtechblog.com/netflix-video-quality-at-scale-with-cosmos-microservices-552be631c113"&gt;视频质量服务（VQS）&lt;/a&gt; ，每个微服务都提供清晰、解耦的功能。作为实现细节，分块编码和组装被抽象到 VES 中。&lt;/p&gt;&lt;h4&gt;视频服务&lt;/h4&gt;&lt;p&gt;上述方法应用于视频处理管道的其余部分，以识别功能和服务边界，从而创建以下视频服务²。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;视频巡检服务（VIS）：该服务以Mezzanine为输入，进行各种巡检。它从夹层的不同层提取元数据以供下游服务。此外，如果观察到无效或意外的元数据，检查服务会标记问题，并向上游团队提供可操作的反馈。&lt;/li&gt;&lt;li&gt;复杂性分析服务 (CAS)：最佳编码方案高度依赖于内容。该服务以夹层作为输入并执行分析以了解内容复杂性。它调用视频编码服务进行&lt;a href="https://netflixtechblog.com/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f"&gt;预编码&lt;/a&gt;，调用视频质量服务进行质量评估。结果被保存到数据库中，以便可以重复使用。&lt;/li&gt;&lt;li&gt;阶梯生成服务 (LGS)：此服务为给定编码系列（H.264、AV1 等）创建完整的比特率阶梯。它从 CAS 获取复杂性数据并运行优化算法来创建编码配方。 CAS 和 LGS 涵盖了我们之前在技术博客中介绍的大部分创新（&lt;a href="http://techblog.netflix.com/2015/12/per-title-encode-optimization.html"&gt;按标题&lt;/a&gt;、&lt;a href="http://techblog.netflix.com/2016/12/more-efficient-mobile-encodes-for.html"&gt;移动编码&lt;/a&gt;、&lt;a href="https://netflixtechblog.com/optimized-shot-based-encodes-now-streaming-4b9464204830"&gt;按镜头&lt;/a&gt;、 &lt;a href="https://netflixtechblog.com/optimized-shot-based-encodes-for-4k-now-streaming-47b516b10bbb"&gt;优化 4K 编码&lt;/a&gt;等）。通过将梯子生成封装到单独的微服务 (LGS) 中，我们将梯子优化算法与复杂性分析数据（驻留在 CAS 中）的创建和管理分离。我们希望这能给我们更大的实验自由和更快的创新速度。&lt;/li&gt;&lt;li&gt;视频编码服务 (VES)：该服务采用夹层和编码配方并创建编码视频。该配方包括所需的编码格式和输出属性，例如分辨率、比特率等。该服务还提供允许根据用例微调延迟、吞吐量等的选项。&lt;/li&gt;&lt;li&gt;视频验证服务 (VVS)：此服务采用编码视频和有关编码的期望列表。这些期望包括编码配方中指定的属性以及编解码器规范的一致性要求。 VVS 分析编码视频并将结果与​​指定的期望进行比较。任何差异都会在响应中标记出来以提醒呼叫者。&lt;/li&gt;&lt;li&gt; &lt;a href="https://netflixtechblog.com/netflix-video-quality-at-scale-with-cosmos-microservices-552be631c113"&gt;视频质量服务（VQS）&lt;/a&gt; ：该服务将夹层和编码视频作为输入，并计算编码视频的质量得分（VMAF）。&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;服务编排&lt;/h4&gt;&lt;p&gt;每个视频服务都提供专用功能，它们协同工作以生成所需的视频资源。目前，Netflix 视频管道的两个主要用例是为会员流媒体和工作室运营制作资产。对于每个用例，我们创建了一个专用的工作流编排器，以便可以自定义服务编排以最好地满足相应的业务需求。&lt;/p&gt;&lt;p&gt;对于流媒体用例，生成的视频将部署到我们的内容交付网络 (CDN) 供 Netflix 会员使用。这些视频很容易被观看数百万次。流媒体工作流程协调器利用几乎所有视频服务来创建流，以提供无可挑剔的会员体验。它利用 VIS 检测和拒绝不合格或低质量的夹层，调用 LGS 进行编码配方优化，使用 VES 编码视频，并调用 VQS 进行质量测量，其中质量数据进一步馈送到 Netflix 的数据管道以进行分析和监控。除了视频服务之外，流媒体工作流程协调器还使用音频和定时文本服务来生成音频和文本资产，并使用打包服务来“容器化”流媒体资产。&lt;/p&gt;&lt;p&gt;对于工作室用例，一些示例视频资产是营销剪辑和日常制作编辑代理。来自工作室方的请求通常对延迟敏感。例如，制作团队的某人可能正在等待视频审核，以便他们决定第二天的拍摄计划。因此，Studio Workflow Orchestrator 针对快速周转进行了优化，并专注于核心媒体处理服务。此时，Studio Workflow Orchestrator 调用 VIS 来提取所摄取资产的元数据，并使用预定义的配方调用 VES。与会员直播相比，工作室运营对视频处理有不同且独特的要求。因此，Studio Workflow Orchestrator 是某些编码功能（例如取证水印和时间码/文本烧录）的独家用户。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Wen2M8BgqhC7Hacc1O2Nqg.png" /&gt;&lt;/figure&gt;&lt;h3&gt;我们现在在哪里&lt;/h3&gt;&lt;p&gt;我们在制作中已经让新的视频管道与《重装上阵》一起运行了几年。在此期间，我们完成了 Reloaded 中所有必要功能的迁移，开始逐渐将流量转移到一个用例，并于 2023 年 9 月完成了切换。&lt;/p&gt;&lt;p&gt;虽然现在还处于早期阶段，但我们已经看到了新平台的好处，特别是功能交付的便捷性。值得注意的是，Netflix于2022年11月推出了&lt;a href="https://netflixtechblog.com/ensuring-the-successful-launch-of-ads-on-netflix-f99490fdf1ba"&gt;广告支持计划。&lt;/a&gt;处理广告创意提出了一些新的挑战：广告的媒体格式与团队熟悉的影视夹层有很大不同，并且有一套新的媒体处理要求与广告的业务需求相关。凭借 Cosmos 的模块化和开发人员生产力优势，我们能够快速迭代管道，以满足不断变化的需求并支持产品的成功发布。&lt;/p&gt;&lt;h3&gt;概括&lt;/h3&gt;&lt;p&gt;重建视频管道对于团队来说是一项艰巨的任务。我们对所取得的成就感到非常自豪，也渴望与技术社区分享我们的旅程。本博客的重点是提供概述：我们的管道和平台的简史、为什么需要重建、这些新服务是什么样的，以及它们如何用于 Netflix 业务。在下一篇博客中，我们将深入研究视频编码服务 (VES) 的详细信息，逐步解释服务创建，并分享经验教训（我们有很多！）。我们还计划在未来的技术博客中介绍其他视频服务。关注&lt;a href="https://netflixtechblog.com/"&gt;Netflix 技术博客&lt;/a&gt;以了解最新动态。&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;致谢&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;大力赞扬 CIS 团队在构建 Cosmos 平台方面所做的出色工作以及他们对服务开发人员反馈的积极接受。&lt;/p&gt;&lt;p&gt;我们要向我们的用户、流编码管道团队和视频工程团队表示感谢。就像我们的反馈有助于完善平台一样，我们用户的反馈也有助于构建高质量的服务。&lt;/p&gt;&lt;p&gt;我们还要感谢 Christos Bampis 和zhi Li 对视频服务的重大贡献，以及我们的两位前团队成员 Chao Chen 和 Megha Manohara 为该项目的早期开发做出的贡献。&lt;/p&gt;&lt;h4&gt;脚注&lt;/h4&gt;&lt;ol&gt;&lt;li&gt;前身为媒体云工程/MCE 团队。&lt;/li&gt;&lt;li&gt;视频服务的实际数量比此处列出的要多。其中一些是 Netflix 特定的，因此本博客中省略。 &lt;/li&gt;&lt;/ol&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=4e5e6310e359" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359"&gt;使用微服务重建 Netflix 视频处理管道&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Wed, 10 Jan 2024 23:20:40 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359?source=rss----2615bd06b42e---4</guid></item></channel></rss>