<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Netflix 技术博客 - Medium</title><link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link><description>了解 Netflix 世界一流的工程工作、公司文化、产品开发等。 - 中等的</description><lastBuildDate>Fri, 28 Jun 2024 03:06:30 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>【Enhancing Netflix Reliability with Service-Level Prioritized Load Shedding】</title><link>https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=rss----2615bd06b42e---4</link><description>&lt;h4&gt;在应用程序级别应用服务质量技术&lt;/h4&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/amendira/"&gt;阿尼鲁德·门迪拉塔&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/kzwang"&gt;凯文·王&lt;/a&gt;、&lt;a href="https://jolynch.github.io/"&gt;乔伊·林奇&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/ivern"&gt;哈维尔·费尔南德斯-艾翁&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/benjamin-fedorka"&gt;本杰明·费多卡&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;p&gt;2020 年 11 月，我们在博客文章&lt;a href="https://netflixtechblog.com/keeping-netflix-reliable-using-prioritized-load-shedding-6cc827b02f94"&gt;“使用优先负载卸载保持 Netflix 可靠性”&lt;/a&gt;中引入了 API 网关级别的优先负载卸载概念。今天，我们很高兴能够更深入地探讨如何将此策略扩展到个人服务级别，重点关注视频流控制平面和数据平面，以进一步增强用户体验和系统弹性。&lt;/p&gt;&lt;h3&gt; Netflix 减载技术的演变&lt;/h3&gt;&lt;p&gt;在 Netflix，确保数百万用户同时获得无缝观看体验至关重要。我们最初的优先负载卸载方法是在 Zuul API 网关层实现的。该系统有效地管理不同类型的网络流量，确保关键回放请求优先于不太关键的遥测流量。&lt;/p&gt;&lt;p&gt;在此基础上，我们认识到需要在架构中更深入地应用类似的优先级逻辑，特别是在服务层，同一服务中不同类型的请求可以有不同的优先级。除了我们的边缘 API 网关之外，在服务级别应用这些技术的优势还包括：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;服务团队可以拥有自己的优先级逻辑，并可以应用更细粒度的优先级。&lt;/li&gt;&lt;li&gt;这可用于后端到后端的通信，即用于不在我们的边缘 API 网关后面的服务。&lt;/li&gt;&lt;li&gt;通过将不同的请求类型合并到一个集群中，并在必要时放弃低优先级请求，而不是维护单独的集群来隔离故障，服务可以更有效地使用云容量。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;引入服务级别优先减载&lt;/h3&gt;&lt;p&gt;PlayAPI 是视频流控制平面上的关键后端服务，负责处理设备发起的清单和开始播放所需的许可证请求。我们根据这些请求的重要性将其分为两类：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;用户发起的请求（关键）：&lt;/strong&gt;这些请求是在用户点击播放时发出的，并直接影响用户开始观看节目或电影的能力。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;预取请求（非关键）：&lt;/strong&gt;当用户在没有点击播放的情况下浏览内容时，会乐观地发出这些请求，以减少用户决定观看特定标题时的延迟。仅预取请求失败不会导致播放失败，但会稍微增加按播放键和屏幕上显示视频之间的延迟。 &lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2KByIB47RWng5UNH" /&gt;&lt;figcaption&gt;&lt;em&gt;Chrome 上的 Netflix 在用户浏览内容时向 PlayAPI 发出预取请求&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;问题&lt;/h4&gt;&lt;p&gt;为了处理大流量峰值、高后端延迟或规模不足的后端服务，PlayAPI 以前使用并发限制器来限制请求，这会同等地降低用户启动请求和预取请求的可用性。这并不理想，因为：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;预取流量的峰值降低了用户发起请求的可用性&lt;/li&gt;&lt;li&gt;当系统有足够的容量来服务所有用户发起的请求时，后端延迟的增加同样会降低用户发起的请求和预取请求的可用性。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;将关键请求和非关键请求分片到单独的集群中是一种选择，它解决了问题 1 并增加了两种类型请求之间的故障隔离，但它带来了更高的计算成本。分片的另一个缺点是它增加了一些运营开销——工程师需要确保为新集群启用 CI/CD、自动扩展、指标和警报。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/371/0*pNfPHfPFe_k8r-YC" /&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;选项 1&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;— 无隔离&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/735/0*BqZJayMkzt5-ZIHB" /&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;选项 2&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;— 隔离但计算成本更高&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;我们的解决方案&lt;/h4&gt;&lt;p&gt;我们在 PlayAPI 中实现了一个并发限制器，该限制器优先考虑用户发起的请求而不是预取请求，而无需物理上分片两个请求处理程序。该机制使用开源&lt;a href="https://github.com/Netflix/concurrency-limits"&gt;Netflix/concurrency-limits&lt;/a&gt; Java 库的分区功能。我们在限制器中创建两个分区：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;用户启动的分区：&lt;/strong&gt;保证 100% 的吞吐量。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;预取分区：&lt;/strong&gt;仅利用多余的容量。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/411/0*BS1KXcwsikLJ4Zok" /&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;选项 3&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;— 具有优先减载功能的单个集群可提供应用程序级隔离并降低计算成本。每个实例都服务两种类型的请求，并具有一个分区，其大小动态调整以确保预取请求仅获得多余的容量。这允许用户发起的请求在必要时“窃取”预取容量。&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;分区限制器配置为预处理&lt;a href="https://github.com/Netflix/concurrency-limits/blob/master/concurrency-limits-servlet/src/main/java/com/netflix/concurrency/limits/servlet/ConcurrencyLimitServletFilter.java"&gt;Servlet Filter&lt;/a&gt; ，它使用设备发送的 HTTP 标头来确定请求的关键性，从而避免需要读取和解析被拒绝的请求的请求正文。这确保了限制器本身不是瓶颈，并且可以在使用最少的 CPU 的同时有效地拒绝请求。例如，过滤器可以初始化为&lt;/p&gt;&lt;pre&gt;过滤器过滤器 = new ConcurrencyLimitServletFilter(&lt;br /&gt; 新的 ServletLimiterBuilder()&lt;br /&gt; .named(&amp;quot;playapi&amp;quot;)&lt;br /&gt; .partitionByHeader(&amp;quot;X-Netflix.Request-Name&amp;quot;)&lt;br /&gt; .partition(&amp;quot;用户启动的&amp;quot;, 1.0)&lt;br /&gt; .partition(&amp;quot;预取&amp;quot;, 0.0)&lt;br /&gt; 。建造（））;&lt;/pre&gt;&lt;p&gt;请注意，在稳定状态下，没有限制，并且优先级对预取请求的处理没有影响。仅当服务器达到并发限制并需要拒绝请求时，优先级机制才会启动。&lt;/p&gt;&lt;h4&gt;测试&lt;/h4&gt;&lt;p&gt;为了验证我们的负载卸载是否按预期工作，我们使用故障注入测试在预取调用中注入 2 秒延迟，其中这些调用的典型 p99 延迟小于 200 毫秒。故障被注入到一个具有定期负载卸载的基线实例和一个具有优先负载卸载的金丝雀实例上。 PlayAPI 调用的一些内部服务使用单独的集群来处理用户发起的请求和预取请求，并更热地运行预取集群。此测试用例模拟下游服务的预取集群遇到高延迟的场景。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oU-FvJW2BCw5Z158" /&gt;&lt;figcaption&gt;&lt;em&gt;基线——没有优先减载。预取和用户启动的可用性均下降&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*hcY1lYOP4CVxn-LS" /&gt;&lt;figcaption&gt;&lt;em&gt;Canary——优先减载。只有预取可用性下降，而用户启动可用性保持在 100%&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果没有优先负载卸载，当注入延迟时，用户启动和预取的可用性都会下降。然而，在添加优先负载卸载之后，用户发起的请求保持了 100% 的可用性，并且只有预取请求受到限制。&lt;/p&gt;&lt;p&gt;我们已准备好将其投入生产，看看它在野外的表现如何！&lt;/p&gt;&lt;h4&gt;实际应用和结果&lt;/h4&gt;&lt;p&gt;Netflix 工程师努力保持我们的系统可用，过了一段时间我们才发生了一次生产事故，测试了我们解决方案的有效性。在部署优先减载几个月后，我们的 Netflix 发生了基础设施中断，影响了许多用户的流媒体播放。故障修复后，我们发现 Android 设备每秒的预取请求激增了 12 倍，这可能是因为排队请求积压了。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0AdiUnX8fdinJTNR" /&gt;&lt;figcaption&gt;&lt;em&gt;Android 预取 RPS 激增&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这可能会导致第二次中断，因为我们的系统无法扩展以应对这次流量高峰。 PlayAPI 中的优先负载卸载对我们有帮助吗？&lt;/p&gt;&lt;p&gt;是的！虽然预取请求的可用性下降了 20%，但由于优先减载，用户发起的请求的可用性&amp;gt; 99.4%。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/996/0*gVNG6nlvDevP-53B" /&gt;&lt;figcaption&gt;&lt;em&gt;预取和用户发起请求的可用性&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们一度限制了 50% 以上的请求，但用户发起的请求的可用性仍然超过 99.4%。&lt;/p&gt;&lt;h3&gt;通用服务工作优先级&lt;/h3&gt;&lt;p&gt;基于这种方法的成功，我们创建了一个内部库，使服务能够基于可插入的利用率措施执行具有多个优先级的优先负载卸载。&lt;/p&gt;&lt;p&gt;与需要处理大量具有不同优先级的请求的 API 网关不同，大多数微服务通常只接收具有几个不同优先级的请求。为了保持不同服务之间的一致性，我们引入了四个受&lt;a href="https://linux.die.net/man/8/tc-prio"&gt;Linux tc-prio 级别&lt;/a&gt;启发的预定义优先级存储桶：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;关键&lt;/strong&gt;：影响核心功能——如果我们没有完全失败，这些功能将永远不会消失。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;降级&lt;/strong&gt;：影响用户体验——随着负载的增加，这些将逐渐消失。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;BEST_EFFORT&lt;/strong&gt; ：不影响用户 - 这些将以尽最大努力的方式响应，并可能在正常操作中逐渐消失。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;BULK&lt;/strong&gt; ：后台工作，预计这些会定期脱落。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;服务可以选择上游客户端的优先级&lt;em&gt;，也&lt;/em&gt;可以通过检查各种请求属性（例如 HTTP 标头或请求正文）将传入请求映射到这些优先级存储桶之一，以实现更精确的控制。以下是服务如何将请求映射到优先级存储桶的示例：&lt;/p&gt;&lt;pre&gt; ResourceLimiterRequestPriorityProvider requestPriorityProvider() {&lt;br /&gt; 返回 contextProvider -&amp;gt; {&lt;br /&gt; if (contextProvider.getRequest().isCritical()) {&lt;br /&gt; 返回 PriorityBucket.CRITICAL；&lt;br /&gt; } else if (contextProvider.getRequest().isHighPriority()) {&lt;br /&gt; 返回 PriorityBucket.DEGRADED；&lt;br /&gt; } else if (contextProvider.getRequest().isMediumPriority()) {&lt;br /&gt; 返回 PriorityBucket.BEST_EFFORT；&lt;br /&gt; } 别的 {&lt;br /&gt; 返回 PriorityBucket.BULK；&lt;br /&gt; }&lt;br /&gt; };&lt;br /&gt; }&lt;/pre&gt;&lt;h4&gt;基于通用 CPU 的减载&lt;/h4&gt;&lt;p&gt;Netflix 的大多数服务都会根据 CPU 利用率自动缩放，因此将系统负载与优先负载削减框架结合起来是一种自然的衡量标准。一旦请求被映射到优先级存储桶，服务就可以根据 CPU 利用率确定何时从特定存储桶中减少流量。为了维持需要扩展的自动缩放信号，优先卸载仅&lt;em&gt;在达到目标 CPU 利用率后才&lt;/em&gt;开始卸载负载，并且随着系统负载的增加，逐渐卸载更关键的流量以尝试维持用户体验。&lt;/p&gt;&lt;p&gt;例如，如果集群的自动缩放目标是 60% CPU 利用率，则可以将其配置为在 CPU 利用率超过此阈值时启动释放请求。当流量高峰导致集群的CPU利用率显着超过此阈值时，它将逐渐丢弃低优先级流量，为高优先级流量节省资源。此方法还允许有更多时间进行自动扩展以向集群添加其他实例。添加更多实例后，CPU 利用率将下降，低优先级流量将恢复正常服务。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/637/0*sdKTOYaSQ_tEjE8r" /&gt;&lt;figcaption&gt;&lt;em&gt;基于不同优先级存储桶的 CPU 利用率（X 轴）的负载卸载请求百分比（Y 轴）&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;基于 CPU 的减载实验&lt;/h4&gt;&lt;p&gt;我们进行了一系列实验，在一个服务上发送大量请求，该服务通常以 45% CPU 为目标进行自动扩展，但为了在极端负载条件下监控 CPU 负载脱落而阻止了扩展。这些实例被配置为在 CPU 利用率达到 60% 后去除非关键流量，在 CPU 利用率达到 80% 后去除关键流量。&lt;/p&gt;&lt;p&gt;当 RPS 调高到自动缩放量的 6 倍以上时，该服务能够首先释放非关键请求，然后释放关键请求。延迟始终保持在合理的范围内，并且成功的 RPS 吞吐量保持稳定。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Wr6bJzQVf3dV4clf" /&gt;&lt;figcaption&gt;&lt;em&gt;使用合成流量的基于 CPU 的负载卸载的实验行为。&lt;/em&gt; &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*DZCzE_AAi2cJXRRr" /&gt;&lt;figcaption&gt;&lt;em&gt;在整个实验过程中，即使 RPS 超过自动缩放目标的 6 倍，P99 延迟仍保持在合理范围内。&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;具有减载功能的反模式&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;反模式 1 — 无脱落&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在上图中，限制器很好地保持了成功请求的低延迟。如果这里没有脱落，我们会看到所有请求的延迟都会增加，而不是某些可以重试的请求快速失败。此外，这可能会导致死亡螺旋，其中一个实例变得不健康，导致其他实例负载增加，导致所有实例在自动扩展启动之前变得不健康。&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/0*Bp5AKoNtQOfHaExB" /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/0*WPvKjlopcBGixDGB" /&gt;&lt;figcaption&gt;&lt;em&gt;无负载卸载：在没有负载卸载的情况下，增加的延迟可能会降低所有请求的性能，而不是拒绝某些请求（可以重试），并且可能会使实例运行状况不佳&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;反模式 2——充血性衰竭&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;另一个需要注意的反模式是充血性衰竭或过度脱落。如果由于流量增加而进行减载，则减载后成功的 RPS 不应下降。以下是充血衰竭的示例： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/775/0*gPGs2BJ1Oxu9O7TK" /&gt;&lt;figcaption&gt;&lt;em&gt;拥塞故障：16:57 后，服务开始拒绝大多数请求，并且无法维持在负载削减启动之前的成功 240 RPS。这可以在固定并发限制器中或当负载削减消耗过多时看到CPU 阻止完成任何其他工作&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们可以在上面的&lt;strong&gt;基于 CPU 的负载卸载实验&lt;/strong&gt;部分中看到，我们的负载卸载实现通过保持低延迟并在负载卸载期间维持与以前一样成功的 RPS 来避免这两种反模式。&lt;/p&gt;&lt;h3&gt;基于通用 IO 的减载&lt;/h3&gt;&lt;p&gt;某些服务不受 CPU 限制，而是通过支持服务或数据存储受到 IO 限制，当计算或存储容量过载时，这些服务或数据存储可以通过增加延迟来施加背压。对于这些服务，我们重新使用优先负载卸载技术，但我们引入了新的利用措施来馈入卸载逻辑。除了标准自适应并发限制器（本身是平均延迟的衡量标准）之外，我们的初始实现还支持两种形式的基于延迟的缩减：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;该服务可以指定每个端点的目标和最大延迟，这样无论后端如何，当服务异常缓慢时，都可以减少服务。&lt;/li&gt;&lt;li&gt;在&lt;a href="https://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6"&gt;数据网关&lt;/a&gt;上运行的 Netflix 存储服务会返回观察到的存储目标和最大延迟 SLO 利用率，从而允许服务在超出分配的存储容量时进行卸载。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这些利用率措施提供了早期预警信号，表明服务正在向后端产生过多负载，并允许其在压垮后端之前放弃低优先级工作。与单独的并发限制相比，这些技术的主要优点是它们需要更少的调整，因为我们的服务已经必须保持严格的延迟服务级别目标（SLO），例如 p50 &amp;lt; 10ms 和 p100 &amp;lt; 500ms。因此，将这些现有的 SLO 重新表述为利用率，使我们能够尽早放弃低优先级工作，以防止对高优先级工作产生进一步的延迟影响。同时，系统&lt;em&gt;将在维持 SLO 的同时接受尽可能多的工作&lt;/em&gt;。&lt;/p&gt;&lt;p&gt;为了创建这些利用率指标，我们计算有多少请求的处理速度比我们的目标和最大延迟目标&lt;em&gt;慢&lt;/em&gt;，并发出未能满足这些延迟目标的请求的百分比。例如，我们的 KeyValue 存储服务为每个命名空间提供 10 毫秒的目标，最大延迟为 500 毫秒，所有客户端都会收到每个数据命名空间的利用率度量，以纳入其优先负载削减。这些措施如下所示：&lt;/p&gt;&lt;pre&gt;利用率（命名空间）= {&lt;br /&gt; 总体 = 12&lt;br /&gt; 延迟时间={&lt;br /&gt; slo_目标 = 12,&lt;br /&gt; slo_max = 0&lt;br /&gt; }&lt;br /&gt; 系统={&lt;br /&gt; 存储=17，&lt;br /&gt; 计算 = 10,&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;在这种情况下，12% 的请求比 10 毫秒目标慢，0% 比 500 毫秒最大延迟（超时）慢，并且利用了 17% 的已分配存储。不同的用例在优先删除时会考虑不同的利用率，例如，当系统存储利用率接近容量时，每天写入数据的批次可能会被删除，因为写入更多数据会造成进一步的不稳定。&lt;/p&gt;&lt;p&gt;延迟利用很有用的一个例子是我们的一项关键文件源服务，该服务接受 AWS 云中新文件的写入，并充当这些文件到我们的 Open Connect CDN 基础设施的源（提供读取服务）。写入是最关键的，永远不应该被服务删除，但是当支持数据存储过载时，合理的做法是逐步将读取删除到对 CDN 不太重要的文件，因为它可以重试这些读取，并且不会影响产品体验。&lt;/p&gt;&lt;p&gt;为了实现这一目标，源服务配置了一个基于 KeyValue 延迟的限制器，当数据存储报告目标延迟利用率超过 40% 时，该限制器开始减少对 CDN 不太重要的文件的读取。然后，我们通过生成超过 50Gbps 的读取流量对系统进行压力测试，其中一些是高优先级文件，一些是低优先级文件： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HI2zGO_MOxD-X1cG" /&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*AZnhEhtrsp9MEJFA" /&gt;&lt;/figure&gt;&lt;p&gt;在此测试中，对低优先级和高优先级文件都有名义数量的关键写入和大量读取。在左上角的图表中，我们将 ~4MiB 文件的读取速度提升到每秒 2000 次，直到我们可以在顶部中心图表中以超过 50Gbps 的速度触发后端存储的过载。当发生这种情况时，右上角的图表显示，即使在很大的负载下，源端&lt;em&gt;也只会&lt;/em&gt;放弃低优先级的读取工作，以保留高优先级的写入和读取。在此更改之前，当我们遇到断点时，关键写入&lt;em&gt;和&lt;/em&gt;读取将会失败以及低优先级读取。在此测试期间，文件服务的 CPU 负载为标称负载 (&amp;lt;10%)，因此在这种情况下，只有基于 IO 的限制器才能保护系统。同样重要的是要注意，只要支持数据存储继续以低延迟接受流量，源就会提供更多流量，从而防止我们过去遇到的并发限制问题，即在实际上没有任何问题时它们会过早地流出，或者当我们进入充血衰竭状态时为时已晚。&lt;/p&gt;&lt;h3&gt;结论和未来方向&lt;/h3&gt;&lt;p&gt;事实证明，即使在出现意外的系统压力时，实施服务级别优先负载卸载也能在为 Netflix 客户维持高可用性和出色的用户体验方面向前迈出重要一步。&lt;/p&gt;&lt;p&gt;请继续关注更多更新，我们会不断创新，让您喜爱的节目顺利播放，无论 SLO 破坏者在等待什么。&lt;/p&gt;&lt;h3&gt;致谢&lt;/h3&gt;&lt;p&gt;我们要感谢 Netflix 消费产品、平台和开放连接团队的许多成员，他们设计、实施和测试了这些优先级划分技术。特别是：&lt;a href="https://www.linkedin.com/in/xiaomei-liu-b475711"&gt;刘晓梅&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/rummadis"&gt;Raj Ummadisetty&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/shyam-gala-5891224/"&gt;Shyam Gala&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/justin-guerra-3282262b"&gt;Justin Guerra&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/william-schor"&gt;William Schor&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/tonyghita"&gt;Tony Ghita&lt;/a&gt;等人。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e735e6ce8f7d" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d"&gt;《通过服务级别优先减载增强 Netflix 可靠性》&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;，人们通过突出显示并回应这个故事来继续讨论。&lt;/p&gt;</description><pubDate>Tue, 25 Jun 2024 22:58:09 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/enhancing-netflix-reliability-with-service-level-prioritized-load-shedding-e735e6ce8f7d?source=rss----2615bd06b42e---4</guid></item><item><title>【A Recap of the Data Engineering Open Forum at Netflix】</title><link>https://netflixtechblog.com/a-recap-of-the-data-engineering-open-forum-at-netflix-6b4d4410b88f?source=rss----2615bd06b42e---4</link><description>&lt;h4&gt;2024 年 4 月 18 日 Netflix 首届数据工程开放论坛会议摘要&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/981/0*k1mwTj0BpJuP0TDi" /&gt;&lt;figcaption&gt; 2024 年 4 月 18 日在 Netflix 举行的数据工程开放论坛。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 Netflix，我们渴望为世界带来欢乐，而我们的数据工程团队通过大规模实现数据驱动的决策，在这一使命中发挥着至关重要的作用。 Netflix 并不是唯一一个数据工程师通过创造性解决方案解决挑战性问题的地方。 2024 年 4 月 18 日，我们在洛斯加托斯办公室举办了首届数据工程开放论坛，汇聚了来自各个行业的数据工程师进行分享、学习和交流。&lt;/p&gt;&lt;p&gt;在会议上，我们的演讲者分享了他们对数据工程的现代发展、当前挑战和未来前景的独特观点。我们很高兴与世界其他地方分享会议的谈话录音。&lt;/p&gt;&lt;h4&gt;开场白&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/9NnnYHuH8GQ?si=nYBpQPhGwxX-oo1l"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/max-schmeiser/"&gt;Max Schmeiser&lt;/a&gt; （工作室和内容数据科学与工程副总裁）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：Max Schmeiser 对所有与会者表示热烈欢迎，这标志着我们首届数据工程开放论坛的开始。&lt;/p&gt;&lt;h4&gt;从基于规则的分类器演变而来：Netflix 数据平台中机器学习支持的自动修复&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/0j6b9V9tmKA?si=fMEuLmrIK5ATi52d"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲嘉宾：&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;a href="https://www.linkedin.com/in/stephanievezich/"&gt;Stephanie Vezich Tamayo&lt;/a&gt; （Netflix 高级机器学习工程师）&lt;/li&gt;&lt;li&gt; &lt;a href="https://www.linkedin.com/in/binbing-hou/"&gt;Binbing Hou&lt;/a&gt; （Netflix 高级软件工程师）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;： 在 Netflix，每天都有数十万个工作流程和数百万个作业在我们的大数据平台上运行，但诊断和修复作业故障可能会带来相当大的运营负担。为了有效地处理错误，Netflix 开发了一种基于规则的错误分类器，称为“Pitive”。然而，随着系统规模和复杂性的增加，由于其对操作自动化的支持有限，特别是在处理内存配置错误和未分类错误方面，Pitive 一直面临着挑战。为了应对这些挑战，我们开发了一项名为“自动修复”的新功能，它将基于规则的分类器与机器学习服务集成在一起。&lt;/p&gt;&lt;h4&gt;自动化数据架构师：用于企业数据建模的生成式 AI&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/DtzIIVJq8wA?si=i5fLXA7G8IMyiF0u"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/jide-o-87602512/"&gt;Jide Ogunjobi&lt;/a&gt; （Context Data 创始人兼首席技术官）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： 随着组织在不同系统中积累越来越大的数据存储，有效查询企业数据并从中获取见解仍然是持续的挑战。为了解决这个问题，我们建议开发一种智能代理，可以自动发现、映射和查询企业内的所有数据。这种“企业数据模型/架构师代理”采用生成式人工智能技术来进行自主企业数据建模和架构。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1e3shrfbfV2J6S4-" /&gt;&lt;figcaption&gt;Netflix 高级数据工程师 Tulika Bhatt 分享了她的团队如何大规模管理印象数据。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;大规模实时交付展示次数&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/ARTHgxoJmCE?si=MDx1Qa8W7nNxkA_m"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者：&lt;/strong&gt; &lt;a href="https://www.linkedin.com/in/tulikabhatt/"&gt;Tulika Bhatt&lt;/a&gt; （Netflix 高级数据工程师）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;： Netflix 每天产生约 180 亿次展示。这些印象显着影响观看者的浏览体验，因为它们对于支持视频排名算法和计算自适应页面至关重要。随着用户界面的发展，对会话中交互的响应更加灵敏，加上对实时自适应推荐的需求不断增长，近乎实时地提供这些印象已变得非常必要。本次演讲将深入探讨 Netflix 部署的创意解决方案，以管理这种大容量、实时数据需求，同时平衡可扩展性和成本。&lt;/p&gt;&lt;h4&gt;在后 GDPR 世界中从头开始构建数据平台的思考&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/WdSsneeI6RE?si=-gpe4DprVQKoZt_V"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/jessmlarson/"&gt;Jessica Larson&lt;/a&gt; （数据工程师、《Snowflake Access Control》作者）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：在 GDPR 后的世界中创建新数据仓库的要求与 GDPR 前的世界有很大不同，例如需要优先考虑敏感数据保护和法规遵从性而不是性能和成本。在本次演讲中，Jessica Larson 分享了她在 GDPR 后构建新数据平台的收获。&lt;/p&gt;&lt;h4&gt;分拆数据仓库：独立存储的案例&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/CmEIJ-lagVU?si=Z4VcYL_FBV4bIGJW"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/jasonreid/"&gt;Jason Reid&lt;/a&gt; （Tabular 联合创始人兼产品主管）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;： 分拆数据仓库意味着将其拆分为通过开放标准接口进行交互的组成组件和模块化组件。在本次演讲中，Jason Reid 讨论了数据仓库捆绑和分拆在性能、治理和灵活性方面的优缺点，并探讨了数据仓库分拆的趋势将如何影响未来 5 年的数据工程格局。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ZPAsa_6qxkh4BJDY" /&gt;&lt;figcaption&gt;Airbnb 的分析工程师 Clark Wright 谈到了 Airbnb 数据质量评分的概念。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;数据质量评分：我们如何发展 Airbnb 的数据质量策略&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/Lv-bFDSzrqw?si=SBdnoFcOHjqe34Ve"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/clark-wright/"&gt;Clark Wright&lt;/a&gt; （Airbnb 资深分析工程师）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：最近，Airbnb 在其技术博客上发布了一篇文章，名为&lt;a href="https://medium.com/airbnb-engineering/data-quality-score-the-next-chapter-of-data-quality-at-airbnb-851dccda19c3"&gt;“数据质量评分：Airbnb 数据质量的下一章”&lt;/a&gt; 。在本次演讲中，Clark Wright 讲述了 Airbnb 的数据从业者如何认识到对更高质量数据的需求，然后提出、概念化并推出 Airbnb 的第一个数据质量评分。&lt;/p&gt;&lt;h4&gt;大规模数据生产力&lt;/h4&gt;&lt;p&gt;&lt;a href="https://youtu.be/KP5ml1tOfbY?si=hmyBjQRx422zUg-k"&gt;&lt;strong&gt;记录&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;演讲者&lt;/strong&gt;： &lt;a href="https://www.linkedin.com/in/izeigerman/"&gt;Iaroslav Zeigerman&lt;/a&gt; （Tobiko Data 联合创始人兼首席架构师）&lt;/p&gt;&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： 与软件开发相比，过时的工具阻碍了数据管道的开发和演变。创建新的开发环境非常麻烦：向其中填充数据需要大量计算，而且部署过程容易出错，导致成本更高、迭代速度更慢以及数据不可靠。 SQLMesh 是一个开源项目，诞生于我们在 Airbnb、Apple、Google 和 Netflix 等公司的集体经验，旨在处理互联网规模不断发展的数据管道的复杂性。在本次演讲中，Iaroslav Zeigerman 讨论了当今数据从业者面临的挑战以及核心 SQLMesh 概念如何解决这些挑战。&lt;/p&gt;&lt;p&gt;最后但并非最不重要的一点是，感谢数据工程开放论坛的组织者： &lt;a href="https://www.linkedin.com/in/chris-colburn/"&gt;Chris Colburn&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/xinranwaibel/"&gt;Xinran Waibel&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/jaibalani/"&gt;Jai Balani&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/rashmi-shamprasad-51630b19/"&gt;Rashmi Shamprasad&lt;/a&gt;和&lt;a href="https://www.linkedin.com/in/patriciapho/"&gt;Patricia Ho&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;直到下一次！&lt;/p&gt;&lt;blockquote&gt;如果您有兴趣参加未来的数据工程开放论坛，我们强烈建议您加入我们的&lt;a href="https://groups.google.com/g/data-engineering-open-forum"&gt;Google 群组&lt;/a&gt;，以随时关注活动公告。 &lt;/blockquote&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6b4d4410b88f" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/a-recap-of-the-data-engineering-open-forum-at-netflix-6b4d4410b88f"&gt;Netflix 数据工程开放论坛回顾&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过强调和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 20 Jun 2024 15:01:27 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/a-recap-of-the-data-engineering-open-forum-at-netflix-6b4d4410b88f?source=rss----2615bd06b42e---4</guid></item><item><title>【Video annotator: building video classifiers using vision-language models and active learning】</title><link>https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=rss----2615bd06b42e---4</link><description>&lt;h3&gt;视频注释器：使用视觉语言模型和主动学习有效构建视频分类器的框架&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/amirziai/"&gt;阿米尔·齐艾&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/aneeshvartakavi"&gt;阿尼什·瓦塔卡维&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/kelli-griggs-32990125/"&gt;凯利·格里格斯&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/eugene-lok-6465045b"&gt;尤金·洛克&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/yvonne-jukes-814ba04"&gt;伊冯娜·朱克斯&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/alejandro-alonso-ba733548"&gt;亚历克斯·阿隆索&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/vi-pallavika-iyengar-144abb1b/"&gt;维·艾扬格&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/anna-pulido-61025063"&gt;安娜·普利多&lt;/a&gt;&lt;/p&gt;&lt;a href="https://medium.com/media/02a5bbf97c619182adba24b45e42edcb/href"&gt;https://medium.com/media/02a5bbf97c619182adba24b45e42edcb/href&lt;/a&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;h4&gt;问题&lt;/h4&gt;&lt;p&gt;高质量且一致的&lt;strong&gt;注释&lt;/strong&gt;是成功开发稳健的机器学习模型的基础。训练机器学习分类器的传统技术是&lt;strong&gt;资源密集型的&lt;/strong&gt;。它们涉及一个循环，其中领域专家注释数据集，然后将其传输给数据科学家来训练模型、审查结果并进行更改。这种标记过程往往既耗时又低效，有时会在几个注释周期后停止。&lt;/p&gt;&lt;h4&gt;影响&lt;/h4&gt;&lt;p&gt;因此，与迭代复杂模型和算法方法以提高性能和修复边缘情况相比，在注释高质量数据集上投入的精力更少。因此，机器学习系统的复杂性迅速增长。&lt;/p&gt;&lt;p&gt;此外，时间和资源的限制通常会导致利用第三方注释者而不是&lt;strong&gt;领域专家&lt;/strong&gt;。这些注释者在没有深入&lt;strong&gt;了解&lt;/strong&gt;模型的预期部署或用途的情况下执行标记任务，通常会对边界或&lt;strong&gt;困难示例&lt;/strong&gt;进行一致的标记，特别是在更主观的任务中，这是一个挑战。&lt;/p&gt;&lt;p&gt;这需要与领域专家进行多轮审查，从而导致意外的成本和延误。这个漫长的周期还可能导致模型&lt;strong&gt;漂移&lt;/strong&gt;，因为修复边缘情况和部署新模型需要更长的时间，可能会损害实用性和利益相关者的信任。&lt;/p&gt;&lt;h4&gt;解决方案&lt;/h4&gt;&lt;p&gt;我们建议领域专家更直接地参与，使用&lt;strong&gt;人机交互&lt;/strong&gt;系统，可以解决许多这些实际挑战。我们引入了一种新颖的框架——&lt;strong&gt;视频注释器&lt;/strong&gt;（VA），它利用&lt;strong&gt;主动学习&lt;/strong&gt;技术和大型&lt;strong&gt;视觉语言&lt;/strong&gt;模型的&lt;strong&gt;零样本&lt;/strong&gt;功能来引导用户将精力集中在逐渐困难的示例上，从而提高模型的样本效率并保持较低的成本。&lt;/p&gt;&lt;p&gt; VA 将模型构建无缝集成到数据注释过程中，方便用户在部署之前验证模型，从而有助于建立&lt;strong&gt;信任&lt;/strong&gt;并培养&lt;strong&gt;主人翁&lt;/strong&gt;意识。 VA 还支持&lt;strong&gt;连续&lt;/strong&gt;注释过程，允许用户快速部署模型、监控其生产质量，并通过注释更多示例和部署新模型版本来快速修复任何边缘情况。&lt;/p&gt;&lt;p&gt;这种自助服务架构使用户无需数据科学家或第三方注释者的积极参与即可进行改进，从而实现快速迭代。&lt;/p&gt;&lt;h3&gt;视频理解&lt;/h3&gt;&lt;p&gt;我们设计 VA 是为了帮助&lt;a href="https://arxiv.org/abs/1904.11451"&gt;精细的视频理解&lt;/a&gt;，这需要识别视频片段中的视觉效果、概念和事件。视频理解是众多应用的基础，例如 &lt;a href="https://netflixtechblog.com/ava-discovery-view-surfacing-authentic-moments-b8cd145491cc"&gt;搜索和发现&lt;/a&gt;、&lt;a href="https://netflixtechblog.com/artwork-personalization-c589f074ad76"&gt;个性化&lt;/a&gt;以及&lt;a href="https://netflixtechblog.com/discovering-creative-insights-in-promotional-artwork-295e4d788db5"&gt;促销资产的创建&lt;/a&gt;。我们的框架允许用户通过开发一组可扩展的二进制视频分类器来有效地训练用于视频理解的机器学习模型，这些分类器支持可扩展的评分和对大量内容的检索。&lt;/p&gt;&lt;h4&gt;视频分类&lt;/h4&gt;&lt;p&gt;视频分类是为任意长度的视频片段分配标签的任务，通常伴随着概率或预测分数，如图 1 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/720/1*jXxtYzxoYFThJRs7TgEmqw.png" /&gt;&lt;figcaption&gt;图 1 - 二值视频分类器的功能视图。 &lt;a href="https://www.netflix.com/title/81130691"&gt;“大学蓝调行动：大学招生丑闻”&lt;/a&gt;中的几秒剪辑被传递到二元分类器，用于检测“建立镜头”标签。分类器输出非常高的分数（分数在 0 到 1 之间），表明该视频剪辑很可能是一个定点镜头。在电影制作中，定场镜头是建筑物或风景的广角镜头（即两个连续剪辑之间的视频剪辑），旨在确定场景的时间和地点。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;通过一组可扩展的视频分类器进行视频理解&lt;/h4&gt;&lt;p&gt;二元分类具有独立性和灵活性，使我们能够独立于其他模型添加或改进一个模型。它还具有更容易为我们的用户理解和构建的额外好处。结合多个模型的预测使我们能够更深入地了解不同粒度级别的视频内容，如图 2 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/620/1*hqNZjvWBlREV5GIv4uEmHw.png" /&gt;&lt;figcaption&gt;图 2-三个视频剪辑以及三个视频理解标签的相应二元分类器分数。请注意，这些标签并不相互排斥。视频剪辑分别来自&lt;a href="https://www.netflix.com/title/81130691"&gt;《大学蓝调行动：大学招生丑闻》&lt;/a&gt; 、 &lt;a href="https://www.netflix.com/title/81001887"&gt;《6 Underground&lt;/a&gt; 》和&lt;a href="https://www.netflix.com/title/81314956"&gt;《Leave The World Behind&lt;/a&gt; 》。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;视频注释器 (VA)&lt;/h3&gt;&lt;p&gt;在本节中，我们将描述 VA 构建视频分类器的三步过程。&lt;/p&gt;&lt;h4&gt;第 1 步 — 搜索&lt;/h4&gt;&lt;p&gt;用户首先在一个大型、多样化的语料库中找到一组初始示例来引导注释过程。我们利用文本到视频搜索来实现这一点，由视觉语言模型中的视频和文本编码器提供支持来提取嵌入。例如，致力于&lt;a href="https://en.wikipedia.org/wiki/Establishing_shot"&gt;建立镜头&lt;/a&gt;模型的注释者可以通过搜索“建筑物的广角镜头”来开始该过程，如图 3 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/468/1*NxGRRz_J5AlNDi6RZOCdMQ.png" /&gt;&lt;figcaption&gt;图 3-步骤 1 — 文本到视频搜索以引导注释过程。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;第 2 步——主动学习&lt;/h4&gt;&lt;p&gt;下一阶段涉及经典的主动学习循环。然后，VA 在视频嵌入上构建一个轻量级二元分类器，随后用于对语料库中的所有剪辑进行评分，并在提要中提供一些示例以进行进一步注释和细化，如图 4 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/471/1*pdm_-ClSOrNOnUUviKZlQA.png" /&gt;&lt;figcaption&gt;图 4-步骤 2 — 主动学习循环。注释者单击“构建”，这将启动视频语料库中所有剪辑的分类器训练和评分。评分剪辑分为四个源。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;得分最高的正面和负面提要分别显示得分最高和最低的示例。我们的用户报告说，这提供了一个有价值的指示，表明分类器是否在训练的早期阶段获取了正确的概念，并发现了训练数据中的偏差情况，并随后进行了修复。我们还提供了模型不自信的“边缘”示例。此提要有助于发现有趣的边缘案例，并激发对其他概念进行标记的需求。最后，随机源由随机选择的剪辑组成，有助于注释不同的示例，这对于泛化很重要。&lt;/p&gt;&lt;p&gt;注释者可以在任何提要中标记其他剪辑，并构建新的分类器并根据需要重复多次。&lt;/p&gt;&lt;h4&gt;第 3 步 — 回顾&lt;/h4&gt;&lt;p&gt;最后一步只是向用户展示所有带注释的剪辑。这是发现注释错误并通过步骤 1 中的搜索确定进一步注释的想法和概念的好机会。从这一步开始，用户通常会返回到步骤 1 或步骤 2 来完善其注释。&lt;/p&gt;&lt;h3&gt;实验&lt;/h3&gt;&lt;p&gt;为了评估 VA，我们请三位视频专家对包含 50 万个镜头的视频语料库中的 56 个标签进行了注释。我们将 VA 与一些基线方法的性能进行了比较，并观察到 ​​VA 可以创建更高质量的视频分类器。图 5 将 VA 的性能与基线进行了比较，作为带注释剪辑数量的函数。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/649/0*QoExXpMrIQgkHOTs" /&gt;&lt;figcaption&gt;图 5 - 模型质量（即平均精度）与“建立镜头”标签的注释剪辑数量的关系。我们观察到所有方法都优于基​​线，并且所有方法都受益于额外的注释数据，尽管程度不同。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;您可以在&lt;a href="https://arxiv.org/pdf/2402.06560"&gt;本文&lt;/a&gt;中找到有关 VA 和我们的实验的更多详细信息。&lt;/p&gt;&lt;h3&gt;结论&lt;/h3&gt;&lt;p&gt;我们提出了视频注释器 (VA)，这是一个交互式框架，它解决了与训练机器学习分类器的传统技术相关的许多挑战。 VA 利用大型视觉语言模型的零样本功能和主动学习技术来提高样本效率并降低成本。它提供了一种独特的方法来注释、管理和迭代视频分类数据集，强调领域专家直接参与人机交互系统。通过使这些用户能够在注释过程中快速对硬样本做出明智的决策，VA 提高了系统的整体效率。此外，它允许连续的注释过程，允许用户快速部署模型，监控其生产质量，并快速修复任何边缘情况。&lt;/p&gt;&lt;p&gt;这种自助服务架构使领域专家能够在没有数据科学家或第三方注释者积极参与的情况下进行改进，并培养主人翁意识，从而建立对系统的信任。&lt;/p&gt;&lt;p&gt;我们进行了实验来研究 VA 的性能，发现在各种视频理解任务中，相对于最具竞争力的基线，它的平均精度中位数提高了 8.3 个点。我们&lt;a href="https://github.com/netflix/videoannotator"&gt;发布了一个包含 56 个视频理解任务的 153k 标签的数据集&lt;/a&gt;，由三位专业视频编辑器使用 VA 进行注释，并发布了&lt;a href="https://github.com/netflix/videoannotator"&gt;代码&lt;/a&gt;来复制我们的实验。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=8ebdda0b2db4" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4"&gt;视频注释器：使用视觉语言模型和主动学习构建视频分类器&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Wed, 19 Jun 2024 15:29:29 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/video-annotator-building-video-classifiers-using-vision-language-models-and-active-learning-8ebdda0b2db4?source=rss----2615bd06b42e---4</guid></item><item><title>【Round 2: A Survey of Causal Inference Applications at Netflix】</title><link>https://netflixtechblog.com/round-2-a-survey-of-causal-inference-applications-at-netflix-fd78328ee0bb?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;在 Netflix，我们希望确保每位当前和未来的会员都能找到让他们今天兴奋并激励他们回来观看更多内容的内容。因果推理是数据科学与工程为这一使命增添的价值的重要组成部分。我们严重依赖&lt;a href="https://netflixtechblog.com/decision-making-at-netflix-33065fa06481"&gt;实验&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/quasi-experimentation-at-netflix-566b57d2e362"&gt;准实验&lt;/a&gt;来帮助我们的团队做出最佳决策，以增加会员的快乐。&lt;/p&gt;&lt;p&gt;在上次成功的&lt;a href="https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f?gi=f599816a7a8b"&gt;因果推理和实验峰会&lt;/a&gt;的基础上，我们今年又举行了为期一周的内部会议，向我们出色的同事学习。我们聚集了来自整个企业的演讲者，了解方法的发展和创新应用。&lt;/p&gt;&lt;p&gt;我们涵盖了广泛的主题，并很高兴在这篇文章中与您分享该会议的五场演讲。这将使您了解 Netflix 进行的一些因果推理研究的幕后花絮！&lt;/p&gt;&lt;h3&gt;增长 A/B 测试的指标预测&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/tendulkar"&gt;米希尔·滕杜卡&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/dhevi-rajendran-7b736b29"&gt;西蒙&lt;/a&gt;&lt;a href="https://www.linkedin.com/in/simon-ejdemyr-22b920123"&gt;·艾德米尔&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/david-hubbard-557a852"&gt;德维·拉詹德兰&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/arushi-tomar"&gt;大卫·哈伯德&lt;/a&gt;、阿鲁什·托马尔、史蒂夫&lt;a href="https://www.linkedin.com/in/steve-beckett-cfa-4384a382"&gt;·贝克特&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/jlantos?original_referer=https%3A%2F%2Fwww.google.com%2F"&gt;朱迪特&lt;/a&gt;·兰托斯、&lt;a href="https://www.linkedin.com/in/codychapmanucsd"&gt;科迪·查普曼&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/achenzion"&gt;阿亚尔·陈-锡安&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/apoorvalal"&gt;阿普尔瓦·拉尔&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/kocaguneli"&gt;埃克雷姆·科卡古内利&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/kshimada"&gt;岛田京子&lt;/a&gt;&lt;/p&gt;&lt;p&gt;实验是 Netflix 的基因。当我们推出新产品功能时，我们会尽可能使用 A/B 测试结果来估计对业务的年度增量影响。&lt;/p&gt;&lt;p&gt;从历史上看，这一估计来自我们的财务、战略和分析 (FS&amp;amp;A) 合作伙伴。对于实验中的每个测试单元，他们使用每月队列手动预测一年内的注册人数、保留概率和累积收入。该过程可能是重复且耗时的。&lt;/p&gt;&lt;p&gt;我们决定构建一种更快的自动化方法，归结为估计两条缺失数据。当我们运行 A/B 测试时，我们可能会分配一个月的用户，并仅监控两个计费周期的结果。在这个简化的示例中，我们有一个成员队列，并且有两个计费周期治疗效果（𝜏.cohort1,period1 和 𝜏.cohort1,period2，我们将分别缩短为 𝜏.1,1 和 𝜏.1,2 ）。&lt;/p&gt;&lt;p&gt;为了衡量年度影响，我们需要估计：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;未观察到的计费周期&lt;/strong&gt;。对于第一组，我们没有第三到第十二个计费周期（𝜏.1,j，其中 j = 3…12）的治疗效果 (TE)。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未被观察到的注册群体&lt;/strong&gt;。我们只观察了一个每月注册的队列，一年中还有十一个队列。我们需要知道这些群组的规模及其 TE（𝜏.i,j，其中 i = 2…12 且 j = 1…12）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;对于第一条缺失数据，我们使用了&lt;a href="https://research.netflix.com/publication/evaluating-the-surrogate-index-as-a-decision-making-tool-using-200-a-b-tests"&gt;替代索引方法&lt;/a&gt;。我们做出一个标准假设，即从治疗到结果（在本例中为收入）的因果路径经过保留的替代。我们利用我们专有的&lt;a href="https://arxiv.org/pdf/1905.03818"&gt;保留模型&lt;/a&gt;和短期观察（在上面的示例中为 𝜏.1,2）来估计 𝜏.1,j ，其中 j = 3…12。&lt;/p&gt;&lt;p&gt;对于第二条缺失数据，我们假设可转移性：每个后续队列的计费周期 TE 与第一个队列的 TE 相同。请注意，如果您有长时间运行的 A/B 测试，那么这是一个可测试的假设！ &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/823/0*e8IMSJf7p60mk-WG" /&gt;&lt;figcaption&gt;&lt;em&gt;图 1：A/B 测试中测量的每月队列活动。绿色表示整个一月的分配窗口，蓝色表示一月队列的观察窗口。由此，我们可以直接观察𝜏.1和𝜏.2，并且我们可以使用基于代理的方法向前投影𝜏.j。我们可以将值从观察到的队列传输到未观察到的队列。&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;现在，我们可以将各个部分拼凑起来。对于第一批，我们将 TE 向前推进。对于未观察到的队列，我们​​传输第一个队列中的 TE，并折叠符号以删除队列索引：𝜏.1,1 现在写为 𝜏.1。我们通过对每个队列的值求和来估计年度影响。&lt;/p&gt;&lt;p&gt;我们通过与长期运行的 AB 测试以及我们 FS&amp;amp;A 合作伙伴的先前结果进行比较，以实证方式验证了该方法的结果。现在，我们可以更快、更准确地估计我们的产品功能为会员提供的长期价值。&lt;/p&gt;&lt;h3&gt;评估游戏事件的系统框架&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/clairewilleck?original_referer=https%3A%2F%2Fwww.google.com%2F"&gt;克莱尔·威莱克&lt;/a&gt;,&lt;a href="https://www.linkedin.com/in/yimeng-tang-49566b207"&gt;唐一萌&lt;/a&gt;&lt;/p&gt;&lt;p&gt;在 Netflix Games DSE 中，实施干预后，我们会被问到许多因果推理问题。例如，产品变更如何影响游戏的性能？或者玩家获取活动如何影响关键指标？&lt;/p&gt;&lt;p&gt;虽然我们理想情况下会进行 AB 测试来衡量干预的影响，但这样做并不总是可行。在上面的第一个场景中，在干预措施启动之前并没有计划进行 A/B 测试，因此我们需要使用观察因果推理来评估其有效性。在第二种情况下，该活动是在国家一级进行的，这意味着该国的每个人都在治疗组中，这使得传统的 A/B 测试变得不可行。&lt;/p&gt;&lt;p&gt;为了评估各种游戏事件和更新的影响并帮助我们的团队扩展规模，我们围绕综合控制的变化设计了一个框架和包。&lt;/p&gt;&lt;p&gt;对于奥运会中的大多数问题，我们有比赛级别或国家级别的干预措施，但数据相对较少。这意味着大多数依赖时间序列预测、单位级数据或工具变量的现有软件包没有用处。&lt;/p&gt;&lt;p&gt;我们的框架利用了各种综合控制（SC）模型，包括增强 SC、鲁棒 SC、惩罚 SC 和综合双重差分，因为不同的方法在不同的情况下可以发挥最佳作用。我们利用无标度指标来评估每个模型的性能，并选择最小化预处理偏差的模型。此外，我们还进行稳健性测试，例如回溯，并根据控制单元的数量应用推理措施。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/772/0*5yOWKQ4AUPmwWOfP" /&gt;&lt;figcaption&gt;图 2：增强综合控制模型的示例，用于通过在训练期间拟合模型并在验证期间评估性能来减少治疗前偏差。在此示例中，增强综合控制模型比其他综合控制变量更能减少验证期间的预处理偏差。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个框架和包允许我们的团队和其他团队使用一致的方法解决一系列广泛的因果推理问题。&lt;/p&gt;&lt;h3&gt;用于权衡指标的双重机器学习&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/apoorvalal"&gt;阿普尔瓦·拉尔&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/winston-chou-6491b0168"&gt;周温斯顿&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/jjschafer"&gt;乔丹·谢弗&lt;/a&gt;&lt;/p&gt;&lt;p&gt;随着 Netflix 扩展到新的业务垂直领域，我们越来越多地看到 A/B 测试中指标权衡的例子 - 例如，游戏指标的增加可能会伴随流媒体指标的减少。为了帮助决策者应对指标不一致的情况，我们开发了一种方法，使用双机器学习（Double Machine Learning）来比较不同指标（被视为“处理”）对北极星指标（保留）的因果影响的相对重要性（ DML）。&lt;/p&gt;&lt;p&gt;在我们第一次解决这个问题时，我们发现，当处理具有不同的边际分布时，使用带有部分线性模型 (PLM) 的 DML 根据平均处理效果对处理进行排序可能会产生不正确的排序。如果治疗效果恒定且相加，则 PLM 排名&lt;em&gt;将&lt;/em&gt;是正确的。然而，当治疗效果存在异质性时，PLM 会提高治疗值最不可预测的成员的效果。这对于比较具有不同基线的治疗是有问题的。&lt;/p&gt;&lt;p&gt;相反，我们将每种治疗方法离散化，并拟合多类别倾向评分模型。这让我们可以使用增强逆倾向权重 (AIPW) 来估计多个平均治疗效果 (ATE)，以反映不同的治疗对比，例如低暴露与高暴露的效果。&lt;/p&gt;&lt;p&gt;然后，我们通过基线分布对这些治疗效果进行加权。这会根据相同总体人群的 ATE 产生“同类”的治疗排名。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/655/0*M2cujmiqIfahtPH1" /&gt;&lt;figcaption&gt;图 3：PLM 与 AIPW 在估计治疗效果方面的比较。由于当效果异质时，PLM 不会估计平均治疗效果，因此它们不会按平均治疗效果对指标进行排名，而 AIPW 则会这样做。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在上面的示例中，我们看到 PLM 将治疗 1 排在治疗 2 之上，而 AIPW 正确地按照治疗的 ATE 顺序对治疗进行排名。这是因为 PLM 会提高治疗分配更不可预测的单位的条件平均治疗效果（在本示例中，由 x = 1 定义的组），而 AIPW 则针对 ATE。&lt;/p&gt;&lt;h3&gt;具有异质无响应偏差的调查 AB 测试&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/andreasaristidou"&gt;安德烈亚斯·阿里斯蒂杜&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/carolyn-chu-263147a9"&gt;朱卡洛琳&lt;/a&gt;&lt;/p&gt;&lt;p&gt;为了提高 Netflix 调查研究的质量和覆盖范围，我们利用了一项研究研究计划，该计划利用了调查 AB 测试等工具。此类实验使我们能够直接测试和验证新想法，例如为完成调查提供激励、改变邀请的主题行、消息设计、发送时间以及许多其他内容。&lt;/p&gt;&lt;p&gt;在我们的实验计划中，我们不仅研究治疗对主要成功指标的影响，而且还研究对护栏指标的影响。我们面临的挑战是，在我们的许多测试中，干预（例如提供更高的激励）和成功指标（例如开始调查的受邀成员的百分比）位于护栏指标的上游，例如旨在测量数据的特定问题的答案质量（例如调查直线化）。&lt;/p&gt;&lt;p&gt;在这种情况下，干预可能（事实上，我们预计它会）扭曲上游指标（尤其是样本组合），而上游指标的平衡是识别下游护栏指标的必要组成部分。这是无响应偏差的结果，这是调查中常见的外部有效性问题，会影响结果的普遍性。&lt;/p&gt;&lt;p&gt;例如，如果一组成员（X 组）对我们的调查邀请的响应率明显低于另一组（Y 组），那么平均治疗效果将偏向 Y 组的行为。此外，在一项调查 AB 中在测试中，对照组和治疗组之间的无反应偏倚类型可能有所不同（例如，不同组的成员可能在测试的不同单元格中代表过多/不足），从而通过引入协变量不平衡来威胁我们测试的内部有效性。我们将这种组合称为异质无反应偏差。&lt;/p&gt;&lt;p&gt;为了克服这一识别问题并研究处理对下游指标的影响，我们结合了多种技术。首先，我们研究特定感兴趣亚群的条件平均治疗效果（CATE），其中混杂协变量在每个阶层中是平衡的。&lt;/p&gt;&lt;p&gt;为了检查平均治疗效果，我们利用倾向得分的组合来纠正内部有效性问题，并利用迭代比例拟合来纠正外部有效性问题。通过这些技术，我们可以确保我们的调查具有最高质量，并且准确地代表我们会员的意见，从而帮助我们构建他们想要看到的产品。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JD_5cvqcKV0yZYMz" /&gt;&lt;/figure&gt;&lt;h3&gt;设计：人类与技术的交叉点&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/rinachang"&gt;张丽娜&lt;/a&gt;&lt;/p&gt;&lt;p&gt;因果推理会议上的设计演讲？为什么是！由于&lt;strong&gt;设计关乎产品的工作原理&lt;/strong&gt;，因此它从根本上与 Netflix 的实验平台交织在一起。我们的产品为 Netflix 的各类内部用户提供服务，他们运行 A/B 测试并使用 A/B 测试的结果。因此，选择如何让我们的用户采取行动以及如何在产品中呈现数据对于通过实验做出决策至关重要。&lt;/p&gt;&lt;p&gt;如果您要显示一些数字和文本，您可能会选择以表格格式显示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QfNTb6td1ROVJ1PV" /&gt;&lt;/figure&gt;&lt;p&gt;虽然这种演示没有本质上的&lt;strong&gt;&lt;em&gt;错误&lt;/em&gt;&lt;/strong&gt;，但它不像更直观的东西那么容易消化。&lt;/p&gt;&lt;p&gt;如果您的目标是说明这三个数字加起来为 100%，因此它们是整体的一部分，那么您可以选择饼图。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1NYJPLPl5PnhKV6H" /&gt;&lt;/figure&gt;&lt;p&gt;如果您想展示这三个数字如何结合起来说明实现目标的进度，那么您可以选择堆积条形图。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bD3cJNsWjDpUatXX" /&gt;&lt;/figure&gt;&lt;p&gt;或者，如果您的目标是将这三个数字相互比较，那么您可以选择条形图。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6wmCdjWrfbq65PuS" /&gt;&lt;/figure&gt;&lt;p&gt;所有这些都显示相同的信息，但呈现方式的选择改变了信息图消费者理解“那又怎样？”的&lt;strong&gt;&lt;em&gt;难易程度&lt;/em&gt;&lt;/strong&gt;。你想表达的观点。请注意，这里没有“正确”的解决方案；相反，这取决于想要的收获。&lt;/p&gt;&lt;p&gt;深思熟虑的设计不仅适用于数据的静态表示，也适用于交互式体验。在此示例中，长表单中的单个项目可以通过具有预填充值来表示。&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/612/0*j0Urc5pk_zDbqYhs" /&gt;&lt;/figure&gt;&lt;p&gt;或者，可以通过以文本形式显示默认值并能够对其进行编辑来实现相同的功能。&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/613/0*pbXqoiaarsdchZZn" /&gt;&lt;/figure&gt;&lt;p&gt;虽然功能相同，但此 UI 更改将用户的叙述从“这个值正确吗？”转变为“这个值正确吗？”到“我需要做一些不‘正常’的事情吗？” ——这是一个更容易回答的问题。进一步缩小，深思熟虑的设计解决了产品级的选择，比如一个人是否知道去哪里完成一项任务。一般来说，深思熟虑的设计会影响产品策略。&lt;/p&gt;&lt;p&gt;设计渗透到 Netflix 实验产品的各个方面，从颜色等小选择到路线图等战略选择。通过深思熟虑的设计，我们可以确保工具帮助团队从我们的实验中学到最多的东西。&lt;/p&gt;&lt;h3&gt;外部演讲者：今井浩介&lt;/h3&gt;&lt;p&gt;除了 Netflix 员工的精彩演讲之外，我们还有幸聆听了哈佛大学政府与统计学教授&lt;a href="https://imai.fas.harvard.edu/"&gt;Kosuke Imai 的&lt;/a&gt;主题演讲。他介绍了“&lt;a href="https://arxiv.org/abs/2403.07031"&gt;补习法&lt;/a&gt;”，这是一种使用通用机器学习算法学习和评估治疗政策的强大而有效的方法。&lt;/p&gt;&lt;p&gt;衡量因果关系是 Netflix 数据科学文化的重要组成部分，我们很自豪拥有许多出色的同事，他们利用实验和准实验来提高会员影响力。这次会议是庆祝彼此工作并强调因果方法论为业务创造价值的方式的好方法。&lt;/p&gt;&lt;p&gt;要了解我们的最新工作信息，请关注&lt;a href="https://netflixtechblog.com/"&gt;Netflix 技术博客&lt;/a&gt;，如果您有兴趣加入我们，我们目前正在寻找&lt;a href="https://jobs.netflix.com/search?q=data%20science&amp;amp;team=Data%20Science%20and%20Engineering"&gt;新的出色同事&lt;/a&gt;来帮助我们娱乐世界！ &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=fd78328ee0bb" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/round-2-a-survey-of-causal-inference-applications-at-netflix-fd78328ee0bb"&gt;第 2 轮：Netflix 因果推理应用调查&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过强调和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 06 Jun 2024 20:10:54 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/round-2-a-survey-of-causal-inference-applications-at-netflix-fd78328ee0bb?source=rss----2615bd06b42e---4</guid></item><item><title>【The Making of VES: the Cosmos Microservice for Netflix Video Encoding】</title><link>https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/liwei-guo/"&gt;郭立伟&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/carvalhovinicius/"&gt;Vinicius Carvalho&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/anush-moorthy-b8451142/"&gt;Anush Moorthy&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/aditya-mavlankar-7139791/"&gt;Aditya Mavlankar&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/lishan-z-51302abb/"&gt;朱立山&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;这是 Netflix 多部分系列的第二篇文章。请参阅&lt;/em&gt;&lt;a href="https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359"&gt;&lt;em&gt;此处&lt;/em&gt;&lt;/a&gt;&lt;em&gt;的第 1 部分，其中概述了我们使用微服务重建 Netflix 视频处理管道的努力。本博客深入探讨了构建视频编码服务 (VES) 的详细信息，并分享了我们的经验教训。&lt;/em&gt;&lt;/p&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad"&gt;Cosmos&lt;/a&gt;是 Netflix 的下一代媒体计算平台。 Cosmos 将微服务架构与异步工作流程和无服务器功能相结合，旨在实现 Netflix 媒体处理管道的现代化，提高灵活性、效率和开发人员生产力。在过去的几年里，Encoding Technologies (ET) 的视频团队一直致力于在 Cosmos 上重建整个视频管道。&lt;/p&gt;&lt;p&gt;这个新管道由许多微服务组成，每个微服务专用于单一功能。其中一种微服务是视频编码服务 (VES)。编码是视频管道的重要组成部分。在较高级别上，它采用摄取的夹层并将其编码为适合 Netflix 流或服务于某些工作室/制作用例的视频流。就 Netflix 而言，这项服务有许多要求：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;鉴于从手机到浏览器再到智能电视的广泛设备，需要支持多种编解码器格式、分辨率和质量级别。&lt;/li&gt;&lt;li&gt;分块编码是满足我们业务需求的延迟要求的必要条件，并且需要适应具有不同延迟敏感度级别的用例。&lt;/li&gt;&lt;li&gt;持续发布的能力对于在流媒体和工作室领域实现快速产品创新至关重要。&lt;/li&gt;&lt;li&gt;每天都有大量的编码工作。该服务需要具有成本效益并充分利用可用资源。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在这篇技术博客中，我们将介绍如何构建 VES 来实现上述目标，并将分享我们从构建微服务中学到的一些经验教训。请注意，为了简单起见，我们选择省略某些特定于 Netflix 的细节，这些细节不属于本博文的主要信息。&lt;/p&gt;&lt;h3&gt;在 Cosmos 上构建视频编码服务&lt;/h3&gt;&lt;p&gt;Cosmos 微服务由三层组成：接收请求的 API 层 (Optimus)、编排媒体处理流的工作流层 (Plato) 以及处理媒体的无服务器计算层 (Stratum)。这三层通过名为&lt;a href="https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f"&gt;Timestone&lt;/a&gt;的本地开发的基于优先级的消息系统进行异步通信。我们选择Protobuf作为payload格式，是因为它的高效性和成熟的跨平台支持。&lt;/p&gt;&lt;p&gt;为了帮助服务开发人员抢占先机，Cosmos 平台提供了强大的服务生成器。该生成器具有直观的用户界面。只需点击几下，它就创建了一个基本但完整的 Cosmos 服务：创建了所有 3 层的代码存储库；启用所有平台功能，包括发现、日志记录、跟踪等；设置发布管道并且可以轻松访问仪表板。我们可以立即开始添加视频编码逻辑并将服务部署到云端进行实验。&lt;/p&gt;&lt;h4&gt;擎天柱&lt;/h4&gt;&lt;p&gt;作为 API 层，Optimus 充当 VES 的网关，这意味着服务用户只能通过 Optimus 与 VES 交互。定义的API接口是VES与外部世界之间的强有力的契约。只要 API 稳定，用户就不会受到 VES 内部更改的影响。这种解耦有助于实现 VES 内部结构的更快迭代。&lt;/p&gt;&lt;p&gt;作为单一用途的服务，VES 的 API 非常干净。我们定义了一个端点&lt;em&gt;encodeVideo&lt;/em&gt; ，它接受&lt;em&gt;EncodeRequest&lt;/em&gt;并返回&lt;em&gt;EncodeResponse&lt;/em&gt; （通过Timestone消息以异步方式）。 &lt;em&gt;EncodeRequest&lt;/em&gt;对象包含有关源视频以及编码配方的信息。编码视频的所有要求（编解码器、分辨率等）以及延迟控制（分块指令）都通过编码配方的数据模型公开。&lt;/p&gt;&lt;pre&gt; //protobuf定义&lt;br /&gt;&lt;br /&gt;消息编码请求{&lt;br /&gt; VideoSource video_source = 1;//待编码源&lt;br /&gt; 菜谱配方=2； //包括编码格式、分辨率等&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;消息编码响应{&lt;br /&gt; 输出视频output_video = 1; //编码后的视频&lt;br /&gt; 误差误差=2； //错误信息（可选）&lt;br /&gt; }&lt;br /&gt;&lt;br /&gt;消息食谱{&lt;br /&gt; 编解码器编解码器=1； //包括编解码格式、配置文件、级别等&lt;br /&gt; 分辨率分辨率=2；&lt;br /&gt; 分块指令 chunking_directives = 3;&lt;br /&gt; ...&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;与任何其他 Cosmos 服务一样，该平台会根据 VES API 数据模型自动生成 RPC 客户端，用户可以使用该客户端构建请求并调用 VES。收到传入请求后，Optimus 会执行验证，并（如果适用）将传入数据转换为内部数据模型，然后再将其传递到下一层 Plato。&lt;/p&gt;&lt;h3&gt;柏拉图&lt;/h3&gt;&lt;p&gt;工作流层 Plato 控制媒体处理步骤。 Cosmos 平台支持 Plato 的两种编程范例：前向链接规则引擎和有向无环图（DAG）。 VES 具有线性工作流程，因此我们选择 DAG 是因为它简单。&lt;/p&gt;&lt;p&gt;在 DAG 中，工作流由节点和边表示。节点代表工作流中的阶段，而边表示依赖关系——只有当所有依赖关系都完成时，阶段才准备好执行。 VES 需要对视频块进行并行编码，以满足其延迟和弹性目标。 DAG 通过 MapReduce 模式促进了这种工作流级并行性。可以通过对节点进行注释来指示这种关系，并且只有当所有关联的 Map 节点都准备就绪时，才会触发 Reduce 节点。&lt;/p&gt;&lt;p&gt;对于 VES 工作流程，我们定义了五个节点及其关联的边，如下图所示：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分割器节点：该节点根据配方中的分块指令将视频分成块。&lt;/li&gt;&lt;li&gt;编码器节点：该节点对视频块进行编码。它是一个Map节点。&lt;/li&gt;&lt;li&gt;汇编器节点：该节点将编码块拼接在一起。它是一个Reduce节点。&lt;/li&gt;&lt;li&gt;验证器节点：该节点执行编码视频的验证。&lt;/li&gt;&lt;li&gt;通知节点：整个工作流程完成后，该节点通知 API 层。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7n3hN3lYhe89Ezk6" /&gt;&lt;/figure&gt;&lt;p&gt;在此工作流程中，Notifier 等节点执行非常轻量级的操作，可以直接在 Plato 运行时中执行。然而，资源密集型操作需要委托给计算层（Stratum）或其他服务。 Plato 调用 Stratum 函数来执行编码和组装等任务，其中节点（编码器和组装器）将消息发布到相应的消息队列。验证器节点调用另一个 Cosmos 服务（视频验证服务）来验证组装的编码视频。&lt;/p&gt;&lt;h4&gt;地层&lt;/h4&gt;&lt;p&gt;计算层 Stratum 是可以访问媒体样本的地方。 Cosmos 服务的开发人员创建 Stratum Functions 来处理媒体。他们可以自带媒体处理工具，这些工具被打包到 Functions 的 Docker 镜像中。然后，这些 Docker 镜像会发布到我们的内部 Docker 注册表（ &lt;a href="https://medium.com/p/f868c9fb5436"&gt;Titus&lt;/a&gt;的一部分）。在生产中，Titus 会根据作业队列的深度自动扩展实例。&lt;/p&gt;&lt;p&gt; VES 需要支持将源视频编码为各种编解码器格式，包括 AVC、AV1 和 VP9 等。我们针对不同的编解码器格式使用不同的编码器二进制文件（简称为“编码器”）。对于AVC这种已有20年历史的格式来说，编码器相当稳定。另一方面， &lt;a href="https://netflixtechblog.com/bringing-av1-streaming-to-netflix-members-tvs-b7fc88e42320"&gt;Netflix 流媒体的最新成员&lt;/a&gt;AV1 正在不断进行积极的改进和实验，因此需要更频繁的编码器升级。为了有效管理这种可变性，我们决定创建多个 Stratum Functions，每个 Stratum Functions 专用于特定的编解码器格式，并且可以独立发布。这种方法可确保升级一个编码器不会影响其他编解码器格式的 VES 服务，从而保持全面的稳定性和性能。&lt;/p&gt;&lt;p&gt;在 Stratum Function 中，Cosmos 平台提供了常见媒体访问模式的抽象。无论文件格式如何，源都统一呈现为本地安装的框架。同样，对于需要保存在云端的输出，平台将过程呈现为写入本地文件。所有细节（例如字节流和错误重试）都被抽象掉。由于平台考虑了基础设施的复杂性，Stratum Function 中视频编码的基本代码可以简单如下。&lt;/p&gt;&lt;pre&gt; ffmpeg -i 输入/源%08d.j2k -vf ... -c:v libx264 ... 输出/encoding.264&lt;/pre&gt;&lt;p&gt;编码是一个资源密集型过程，所需资源与编解码格式和编码配方密切相关。我们进行了基准测试，以了解不同编码方案的资源使用模式，特别是 CPU 和 RAM。根据结果​​，我们利用了 Cosmos 平台的“容器整形”功能。&lt;/p&gt;&lt;p&gt;我们定义了许多不同的“容器形状”，指定了 CPU 和 RAM 等资源的分配。&lt;/p&gt;&lt;pre&gt; # 容器形状的定义示例&lt;br /&gt;组：containerShapeExample1&lt;br /&gt;资源：&lt;br /&gt; 中央处理器数量：2&lt;br /&gt; 内存（MB）：4000&lt;br /&gt; 网络 Mbp: 750&lt;br /&gt; 磁盘大小（MB）：12000&lt;/pre&gt;&lt;p&gt;创建路由规则以根据编解码器格式和编码分辨率的组合将编码作业分配给不同的形状。这有助于平台执行“装箱”，从而最大限度地提高资源利用率。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/425/1*UZ7G9SlWnqLXd4umutzhcQ.png" /&gt;&lt;figcaption&gt; “装箱”的一个例子。圆圈代表 CPU 核心，区域代表 RAM。此 16 核 EC2 实例包含 5 个 3 种不同形状（用不同颜色表示）的编码容器（矩形）。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;持续发布&lt;/h3&gt;&lt;p&gt;在我们完成所有三层的开发和测试后，VES 投入生产。然而，这并不标志着我们工作的结束。恰恰相反，我们相信并且仍然相信服务价值的很大一部分是通过迭代实现的：支持新的业务需求、增强性能和提高弹性。我们愿景的一个重要部分是让 Cosmos 服务能够以安全的方式持续将代码更改发布到生产中。&lt;/p&gt;&lt;p&gt;专注于单一功能，与 VES 中添加的单一功能相关的代码更改通常较小且具有凝聚力，因此易于审查。由于调用者只能通过其 API 与 VES 交互，因此内部代码是真正可以安全更改的“实现细节”。显式的 API 契约限制了 VES 的测试面。此外，Cosmos 平台还提供了基于&lt;a href="https://martinfowler.com/articles/practical-test-pyramid.html"&gt;金字塔&lt;/a&gt;的测试框架，指导开发人员创建不同级别的测试。&lt;/p&gt;&lt;p&gt;经过测试和代码审查后，更改将被合并并准备发布。发布管道是完全自动化的：合并后，管道会检查代码、编译、构建、按规定运行单元/集成/端到端测试，如果没有遇到问题，则继续进行完整部署。通常，从代码合并到功能落地大约需要 30 分钟（在我们的上一代平台中，这个过程需要 2-4 周！）。较短的发布周期为开发人员提供了更快的反馈，并帮助他们在上下文仍然新鲜的情况下进行必要的更新。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1020/1*fgKHQg4IZVMkWQlHGaCT2Q.png" /&gt;&lt;figcaption&gt;在我们的生产环境中运行的发布管道的屏幕截图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在生产环境中运行时，该服务会不断发出指标和日志。平台收集它们以可视化仪表板并驱动监控/警报系统。与基线偏差太大的指标将触发警报，并可能导致服务自动回滚（启用“金丝雀”功能时）。&lt;/p&gt;&lt;h3&gt;学到的东西：&lt;/h3&gt;&lt;p&gt; VES 是我们团队构建的第一个微服务。我们从微服务的基本知识开始，并在此过程中学到了很多经验教训。这些知识加深了我们对微服务的理解，并帮助我们改进了设计选择和决策。&lt;/p&gt;&lt;h4&gt;定义适当的服务范围&lt;/h4&gt;&lt;p&gt;微服务架构的一个原则是应该为单一功能构建服务。这听起来很简单，但是什么才算是“单一功能”呢？ “编码视频”听起来不错，但“将视频编码为AVC格式”难道不是一个更具体的单一功能吗？&lt;/p&gt;&lt;p&gt;当我们开始构建 VES 时，我们采用了为每种编解码器格式创建单独的编码服务的方法。虽然这具有解耦工作流程等优点，但很快我们就被开发开销淹没了。想象一下，用户请求我们在编码中添加水印功能。我们需要对多个微服务进行更改。更糟糕的是，所有这些服务的变化都非常相似，本质上我们一次又一次地添加相同的代码（和测试）。这种重复性的工作很容易让开发人员疲惫不堪。&lt;/p&gt;&lt;p&gt;本博客中介绍的服务是我们的 VES 的第二次迭代（是的，我们已经经历了一次迭代）。在此版本中，我们将不同编解码器格式的编码合并到单个服务中。它们共享相同的 API 和工作流程，而每种编解码器格式都有自己的层函数。到目前为止，这似乎取得了很好的平衡：公共 API 和工作流程减少了代码重复，而单独的 Stratum Functions 则保证了每种编解码器格式的独立演进。&lt;/p&gt;&lt;p&gt;我们所做的改变并非不可逆转。如果将来的某一天，一种特定编解码器格式的编码演变成完全不同的工作流程，我们可以选择将其分离到自己的微服务中。&lt;/p&gt;&lt;h4&gt;对数据建模要务实&lt;/h4&gt;&lt;p&gt;一开始，我们对数据模型分离非常严格——我们坚信共享就等于耦合，而耦合可能会导致未来潜在的灾难。为了避免这种情况，对于每个服务以及服务中的三个层，我们定义了自己的数据模型并构建了转换器来在不同的数据模型之间进行转换。&lt;/p&gt;&lt;p&gt;我们最终为系统中的位深度和分辨率等方面创建了多个数据模型。公平地说，这确实有一些优点。例如，我们的编码管道支持 AVC 编码（8 位）和 AV1 编码（10 位）的不同位深度。通过定义&lt;em&gt;AVC.BitDepth&lt;/em&gt;和&lt;em&gt;AV1.BitDepth&lt;/em&gt; ，可以将位深度的约束构建到数据模型中。然而，这种差异化能力的好处是否大于缺点（即多个数据模型转换）是有争议的。&lt;/p&gt;&lt;p&gt;最终，我们创建了一个库来托管视频领域常见概念的数据模型。此类概念的示例包括帧速率、扫描类型、色彩空间等。如您所见，它们非常常见且稳定。这个“通用”数据模型库在视频团队拥有的所有服务之间共享，避免了不必要的重复和数据转换。在每个服务中，为特定于服务的对象定义了附加数据模型。&lt;/p&gt;&lt;h4&gt;拥抱服务 API 变更&lt;/h4&gt;&lt;p&gt;这听起来可能很矛盾。我们一直在说 API 是服务与其用户之间的强有力的契约，保持 API 的稳定可以保护用户免受内部变化的影响。这是绝对正确的。然而，当我们设计服务 API 的第一个版本时，我们谁都没有水晶球。不可避免的是，在某个时刻，这个 API 会变得不够用。如果我们过于坚信“API 不能改变”，那么开发人员将被迫寻找变通办法，而这几乎肯定不是最佳的。&lt;/p&gt;&lt;p&gt;有很多关于优雅地发展 API 的优秀技术文章。我们相信我们还拥有独特的优势：VES 是 Netflix Encoding Technologies (ET) 的一项内部服务。我们的两个用户（Streaming Workflow Orchestrator 和 Studio Workflow Orchestrator）由 ET 内的工作流团队拥有。我们的团队拥有相同的背景并为共同的目标而努力。如果我们认为更新 API 符合 Netflix 的最佳利益，我们就会与他们会面以寻求一致。一旦达成更新 API 的共识，团队就会协作以确保平稳过渡。&lt;/p&gt;&lt;h3&gt;敬请关注…&lt;/h3&gt;&lt;p&gt;这是我们的技术博客系列使用微服务重建 Netflix 视频管道的第二部分。在这篇文章中，我们详细描述了视频编码服务（VES）的构建过程以及我们的经验教训。我们的管道包括我们也计划分享的一些其他服务。请继续关注我们未来关于微服务主题的博客！ &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=946b9b3cd300" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300"&gt;VES 的制作：Netflix 视频编码的 Cosmos 微服务&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Tue, 09 Apr 2024 22:12:32 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/the-making-of-ves-the-cosmos-microservice-for-netflix-video-encoding-946b9b3cd300?source=rss----2615bd06b42e---4</guid></item><item><title>【Reverse Searching Netflix’s Federated Graph】</title><link>https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;作者：&lt;a href="https://www.linkedin.com/in/rickygardiner/"&gt;瑞奇·加德纳&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/ahutter/"&gt;亚历克斯·哈特&lt;/a&gt;和&lt;a href="https://www.linkedin.com/in/katielefevre/"&gt;凯蒂·勒费夫尔&lt;/a&gt;&lt;/p&gt;&lt;p&gt;自从我们之前关于内容工程在 Netflix 联合图内启用搜索功能的作用的文章（ &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf"&gt;第一篇文章&lt;/a&gt;，我们识别问题并详细说明索引架构， &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c"&gt;第二篇文章&lt;/a&gt;，我们详细介绍如何促进查询）以来，已经发生了重大的变化。事态发展。我们已将 Studio Search 开放给内容工程之外的 Netflix 整个工程部门，并将其重命名为 Graph Search。有超过 100 个应用程序与图搜索集成，我们支持近 50 个索引。我们继续向该服务添加功能。正如上一篇文章中所承诺的，我们将分享我们如何与 Studio 工程团队合作构建反向搜索。反向搜索反转了标准查询模式：它不是查找与查询匹配的文档，而是查找与文档匹配的查询。&lt;/p&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;p&gt;蒂芙尼 (Tiffany) 是 Netflix 的后期制作协调员，负责监督近十几部处于不同预制作、制作和后期制作状态的电影。蒂芙尼和她的团队与各种跨职能合作伙伴（包括法律、创意和影片发行管理）合作，跟踪她的电影的进展和健康状况。&lt;/p&gt;&lt;p&gt;因此，蒂芙尼订阅了特定于某些关注领域的通知和日历更新，例如“在墨西哥城拍摄但没有分配关键角色的电影”，或“有可能在发布日期前准备就绪的电影”。&lt;/p&gt;&lt;p&gt; &lt;strong&gt;&lt;em&gt;Tiffany 不是订阅特定电影的更新，而是订阅返回电影动态子集的查询。&lt;/em&gt;这给我们这些负责向她发送这些通知的人带来了一个问题。当电影发生变化时，我们不知道该通知谁，因为员工和他们感兴趣的电影之间没有关联。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;我们可以保存这些搜索，然后重复查询每个搜索的结果，但由于我们是大型联合图的一部分，这会对我们连接到的每个服务产生大量流量影响。我们必须决定是否需要及时通知或减少图表上的负载。&lt;/p&gt;&lt;p&gt;如果我们能够回答“此查询是否会返回这部电影”的问题，我们就可以根据变化事件以激光精度重新查询，而不会影响更广泛的生态系统。&lt;/p&gt;&lt;h3&gt;解决方案&lt;/h3&gt;&lt;p&gt;图搜索建立在 Elasticsearch 之上，它具有我们所需的确切功能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可用于索引 Elasticsearch 查询的&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/percolator.html#:~:text=The%20percolator%20field%20type%20parses,to%20be%20a%20percolator%20field."&gt;渗透器字段&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html"&gt;渗透查询&lt;/a&gt;可用于确定哪些索引查询与输入文档匹配。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GCZRoNqT8seObcUFzYthXg.png" /&gt;&lt;/figure&gt;&lt;p&gt;渗透查询不是进行搜索（例如“在墨西哥城拍摄的西班牙语电影”）并返回匹配的文档（一个用于 Roma，一个用于 Familia），而是采用一个文档（一个用于 Roma）并返回匹配的搜索。匹配该文档，例如“西班牙语电影”和“剧本戏剧”。&lt;/p&gt;&lt;p&gt;我们将此功能称为保存搜索的功能，称为 SavedSearches，它是现有索引上的持久过滤器。&lt;/p&gt;&lt;pre&gt;输入已保存的搜索{&lt;br /&gt; 身份证：身份证！&lt;br /&gt; 过滤器：字符串&lt;br /&gt; 索引：搜索索引！&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;该过滤器以 Graph Search DSL 编写，被转换为 Elasticsearch 查询并在 percolator 字段中建立索引。要了解有关图搜索 DSL 的更多信息以及我们创建它而不是直接使用 Elasticsearch 查询语言的原因， &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-part-2-49348511c06c"&gt;请参阅“Netflix 内容工程如何使联合图可搜索（第 2 部分）”的查询语言部分&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们将查找匹配的已保存搜索的过程称为反向搜索。这是该产品中最直接的部分。我们向&lt;a href="https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2"&gt;域图服务&lt;/a&gt;(DGS) 添加了一个新的解析器以进行图搜索。它获取感兴趣的索引和文档，并通过发出渗透查询返回与文档匹配的所有已保存搜索。&lt;/p&gt;&lt;pre&gt; ”“”&lt;br /&gt;用于检索给定索引中所有注册的已保存搜索的查询，&lt;br /&gt;根据提供的文件。本例中的文档是 ElasticSearch&lt;br /&gt;根据索引的配置生成的文档。&lt;br /&gt; ”“”&lt;br /&gt;反向搜索(&lt;br /&gt; 之后：字符串，&lt;br /&gt; 文档：JSON！，&lt;br /&gt; 第一：Int！，&lt;br /&gt; 索引：SearchIndex！）：SavedSearchConnection&lt;/pre&gt;&lt;p&gt;持久保存 SavedSearch 是作为图搜索 DGS 上的新突变实现的。这最终会触发 Percolator 字段中 Elasticsearch 查询的索引。&lt;/p&gt;&lt;pre&gt; ”“”&lt;br /&gt;用于注册和更新已保存搜索的突变。他们需要更新&lt;br /&gt;任何时候用户调整他们的搜索条件。&lt;br /&gt; ”“”&lt;br /&gt; upsertSavedSearch（输入：UpsertSavedSearchInput！）：UpsertSavedSearchPayload&lt;/pre&gt;&lt;p&gt;支持渗透器字段从根本上改变了我们为图搜索提供索引管道的方式（ &lt;a href="https://netflixtechblog.com/how-netflix-content-engineering-makes-a-federated-graph-searchable-5c0c1c7d7eaf"&gt;请参阅 Netflix 内容工程如何使联合图可搜索的架构部分）&lt;/a&gt; 。我们现在有两个索引管道，而不是每个图形搜索索引都有一个索引管道：一个用于索引文档，另一个用于将保存的搜索索引到渗透索引。我们选择将渗透器字段添加到单独的索引中，以便分别调整两种类型查询的性能。&lt;/p&gt;&lt;p&gt; Elasticsearch 要求渗透索引具有与其存储的查询结构相匹配的映射，因此必须与文档索引的映射相匹配。索引模板定义创建新索引时应用的​​映射。通过使用索引模板的&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates-v1.html#put-index-template-v1-api-request-body"&gt;index_patterns&lt;/a&gt;功能，我们能够在两者之间共享文档索引的映射。 index_patterns 还为我们提供了一种简单的方法来将渗透器字段添加到我们创建的每个渗透索引中。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;文档索引映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;索引模式 — application_*&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “订单”：1，&lt;br /&gt; “index_patterns”：[“application_*”]，&lt;br /&gt; “映射”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “电影标题”： {&lt;br /&gt; “类型”：“关键字”&lt;br /&gt; },&lt;br /&gt; “已存档”：{&lt;br /&gt; “类型”：“布尔值”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;渗透索引映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;索引模式 — *_percolate&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “订单”：2，&lt;br /&gt; “index_patterns”：[“*_percolate*”]，&lt;br /&gt; “映射”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “渗透查询”：{&lt;br /&gt; “类型”：“渗滤器”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;生成的映射示例&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Percolate 索引名称为 application_v1_percolate&lt;/strong&gt;&lt;/p&gt;&lt;pre&gt; {&lt;br /&gt; “application_v1_percolate”：{&lt;br /&gt; “映射”：{&lt;br /&gt; “_doc”：{&lt;br /&gt; “特性”： {&lt;br /&gt; “电影标题”： {&lt;br /&gt; “类型”：“关键字”&lt;br /&gt; },&lt;br /&gt; “已存档”：{&lt;br /&gt; “类型”：“布尔值”&lt;br /&gt; },&lt;br /&gt; “渗透查询”：{&lt;br /&gt; “类型”：“渗滤器”&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;br /&gt; }&lt;/pre&gt;&lt;h3&gt;渗透索引管道&lt;/h3&gt;&lt;p&gt;渗透索引并不像从 GraphQL 突变获取输入、将其转换为 Elasticsearch 查询并为其建立索引那么简单。我们稍后将详细讨论版本控制，它露出了丑陋的一面，并使事情变得更加复杂。这是渗透索引管道的设置方式。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KSZuvPeOxDOKNrPiNnNvFg.png" /&gt;&lt;figcaption&gt;&lt;em&gt;请参阅&lt;/em&gt;&lt;a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873"&gt;Netflix 的数据网格 — 数据移动和处理平台，&lt;/a&gt;了解有关数据网格的更多信息。&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;当 SavedSearches 被修改时，我们将它们存储在 CockroachDB 中，并且 Cockroach 数据库的源连接器会发出 CDC 事件。&lt;/li&gt;&lt;li&gt;共享一个表来存储所有 SavedSearches，因此下一步是使用过滤处理器过滤到仅用于 *this* 索引的表。&lt;/li&gt;&lt;li&gt;如前所述，数据库中存储的是我们自定义的图形搜索过滤器 DSL，它与 Elasticsearch DSL 不同，因此我们无法直接将事件索引到渗透索引。相反，我们对图搜索 DGS 进行了修改。图形搜索 DGS 将 DSL 转换为 Elasticsearch 查询。&lt;/li&gt;&lt;li&gt;然后，我们将 Elasticsearch 查询作为适当渗透索引中的渗透字段进行索引。&lt;/li&gt;&lt;li&gt;返回 SavedSearch 索引成功或失败。失败时，SavedSearch 事件将发送到死信队列 (DLQ)，该队列可用于解决任何故障，例如从索引中删除搜索查询中引用的字段。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;现在稍微介绍一下版本控制，以解释为什么上述内容是必要的。想象一下，我们已经开始为有动物的电影添加标签。如果我们希望用户能够创建“动物电影”的视图，我们需要将这个新字段添加到现有搜索索引中以标记电影。但是，当前索引中的映射不包含它，因此我们无法对其进行过滤。为了解决这个问题，我们有索引版本。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AnB07zkL_g4a30TUgAHGSA.jpeg" /&gt;&lt;figcaption&gt; &lt;a href="https://www.netflix.com/title/81730862"&gt;《动物宝宝》&lt;/a&gt;系列中的达莉亚和福雷斯特&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当索引定义发生更改而需要新映射时（例如添加动物标签时），图形搜索会创建新版本的 Elasticsearch 索引和新的管道来填充它。这个新的管道从数据网格中的日志压缩的 Kafka 主题中读取数据——这就是我们如何重新索引整个语料库，而无需要求数据源重新发送所有旧事件。新管道和旧管道并行运行，直到新管道处理完积压的工作，此时图搜索会切换到使用 Elasticsearch 索引别名的版本。&lt;/p&gt;&lt;p&gt;为我们的文档创建新索引意味着我们还需要为查询创建新的渗透索引，以便它们可以具有一致的索引映射。当我们更改版本时，这个新的渗透索引也需要回填。这就是管道如此工作的原因——当我们启动新的渗透索引管道时，我们可以再次利用数据网格中的日志压缩主题来重新索引 SavedSearches 语料库。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*bAxTCeTOeZ_g4ueiN0cLiQ.png" /&gt;&lt;figcaption&gt;&lt;em&gt;我们将用户提供的过滤器 DSL 持久保存到数据库中，而不是立即将其转换为 Elasticsearch 查询语言。这使我们能够在将保存的搜索 DSL 转换为 Elasticsearch 查询时进行更改或修复。我们可以通过创建新版本的索引来部署这些更改，因为引导过程将重新翻译每个保存的搜索。&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;另一个用例&lt;/h3&gt;&lt;p&gt;我们希望反向搜索功能最终对其他工程团队有用。我们几乎立即就收到了反向搜索可以解决的问题。&lt;/p&gt;&lt;p&gt;根据电影的类型，制作电影的方式可能会非常不同。一部电影可能会经历一组不适用于另一部电影的阶段，或者可能需要安排另一部电影不需要的某些事件。我们应该能够定义电影分类的方法，并使用它来自动将它们分配给工作流程，而不是根据电影的分类手动配置电影的工作流程。但确定电影的分类具有挑战性：您可以仅根据类型来定义这些电影分类，例如“动作”或“喜剧”，但您可能需要更复杂的定义。也许它是由流派、地区、格式、语言或其某种微妙的组合来定义的。电影匹配服务提供了一种根据匹配标准的任意组合对电影进行分类的方法。在幕后，匹配标准存储为反向搜索，并且为了确定电影与哪个标准匹配，电影的文档被提交到反向搜索端点。&lt;/p&gt;&lt;p&gt;简而言之，反向搜索正在为外部化标准匹配器提供动力。它现在用于电影标准，但由于每个图形搜索索引现在都具有反向搜索功能，因此任何索引都可以使用此模式。&lt;/p&gt;&lt;h3&gt;可能的未来：订阅&lt;/h3&gt;&lt;p&gt;反向搜索看起来也像是创建响应速度更快的 UI 的有前景的基础。搜索结果可以通过 GraphQL 订阅提供，而不是作为查询获取一次结果。这些订阅可以与 SavedSearch 相关联，并且当索引发生变化时，可以使用反向搜索来确定何时更新订阅返回的键集。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=222ac5d23576" width="1" /&gt;&lt;hr /&gt;&lt;p&gt;&lt;a href="https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576"&gt;反向搜索 Netflix 的 Federated Graph&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix TechBlog&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 04 Apr 2024 21:26:42 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/reverse-searching-netflixs-federated-graph-222ac5d23576?source=rss----2615bd06b42e---4</guid></item><item><title>【Sequential Testing Keeps the World Streaming Netflix Part 2: Counting Processes】</title><link>https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642?source=rss----2615bd06b42e---4</link><description>&lt;h3&gt;&lt;strong&gt;顺序 A/B 测试让世界保持流媒体 Netflix 第 2 部分：计算进程&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.linkedin.com/in/michaelslindon/"&gt;迈克尔·林登&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/csanden/"&gt;克里斯·桑登&lt;/a&gt;、 &lt;a href="https://www.linkedin.com/in/vshirikian/"&gt;Vache Shirikian&lt;/a&gt; 、&lt;a href="https://www.linkedin.com/in/liuyanjun/"&gt;刘彦军&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/minalmishra/"&gt;米纳尔·米什拉&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/martintingley/"&gt;马丁·廷利&lt;/a&gt;&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7DNyGA0x7r7msS7w1Zpvpw.jpeg" /&gt;&lt;/figure&gt;&lt;p&gt;您在流式传输 Netflix 时遇到过错误吗？您的标题是否意外停止，或者根本没有开始？在有关顺序测试的博客系列的第一部分中，我们描述了&lt;a href="https://medium.com/p/cba6c7ed49df"&gt;用于连续指标（例如&lt;em&gt;播放延迟）&lt;/em&gt;的金丝雀测试方法&lt;/a&gt;。我们的一位读者评论道&lt;/p&gt;&lt;blockquote&gt;如果新版本与新的播放/流媒体功能无关怎么办？例如，如果新版本包含修改后的登录功能怎么办？您还会监控“播放延迟”指标吗？&lt;/blockquote&gt;&lt;p&gt; Netflix 监控大量指标，其中许多指标可以归类为计数。其中包括登录次数、错误次数、游戏成功开始次数，甚至客户呼叫中心联系次数等指标。在第二部分中，我们描述了用于测试计数指标的顺序方法，该方法在 NeurIPS 论文&lt;a href="https://openreview.net/forum?id=a4zg0jiuVi"&gt;&lt;em&gt;《多项计数数据的随时有效推理》&lt;/em&gt;&lt;/a&gt;中概述。&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;指出不同&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;假设我们要部署改变登录行为的新代码。为了降低软件推出的风险，我们对新代码进行 A/B 测试，也称为金丝雀测试。每当发生登录等事件时，日志就会流经我们的实时后端，并记录相应的时间戳。图 1 说明了分配给新（治疗）和现有（控制）软件版本的设备生成的时间戳序列。我们自然关心的一个问题是处理中是否减少了登录事件。你能告诉？ &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AGdASLgCaQCNo72VV7rOFg.gif" /&gt;&lt;figcaption&gt;图 1：控制和治疗中发生的事件的时间戳&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过简单检查图 1 中的点过程并不能立即明显看出这一点。当我们将观察到的&lt;a href="https://en.wikipedia.org/wiki/Counting_process#:~:text=Counting%20processes%20deal%20with%20the,be%20a%20Markov%20counting%20process."&gt;计数过程&lt;/a&gt;可视化时，差异立即变得明显，如图 2 所示。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hAQupmPzmKi7squd4iEwig.png" /&gt;&lt;figcaption&gt;图 2：可视化计数过程 — 时间 t 观察到的事件数量&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;计数过程是每当新事件到达时就加 1 的函数。显然，治疗组中发生的事件比对照组少。如果这些是登录事件，则表明新代码包含一个错误，该错误会阻止某些用户成功登录。&lt;/p&gt;&lt;p&gt;这是处理事件时间戳时的常见情况。举另一个例子，如果事件对应于错误或崩溃，我们想知道这些事件在治疗中是否比在对照组中累积得更快。此外，我们希望&lt;em&gt;尽快&lt;/em&gt;回答这个问题，以防止服务进一步中断&lt;em&gt;。&lt;/em&gt;这需要&lt;a href="https://medium.com/p/cba6c7ed49df"&gt;第 1 部分&lt;/a&gt;中介绍的顺序测试技术。&lt;/p&gt;&lt;h4&gt;&lt;strong&gt;时间非齐次泊松过程&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;我们每个治疗组的数据是一维点过程的实现，即时间戳序列。由于事件到达的速率是随时间变化的（在处理和控制中），我们将点过程建模为时间不均匀的 &lt;a href="https://en.wikipedia.org/wiki/Poisson_point_process#Inhomogeneous_Poisson_point_process"&gt;泊松点过程&lt;/a&gt;。该点过程由强度函数 λ 定义：ℝ → [0, ∞)。区间 [0,t) 中的事件数量，表示为 N(t)，具有以下泊松分布&lt;/p&gt;&lt;p&gt;N(t) ~ Poisson(Λ(t))，其中 Λ(t) = ∫₀ᵗ λ(s) ds。&lt;/p&gt;&lt;p&gt;我们试图检验原假设 H₀：对于所有 t，即控制 (A) 和处理 (B) 的强度函数相同，λᴬ(t) = λᴮ(t)。这可以半参数地完成，无需对强度函数 λᴬ 和 λᴮ 做出任何假设。此外，该研究的新颖之处在于，这可以按顺序完成，如我们论文&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;第 4 节&lt;/a&gt;所述。方便的是，在时间 t 检验该假设所需的唯一数据是 Nᴬ(t) 和 Nᴮ(t)，即迄今为止在对照和治疗中观察到的事件总数。换句话说，检验原假设所需的只是两个整数，这两个整数可以在新事件到达时轻松更新。这是模拟 A/A 测试的一个示例，其中我们通过设计知道对照组 (A) 和处理组 (B) 的强度函数是相同的，尽管是非平稳的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RDdOPECxhDmLt9FkAOuWog.png" /&gt;&lt;figcaption&gt;图 3：（左）两个非齐次泊松点过程的 A/A 模拟。 （右）强度函数对数差值的置信序列和序列 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 3 提供了 A/A 设置的图示。左图展示了原始数据和强度函数，右图展示了序贯统计分析。蓝色和红色地毯图分别表示观察到的来自处理流和控制流的事件的到达时间戳。虚线是观察到的计数过程。由于该数据是在零值下模拟的，因此强度函数是相同的并且相互重叠。右图的左轴可视化强度函数对数差值上置信序列的演变。右图的右轴可视化了序列 p 值的演变。我们可以做出以下两个观察&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在零值下，对数强度差为零，这始终被 0.95 置信度序列正确覆盖。&lt;/li&gt;&lt;li&gt;连续 p 值始终大于 0.05&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在让我们考虑 A/B 设置的示例。图 4 显示了强度函数不同时观察到的治疗和对照到达时间。由于这是模拟，因此对数强度之间的真实差异是已知的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KChDT2L5gSw3BEDvH2Pu0g.png" /&gt;&lt;figcaption&gt;图 4：（左）两个非齐次泊松点过程的 A/B 模拟。 （右）强度函数对数差异的置信序列和序列 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们可以做出以下观察&lt;/p&gt;&lt;ul&gt;&lt;li&gt;0.95 置信度序列始终涵盖真实的对数差异&lt;/li&gt;&lt;li&gt;连续 p 值降至 0.05 以下，同时 0.95 置信度序列排除零值&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;现在，我们提供了一些案例研究，其中该方法可以快速检测到许多计数指标中的严重问题&lt;/p&gt;&lt;h4&gt;案例研究 1：成功的标题开始下降&lt;/h4&gt;&lt;p&gt;图 2 实际上显示了真实金丝雀测试中的标题开始事件的计数。每当影片成功启动时，就会从设备向 Netflix 发送一个&lt;a href="https://netflixtechblog.com/sps-the-pulse-of-netflix-streaming-ae4db0e05f8a"&gt;事件&lt;/a&gt;。我们有来自治疗设备的标题开始事件流和来自控制设备的标题开始事件流。每当在治疗设备中观察到较少的标题开始时，新客户端中通常存在阻止播放的错误。&lt;/p&gt;&lt;p&gt;在这种情况下，金丝雀测试检测到一个错误，后来确定该错误导致大约 60% 的治疗设备无法启动其流。除了（顺序）p 值之外，置信序列如图 5 所示。虽然省略了确切的时间单位，但该错误是在&lt;em&gt;亚秒&lt;/em&gt;级检测到的。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mfkXe5aNK3Y8X7aAXbxhJw.png" /&gt;&lt;figcaption&gt;图 5：对数强度与序列 p 值之差的 0.99 置信序列。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;案例研究 2：异常停机增加&lt;/h4&gt;&lt;p&gt;除了标题开始事件之外，我们还会监控 Netflix 客户端何时意外关闭。和以前一样，我们有两股异常关闭事件，一是来自处理设备，一是来自控制设备。以下屏幕截图直接取自我们的&lt;a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c"&gt;Lumen&lt;/a&gt;仪表板。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*48VrtyraTis6BFnls0hECA.png" /&gt;&lt;figcaption&gt;图 6：一段时间内的异常停机计数（累积和非累积）。处理（黑色）和对照（蓝色）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 6 说明了两个要点。异常关闭事件的到来明显存在非平稳性。从非累积的角度来看，也不容易明显看出治疗和对照之间的任何差异。然而，通过观察计数过程，从累积视图中更容易看出差异。治疗中异常停机的数量有小幅但明显的增加。图 7 显示了我们的序贯统计方法如何能够识别如此小的差异。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*reK3Rxz4nybHPEHYd6LZrQ.png" /&gt;&lt;figcaption&gt;图 7：异常关闭。 （上图）λᴮ(t)/λᴬ(t)（蓝色阴影）的置信序列，以及观察到的治疗（黑色虚线）和对照（蓝色虚线）的计数过程。 （下图）连续 p 值。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;案例研究 3：错误增加&lt;/h4&gt;&lt;p&gt;Netflix 还监控治疗和控制产生的错误数量。这是一个高基数度量，因为每个错误都用指示错误类型的代码进行注释。监控按代码分段的错误有助于开发人员快速诊断问题。图 8 显示了 Netflix 在客户端推出期间监控的一组错误代码的对数标度的连续 p 值。在此示例中，我们检测到治疗设备产生的&lt;a href="https://help.netflix.com/en/node/100573?q=3.1.18"&gt;3.1.18&lt;/a&gt;错误数量较多。遇到此错误的设备会显示以下消息：&lt;/p&gt;&lt;blockquote&gt; “我们现在玩这个游戏遇到了麻烦” &lt;/blockquote&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pef_CT_X6avRIALRT8Q7MA.png" /&gt;&lt;figcaption&gt;图 8：按错误代码列出的开始播放错误的顺序 p 值&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*byxD-v65SdLe8HRaAPGB4g.png" /&gt;&lt;figcaption&gt;图 9：观察到的错误 3.1.18 时间戳和治疗（蓝色）和对照（红色）的计数过程&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;了解&lt;em&gt;哪些&lt;/em&gt;错误增加了可以简化开发人员识别错误的过程。我们立即通过 Slack 集成向开发人员发送警报，例如以下内容&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/565/1*moE4xE6_A-o0l_6Z4cAK1A.png" /&gt;&lt;figcaption&gt;图 10：通过 Slack 集成发出的通知&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;下次当您观看 Netflix 时遇到错误时，请知道我们正在处理它！&lt;/p&gt;&lt;h4&gt;试试看！&lt;/h4&gt;&lt;p&gt;我们&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;论文&lt;/a&gt;中概述的统计方法在实践中非常容易实施。您所需要的只是两个整数，即迄今为止在治疗和控制中观察到的事件数量。该代码可以在这个简短的&lt;a href="https://gist.github.com/michaellindon/5ce04c744d20755c3f653fbb58c2f4dd"&gt;GitHub gist&lt;/a&gt;中找到。下面是两个使用示例：&lt;/p&gt;&lt;pre&gt; &amp;gt; 计数 = [100, 101]&lt;br /&gt; &amp;gt; 分配概率 = [0.5, 0.5]&lt;br /&gt; &amp;gt;equential_p_value（计数，分配概率）&lt;br /&gt; 1&lt;br /&gt;&lt;br /&gt; &amp;gt; 计数 = [100, 201]&lt;br /&gt; &amp;gt; 分配概率 = [0.5, 0.5]&lt;br /&gt; &amp;gt;equential_p_value（计数，分配概率）&lt;br /&gt; 5.06061172163498e-06&lt;/pre&gt;&lt;p&gt;该&lt;a href="https://gist.github.com/michaellindon/5ce04c744d20755c3f653fbb58c2f4dd"&gt;代码&lt;/a&gt;推广到的不仅仅是两个治疗组。有关完整详细信息，包括超参数调整，请参阅本文的&lt;a href="https://openreview.net/pdf?id=a4zg0jiuVi"&gt;第 4 节&lt;/a&gt;。&lt;/p&gt;&lt;h4&gt;进一步阅读&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://openreview.net/forum?id=a4zg0jiuVi"&gt;多项计数数据随时有效推理&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://netflixtechblog.com/sequential-a-b-testing-keeps-the-world-streaming-netflix-part-1-continuous-data-cba6c7ed49df"&gt;顺序 A/B 测试让世界保持流媒体 Netflix 第 1 部分：连续数据&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=da6805341642" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642"&gt;顺序测试让世界保持流媒体播放 Netflix 第 2 部分：计数进程&lt;/a&gt;最初发布在 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;上，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Mon, 18 Mar 2024 12:46:46 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/sequential-testing-keeps-the-world-streaming-netflix-part-2-counting-processes-da6805341642?source=rss----2615bd06b42e---4</guid></item><item><title>【Supporting Diverse ML Systems at Netflix】</title><link>https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;a href="https://www.linkedin.com/in/david-j-berg/"&gt;&lt;em&gt;David J. Berg&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/romain-cledat-4a211a5/"&gt;&lt;em&gt;Romain Cledat&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/seeleykayla/"&gt;&lt;em&gt;Kayla Seeley&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/shashanksrikanth/"&gt;&lt;em&gt;Shashank Srikanth&lt;/em&gt;&lt;/a&gt; &lt;em&gt;、&lt;/em&gt;&lt;a href="https://www.linkedin.com/in/chaoying-wang/"&gt;&lt;em&gt;王超英&lt;/em&gt;&lt;/a&gt;&lt;em&gt;、&lt;/em&gt; &lt;a href="https://www.linkedin.com/in/zitingyu/"&gt;&lt;em&gt;Darin Yu&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt; Netflix 在公司的各个方面使用数据科学和机器学习，为从&lt;a href="https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b"&gt;内部基础设施&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f"&gt;内容需求建模&lt;/a&gt;到&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;媒体理解&lt;/a&gt;的广泛业务应用程序提供支持。 Netflix 的机器学习平台 (MLP) 团队围绕&lt;a href="https://metaflow.org"&gt;Metaflow&lt;/a&gt; （我们发起的开源机器学习基础架构框架）提供了完整的工具生态系统，使数据科学家和机器学习从业者能够构建和管理各种机器学习系统。&lt;/p&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9"&gt;自诞生以来&lt;/a&gt;，Metaflow 一直致力于提供人性化的 API，用于构建数据和 ML（以及今天的 AI）应用程序，并将它们顺利地部署在我们的生产基础设施中。虽然人性化的 API 令人愉悦，但真正赋予 Metaflow 超能力的是与我们的生产系统的集成。如果没有这些集成，项目将陷入原型设计阶段，或者必须作为我们工程团队维护的系统之外的异常值进行维护，从而产生不可持续的运营开销。&lt;/p&gt;&lt;p&gt;鉴于我们支持的机器学习和人工智能用例非常多样化——今天我们在内部部署了数百个 Metaflow 项目——我们不期望所有项目都遵循从原型到生产的相同路径。相反，我们提供了一个强大的基础层，与我们公司范围内的数据、计算和编排平台集成，以及将应用程序顺利部署到生产的各种路径。除此之外，团队还构建了自己的特定领域库来支持他们的特定用例和需求。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4hoAg4FX6oeua708alTMlA.png" /&gt;&lt;/figure&gt;&lt;p&gt;在本文中，我们将介绍为 Netflix 的 Metaflow 堆栈各个层提供的一些关键集成，如上图所示。我们还将展示依赖它们的现实机器学习项目，以了解我们支持的项目的广度。请注意，所有项目都利用多种集成，但我们在它们最显着使用的集成的上下文中突出显示它们。重要的是，所有用例都是由从业者自己设计的。&lt;/p&gt;&lt;p&gt;这些集成是通过&lt;a href="https://github.com/Netflix/metaflow-extensions-template"&gt;Metaflow 的扩展机制&lt;/a&gt;实现的，该机制是公开可用的，但可能会发生变化，因此还不是 Metaflow 稳定 API 的一部分。如果您对实现自己的扩展感到好奇，请在&lt;a href="http://chat.metaflow.org"&gt;Metaflow 社区 Slack&lt;/a&gt;上与我们联系。&lt;/p&gt;&lt;p&gt;让我们从最基本的集成开始逐层浏览堆栈。&lt;/p&gt;&lt;h3&gt;数据：快速数据&lt;/h3&gt;&lt;p&gt;我们的主要数据湖&lt;a href="https://www.youtube.com/watch?v=jMFMEk8jFu8"&gt;托管在 S3 上，组织为 Apache Iceberg 表&lt;/a&gt;。对于 ETL 和其他繁重的数据处理，我们主要依赖 Apache Spark。除了 Spark 之外，我们还希望支持 Python 中的最后一英里数据处理，解决特征转换、批量推理和训练等用例。有时，这些用例涉及 TB 级的数据，因此我们必须关注性能。&lt;/p&gt;&lt;p&gt;为了实现对 Netflix 数据仓库的快速、可扩展且稳健的访问，我们为 Metaflow 开发了一个&lt;em&gt;快速&lt;/em&gt;数据库，它利用了 Python 数据生态系统中的高性能组件： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OGn9AcNNdMXAhxLq8WugkQ.png" /&gt;&lt;/figure&gt;&lt;p&gt;如图所示，Fast Data 库由两个主要接口组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; Table 对象负责与 Netflix 数据仓库交互，其中包括解析 Iceberg（或旧版 Hive）表元数据、解析分区和 Parquet 文件以供读取。最近，我们添加了对写入路径的支持，因此也可以使用该库更新表。&lt;/li&gt;&lt;li&gt;一旦我们发现要处理的 Parquet 文件，MetaflowDataFrame 就会接管：它使用 Metaflow 的高吞吐量 S3 客户端将数据直接下载到进程的内存，这&lt;a href="https://outerbounds.com/blog/metaflow-fast-data/"&gt;通常优于读取本地文件&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们使用&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;来解码 Parquet 并托管数据的内存表示。用户可以选择最合适的工具来操作数据，例如&lt;a href="https://pandas.pydata.org"&gt;Pandas&lt;/a&gt;或&lt;a href="https://pola.rs"&gt;Polars&lt;/a&gt;来使用数据帧 API，或者使用我们的内部 C++ 库之一来进行各种高性能操作。感谢 Arrow，可以通过这些库以零拷贝的方式访问数据。&lt;/p&gt;&lt;p&gt;我们还关注依赖问题：(Py)Arrow 是许多 ML 和数据库的依赖项，因此我们不希望我们的自定义 C++ 扩展依赖于特定版本的 Arrow，这很容易导致无法解析的依赖关系图。相反，在&lt;a href="https://github.com/apache/arrow-nanoarrow"&gt;nanoarrow&lt;/a&gt;的风格中，我们的 Fast Data 库仅依赖于&lt;a href="https://arrow.apache.org/docs/format/CDataInterface.html"&gt;稳定的 Arrow C 数据接口&lt;/a&gt;，生成一个密封的库，没有外部依赖项。&lt;/p&gt;&lt;h4&gt;示例用例：内容知识图&lt;/h4&gt;&lt;p&gt;我们的娱乐世界知识图对电影或电视剧的标题、演员和其他属性之间的关系进行编码，支持 Netflix 业务的各个方面。&lt;/p&gt;&lt;p&gt;创建知识图的一个关键挑战是实体解析。关于标题的信息可能有许多不同的表示，这些表示略有不同或相互冲突，必须予以解决。这通常是通过每个实体的成对匹配过程来完成的，这在规模化时变得非常重要。&lt;/p&gt;&lt;p&gt;该项目利用快速数据和水平扩展以及&lt;a href="https://docs.metaflow.org/v/r/metaflow/basics#foreach"&gt;Metaflow 的 foreach 结构&lt;/a&gt;来加载存储在 Netflix 数据仓库中的大量标题信息（大约十亿对），因此这些对可以在许多 Metaflow 任务中并行匹配。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KG7TUCqTF3uRU6lUncCHtg.png" /&gt;&lt;/figure&gt;&lt;p&gt;我们使用 metaflow.Table 来解析分配给 Metaflow 任务的所有输入分片，这些任务负责共同处理 TB 级的数据。每个任务都使用 metaflow.MetaflowDataFrame 加载数据，使用 Pandas 执行匹配，并在输出表中填充相应的分片。最后，当所有匹配完成并写入数据时，新表将被提交，以便其他作业可以读取它。&lt;/p&gt;&lt;h3&gt;计算：提图斯&lt;/h3&gt;&lt;p&gt;Metaflow 的开源用户依赖&lt;a href="https://docs.metaflow.org/scaling/remote-tasks/introduction"&gt;AWS Batch 或 Kubernetes 作为计算后端&lt;/a&gt;，而我们则依赖&lt;a href="https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436"&gt;我们的集中式计算平台 Titus&lt;/a&gt; 。在底层，Titus&lt;a href="https://www.slideshare.net/aspyker/herding-kats-netflixs-journey-to-kubernetes-public"&gt;由 Kubernetes 提供支持&lt;/a&gt;，但它比现成的 Kubernetes 提供了一层厚厚的增强功能，&lt;a href="https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225"&gt;使其更具可观察性&lt;/a&gt;、 &lt;a href="https://netflixtechblog.com/evolving-container-security-with-linux-user-namespaces-afbe3308c082"&gt;安全性&lt;/a&gt;、&lt;a href="https://netflixtechblog.com/auto-scaling-production-services-on-titus-1f3cd49f5cd7"&gt;可扩展性&lt;/a&gt;和&lt;a href="https://netflixtechblog.com/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7"&gt;成本效益&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;通过以 @titus 为目标，Metaflow 任务可以从这些久经考验的开箱即用功能中受益，无需 ML 工程师或数据科学家具备深入的技术知识或工程知识。然而，为了从可扩展的计算中受益，我们需要帮助开发人员以可重现的方式（最好是快速）在远程 Pod 中打包和补充项目的整个执行环境。具体来说，我们不想要求开发人员手动管理自己的 Docker 镜像，这很快会导致比解决的问题更多的问题。&lt;/p&gt;&lt;p&gt;这就是&lt;a href="https://docs.metaflow.org/scaling/dependencies"&gt;Metaflow 提供开箱即用的依赖管理支持的&lt;/a&gt;原因。最初，我们只支持 @conda，但基于我们在&lt;a href="https://github.com/Netflix/metaflow-nflx-extensions"&gt;可移植执行环境&lt;/a&gt;方面的工作，开源 Metaflow 几个月前也获得了对&lt;a href="https://outerbounds.com/blog/pypi-announcement/"&gt;@pypi&lt;/a&gt;&lt;a href="https://outerbounds.com/blog/pypi-announcement/"&gt;的支持&lt;/a&gt;。&lt;/p&gt;&lt;h4&gt;示例用例：构建模型解释器&lt;/h4&gt;&lt;p&gt;这是一个关于可移植执行环境有用性的有趣示例。对于我们的许多应用程序来说，模型的可解释性很重要。利益相关者喜欢了解为什么模型会产生特定的输出以及为什么它们的行为会随着时间的推移而变化。&lt;/p&gt;&lt;p&gt;有多种方法可以为模型提供可解释性，但一种方法是基于每个经过训练的模型来训练解释器模型。无需详细说明这是如何完成的，只需说 Netflix 训练了很多模型，因此我们也需要训练很多解释器。&lt;/p&gt;&lt;p&gt;借助 Metaflow，我们可以允许每个应用程序为其用例选择最佳的建模方法。相应地，每个应用程序都有自己定制的依赖项集。因此，训练解释器模型需要：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;访问原始模型及其训练环境，以及&lt;/li&gt;&lt;li&gt;特定于构建解释器模型的依赖关系。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这对依赖管理提出了一个有趣的挑战：我们需要一个高阶训练系统，即下图中的“Explainer flow”，它能够将另一个训练系统的完整执行环境作为输入，并基于它生成模型。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WoHEm2yvuo22NRp4qf0W9g.png" /&gt;&lt;/figure&gt;&lt;p&gt;解释器流由上游流事件触发，例如图中的模型 A、B、C 流。 build_environment 步骤使用&lt;a href="https://github.com/Netflix/metaflow-nflx-extensions"&gt;我们的可移植环境&lt;/a&gt;提供的元流环境命令来构建一个环境，其中包括输入模型的要求以及构建解释器模型本身所需的要求。&lt;/p&gt;&lt;p&gt;构建的环境被赋予一个唯一的名称，该名称取决于运行标识符（以提供唯一性）以及模型类型。给定这个环境，train_explainer 步骤就能够引用这个唯一命名的环境，并在既可以访问输入模型又可以训练解释器模型的环境中运行。请注意，与使用 vanilla @conda 或 @pypi 的典型流程不同，可移植环境扩展允许用户在执行时直接获取这些环境，而不是在部署时，因此允许用户在本例中解析环境在下一步使用它之前。&lt;/p&gt;&lt;h3&gt;编曲：大师&lt;/h3&gt;&lt;p&gt;如果数据是机器学习的燃料，计算层是肌肉，那么神经一定是编排层。几年前，当&lt;a href="https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280"&gt;我们发布对 AWS Step Functions 的支持&lt;/a&gt;时，我们已经讨论过 Metaflow 背景下生产级工作流编排器的重要性。从那时起，开源 Metaflow 获得了对 Kubernetes 原生编排器&lt;a href="https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow/"&gt;Argo Workflows&lt;/a&gt;的支持，以及&lt;a href="https://outerbounds.com/blog/better-airflow-with-metaflow/"&gt;对至今仍被数据工程团队广泛使用的 Airflow 的支持&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;在内部，我们使用&lt;a href="https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c"&gt;名为 Maestro 的生产工作流程协调器&lt;/a&gt;。 Maestro 帖子分享了有关系统如何支持可扩展性、高可用性和可用性的详细信息，这为我们生产中的所有 Metaflow 项目提供了支柱。&lt;/p&gt;&lt;p&gt;一个经常被忽视的非常重要的细节是&lt;a href="https://docs.metaflow.org/production/event-triggering"&gt;事件触发&lt;/a&gt;：它允许团队使用与整个组织，如下面的示例用例所示。&lt;/p&gt;&lt;h4&gt;示例用例：内容决策&lt;/h4&gt;&lt;p&gt;Metaflow 上运行的最关键的业务系统之一&lt;a href="https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f"&gt;支持我们的内容决策&lt;/a&gt;，即 Netflix 应该为服务带来什么内容的问题。我们支持来自 190 多个国家/地区超过 2.6 亿的大规模订阅者，他们代表着截然不同的文化和品味，我们希望我们的内容能够让他们满意。反映挑战的广度和深度，针对该问题的系统和模型已经变得非常复杂。&lt;/p&gt;&lt;p&gt;我们从多个角度解决这个问题，但我们拥有一组核心数据管道和模型，为决策提供基础。为了说明核心组件的复杂性，请考虑以下高级图表： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rp4sF-nIWgTt8kdt" /&gt;&lt;/figure&gt;&lt;p&gt;在此图中，灰色框表示与下游和上游合作伙伴团队的集成，绿色框是各种 ETL 管道，蓝色框是 Metaflow 流。这些盒子封装了数百个高级模型和复杂的业务逻辑，每天处理大量数据。&lt;/p&gt;&lt;p&gt;尽管该系统很复杂，但它是由相对较小的工程师和数据科学家团队自主管理的。 Metaflow 的一些关键功能使这成为可能：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;所有的盒子都是事件触发的，由 Maestro 精心策划。 Metaflow 流之间的依赖关系通过&lt;a href="https://docs.metaflow.org/production/event-triggering/flow-events"&gt;@trigger_on_finish&lt;/a&gt;触发，通过&lt;a href="https://docs.metaflow.org/production/event-triggering/external-events"&gt;@trigger&lt;/a&gt;触发对外部系统的依赖关系。&lt;/li&gt;&lt;li&gt;通过&lt;a href="https://docs.metaflow.org/scaling/tagging"&gt;Metaflow 命名空间&lt;/a&gt;实现快速开发，因此个人开发人员可以在不干扰生产部署的情况下进行开发。&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.metaflow.org/production/coordinating-larger-metaflow-projects"&gt;分支开发和部署是通过&lt;/a&gt;&lt;a href="https://docs.metaflow.org/production/coordinating-larger-metaflow-projects"&gt;@project&lt;/a&gt;进行管理的，它还&lt;a href="https://docs.metaflow.org/production/event-triggering/project-events"&gt;隔离了不同分支之间的事件&lt;/a&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该团队还开发了自己的特定领域库和配置管理工具，帮助他们改进和操作系统。&lt;/p&gt;&lt;h3&gt;部署：缓存&lt;/h3&gt;&lt;p&gt;为了产生商业价值，我们所有的 Metaflow 项目都被部署为与其他生产系统配合使用。在许多情况下，集成可能是通过数据仓库中的共享表进行的。在其他情况下，通过低延迟 API 共享结果会更方便。&lt;/p&gt;&lt;p&gt;值得注意的是，并非所有基于 API 的部署都需要实时评估，我们将在下面的部分中介绍这一点。我们拥有许多关键业务应用程序，可以预先计算部分或全部预测，从而保证全球范围内尽可能低的延迟和操作简单的高可用性。&lt;/p&gt;&lt;p&gt;我们开发了一个官方支持的模式来涵盖此类用例。虽然系统依赖于我们的内部缓存基础设施，但您可以使用&lt;a href="https://aws.amazon.com/elasticache/"&gt;Amazon ElasticCache&lt;/a&gt;或&lt;a href="https://aws.amazon.com/dynamodb/"&gt;DynamoDB&lt;/a&gt;等服务遵循相同的模式。&lt;/p&gt;&lt;h4&gt;示例用例：内容性能可视化&lt;/h4&gt;&lt;p&gt;决策者利用片名的历史表现来理解和改进电影和连续剧目录。性能指标可能很复杂，并且通常最容易被人类通过可视化来理解，这些可视化可以交互式地跨感兴趣的参数分解指标。内容决策者可以通过使用metaflow.Cache构建的实时Web应用程序来实现自助可视化，该应用程序可以通过metaflow.Hosting提供的API进行访问。 &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*i3YtLUvobXxEYE6crwQy_w.png" /&gt;&lt;/figure&gt;&lt;p&gt;每日计划的 Metaflow 作业并行计算感兴趣的总量。该作业使用 metaflow.Cache 将大量结果写入在线键值存储。 &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;应用程序包含可视化软件和数据聚合逻辑。用户可以动态更改可视化应用程序的参数，并实时将消息发送到简单的&lt;a href="https://netflixtechblog.com/feed#a890"&gt;Metaflow 托管服务&lt;/a&gt;，该服务在缓存中查找值、执行计算，并将结果作为 JSON blob 返回到 Streamlit 应用程序。&lt;/p&gt;&lt;h3&gt;部署：Metaflow 托管&lt;/h3&gt;&lt;p&gt;对于需要 API 和实时评估的部署，我们提供集成模型托管服务 Metaflow Hosting。尽管细节已经发生了很大变化，但&lt;a href="https://www.youtube.com/watch?v=sBM5cSBGZS4"&gt;这段老话仍然很好地概述了该服务&lt;/a&gt;。&lt;/p&gt;&lt;p&gt; Metaflow Hosting 专门用于托管 Metaflow 中生成的工件或模型。这在 Netflix 现有的微服务基础设施之上提供了一个易于使用的界面，使数据科学家能够快速将他们的工作从实验转移到生产级 Web 服务，该服务可以通过 HTTP REST API 以最小的开销使用。&lt;/p&gt;&lt;p&gt;其主要优点包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;用于创建 RESTFull 端点的简单装饰器语法。&lt;/li&gt;&lt;li&gt;后端根据流量自动扩展用于支持服务的实例数量。&lt;/li&gt;&lt;li&gt;如果在指定时间后没有向后端发出请求，则后端将缩放至零，从而节省成本，特别是如果您的服务需要 GPU 有效地产生响应。&lt;/li&gt;&lt;li&gt;请求 Netflix 基础设施的日志记录、警报、监控和跟踪挂钩&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;考虑该服务类似于&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html"&gt;AWS Sagemaker Model Hosting&lt;/a&gt;等托管模型托管服务，但与我们的微服务基础设施紧密集成。&lt;/p&gt;&lt;h4&gt;示例用例：媒体&lt;/h4&gt;&lt;p&gt;我们在使用机器学习来处理媒体资产方面有着悠久的历史，例如，&lt;a href="https://netflixtechblog.com/artwork-personalization-c589f074ad76"&gt;个性化艺术品&lt;/a&gt;并帮助我们的 &lt;a href="https://netflixtechblog.com/new-series-creating-media-with-machine-learning-5067ac110bcd"&gt;创意人员有效地创建促销内容&lt;/a&gt;。处理大量媒体资产在技术上并不简单，而且计算成本高昂，因此多年来，我们开发了许多专门用于此目的的&lt;a href="https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359"&gt;专用基础设施&lt;/a&gt;，特别是&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;支持媒体 ML 用例的基础设施&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;为了演示 Metaflow Hosting 提供支持同步和异步查询的通用 API 层的优势，请考虑涉及&lt;a href="https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243"&gt;Amber（我们的媒体功能存储）的&lt;/a&gt;用例。&lt;/p&gt;&lt;p&gt;虽然 Amber 是一个特征&lt;em&gt;存储&lt;/em&gt;，但提前预计算和存储所有媒体特征是不可行的。相反，我们按需计算和缓存特征，如下所示： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SP7GASNee-YB_dDu35ldJA.png" /&gt;&lt;/figure&gt;&lt;p&gt;当一项服务向 Amber 请求一项功能时，它会计算功能依赖关系图，然后向 Metaflow Hosting 发送一个或多个异步请求，Metaflow Hosting 将请求放入队列中，最终在计算资源可用时触发功能计算。 Metaflow Hosting 会缓存响应，因此 Amber 可以在一段时间后获取它。我们本可以专门为此用例构建专用的微服务，但由于 Metaflow Hosting 的灵活性，我们能够更快地交付该功能，而无需额外的操作负担。&lt;/p&gt;&lt;h3&gt;未来的工作&lt;/h3&gt;&lt;p&gt;我们对在不同用例中应用机器学习的兴趣只会越来越大，因此我们的 Metaflow 平台将相应地不断扩大其足迹，并继续为 Netlfix 其他团队构建的系统提供令人愉快的集成。例如，我们计划通过为工件和模型管理提供更多选项来改进版本控制层（本文未涵盖）。&lt;/p&gt;&lt;p&gt;我们还计划与 Netflix 姐妹团队正在开发的其他系统建立更多集成。例如，Metaflow 托管模型目前尚未很好地集成到模型日志记录设施中 - 我们计划致力于改进这一点，以使使用 Metaflow 开发的模型与训练新模型至关重要的反馈循环更加集成。我们希望以可插入的方式做到这一点，允许其他用户与他们自己的日志系统集成。&lt;/p&gt;&lt;p&gt;此外，我们希望提供更多方式将 Metaflow 工件和模型集成到非 Metaflow 环境和应用程序中，例如基于 JVM 的边缘服务，以便基于 Python 的数据科学家可以轻松地为非 Python 工程系统做出贡献。这将使我们能够更好地弥合 Metaflow 提供的快速迭代（使用 Python）与服务 Netflix 会员面临的请求的基础设施所施加的要求和约束之间的差距。&lt;/p&gt;&lt;p&gt;如果您正在组织中构建业务关键型 ML 或 AI 系统，&lt;a href="http://chat.metaflow.org"&gt;请加入 Metaflow Slack 社区&lt;/a&gt;！我们很乐意分享经验、回答任何问题，并欢迎您为 Metaflow 做出贡献。&lt;/p&gt;&lt;h4&gt;致谢：&lt;/h4&gt;&lt;p&gt;感谢 Wenbing Bai、Jan Florjanczyk、Michael Li、Aliki Mavromoustaki 和 Sejal Rai 在用例和数据方面提供的帮助。感谢我们的 OSS 贡献者使 Metaflow 成为更好的产品。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d2e6b6d205d" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d"&gt;《Supporting Diverse ML Systems at Netflix》&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix TechBlog&lt;/a&gt; ，人们通过强调和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Thu, 07 Mar 2024 18:33:07 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d?source=rss----2615bd06b42e---4</guid></item><item><title>【Bending pause times to your will with Generational ZGC】</title><link>https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b?source=rss----2615bd06b42e---4</link><description>&lt;p&gt;&lt;em&gt;Z 垃圾收集器中几代人的令人惊讶和不那么令人惊讶的好处。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;作者：Danny Thomas，JVM 生态系统团队&lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GuEZ-RMhzNnYgLQd" /&gt;&lt;/figure&gt;&lt;p&gt;JDK 的最新长期支持版本为&lt;a href="https://docs.oracle.com/en/java/javase/21/gctuning/z-garbage-collector.html"&gt;Z 垃圾收集器&lt;/a&gt;提供了世代支持。由于并发垃圾收集的显着优势，Netflix 在 JDK 21 及更高版本上默认从 G1 切换到分代 ZGC。&lt;/p&gt;&lt;p&gt;我们超过一半的关键流视频服务现在都在带有 Generational ZGC 的 JDK 21 上运行，因此现在是谈论我们的经验和我们所看到的好处的好时机。如果您对我们如何在 Netflix 使用 Java 感兴趣，Paul Bakker 的演讲&lt;a href="https://www.infoq.com/presentations/netflix-java/"&gt;《Netflix 如何真正使用 Java&lt;/a&gt; 》是一个很好的起点。&lt;/p&gt;&lt;h3&gt;减少尾部延迟&lt;/h3&gt;&lt;p&gt;在我们的 GRPC 和&lt;a href="https://netflix.github.io/dgs/"&gt;DGS 框架&lt;/a&gt;服务中，GC 暂停是尾部延迟的重要来源。对于我们的 GRPC 客户端和服务器来说尤其如此，其中由于超时而导致的请求取消与重试、对冲和回退等可靠性功能相互作用。每个错误都是取消的请求，导致重试，因此这种减少进一步按以下速率减少总体服务流量： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*SCVt4VGlA517hZDi" /&gt;&lt;figcaption&gt;每秒的错误率。上周（白色）与当前取消率（紫色），因为 ZGC 已于 11 月 16 日在服务集群上启用&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;消除暂停噪音还使我们能够识别端到端延迟的实际来源，否则这些延迟来源将隐藏在噪音中，因为最大暂停时间异常值可能很重要： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rW029WscxSKDQRQ6" /&gt;&lt;figcaption&gt;对于与上述相同的服务集群，按原因划分的最大 GC 暂停时间。是的，那些 ZGC 暂停通常低于一毫秒&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;效率&lt;/h3&gt;&lt;p&gt;即使我们在评估中看到了非常有希望的结果，我们仍然期望采用 ZGC 是一种权衡：由于存储和加载障碍、在线程本地握手中执行的工作以及 GC 与应用程序竞争，应用程序吞吐量会稍微减少为了资源。我们认为这是一个可以接受的权衡，因为避免暂停所带来的好处将超过开销。&lt;/p&gt;&lt;p&gt;事实上，我们发现我们的服务和架构不存在这样的权衡。对于给定的 CPU 利用率目标，与 G1 相比，ZGC 改善了平均延迟和 P99 延迟，且 CPU 利用率相同或更好。&lt;/p&gt;&lt;p&gt;我们在许多服务中看到的请求率、请求模式、响应时间和分配率的一致性无疑对 ZGC 有帮助，但我们发现它同样能够处理不太一致的工作负载（当然有例外；更多内容见下文）。&lt;/p&gt;&lt;h3&gt;操作简单&lt;/h3&gt;&lt;p&gt;服务所有者经常向我们询问有关暂停时间过长的问题并寻求调整方面的帮助。我们有几个框架可以定期刷新大量堆上数据，以避免外部服务调用以提高效率。这些堆上数据的定期刷新非常适合让 G1 感​​到惊讶，从而导致暂停时间异常值远远超出默认的暂停时间目标。&lt;/p&gt;&lt;p&gt;这些长期存在的堆上数据是我们之前不采用非分代 ZGC 的主要因素。在我们评估的最坏情况下，对于相同的工作负载，非分代 ZGC 的 CPU 利用率比 G1 高出 36%。一代 ZGC 的性能提升了近 10%。&lt;/p&gt;&lt;p&gt;流视频所需的所有服务中有一半使用我们的&lt;a href="https://hollow.how/)"&gt;Hollow&lt;/a&gt;库来获取堆上元数据。消除暂停这一问题使我们能够&lt;a href="https://github.com/Netflix/hollow/commit/4f21ab593543bb622d9ccea2f8e6295eae5e8080"&gt;消除阵列池缓解措施&lt;/a&gt;，从而释放数百兆字节的内存以进行分配。&lt;/p&gt;&lt;p&gt;操作简单性也源于 ZGC 的启发式和默认设置。无需显式调整即可实现这些结果。分配停滞很少见，通常与分配率的异常峰值同时发生，并且比我们在 G1 中看到的平均暂停时间短。&lt;/p&gt;&lt;h3&gt;内存开销&lt;/h3&gt;&lt;p&gt;我们预计，由于&lt;a href="https://youtu.be/YyXjC68l8mw?t=816"&gt;彩色指针需要 64 位对象指针&lt;/a&gt;，因此在小于 32G 的堆上丢失&lt;a href="https://shipilev.net/jvm/anatomy-quarks/23-compressed-references/"&gt;压缩引用&lt;/a&gt;将成为选择垃圾收集器的主要因素。&lt;/p&gt;&lt;p&gt;我们发现，虽然这是 stop-the-world GC 的一个重要考虑因素，但 ZGC 的情况并非如此，即使在小堆上，分配率的增加也会因效率和操作改进而摊销。我们感谢 Oracle 的 Erik Österlund 解释了彩色指针在并发垃圾收集器方面不太直观的好处，这使我们能够比最初计划更广泛地评估 ZGC。&lt;/p&gt;&lt;p&gt;在大多数情况下，ZGC 还能够持续为应用程序提供更多可用内存： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3eTNEdI2mHfL1Yvk" /&gt;&lt;figcaption&gt;对于与上述相同的服务集群，每个 GC 周期后的已用堆容量与可用堆容量&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;ZGC 的固定开销为堆大小的 3%，需要比 G1 更多的本机内存。除了少数情况外，无需降低最大堆大小以留出更多空间，而这些服务的本机内存需求高于平均水平。&lt;/p&gt;&lt;p&gt;参考处理也仅在 ZGC 的主要集合中执行。我们特别关注直接字节缓冲区的重新分配，但到目前为止我们还没有看到任何影响。引用处理中的这种差异确实导致了&lt;a href="https://bugs.openjdk.org/browse/JDK-8321178"&gt;JSON 线程转储支持的性能问题&lt;/a&gt;，但这是一种不寻常的情况，因为框架意外地为每个请求创建了未使用的 ExecutorService 实例。&lt;/p&gt;&lt;h3&gt;透明大页&lt;/h3&gt;&lt;p&gt;即使您不使用 ZGC，您也可能应该使用大页面，而&lt;a href="https://shipilev.net/jvm/anatomy-quarks/2-transparent-huge-pages/"&gt;透明大页面&lt;/a&gt;是使用它们的最方便的方式。&lt;/p&gt;&lt;p&gt; ZGC 对堆使用共享内存，并且许多 Linux 发行版将 shmem_enabled 配置为&lt;em&gt;never&lt;/em&gt; ，这会默默地阻止 ZGC 使用 -XX:+UseTransparentHugePages 的大页面。&lt;/p&gt;&lt;p&gt;这里我们部署了一个服务，没有进行任何其他更改，但 shmem_enabled 从 never 变为建议，从而显着降低了 CPU 利用率： &lt;/p&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bGoc3W9P_E2kjghe" /&gt;&lt;figcaption&gt;部署从 4k 页面变为 2m 页面。忽略差距，这是我们的不可变部署过程暂时将集群容量加倍&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们的默认配置：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;将堆最小值和最大值设置为相等大小&lt;/li&gt;&lt;li&gt;配置 -XX:+UseTransparentHugePages -XX:+AlwaysPreTouch&lt;/li&gt;&lt;li&gt;使用以下透明_hugepage 配置：&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;回声疯狂 | sudo tee /sys/kernel/mm/transparent_hugepage/enabled&lt;br /&gt;回声建议| sudo tee /sys/kernel/mm/transparent_hugepage/shmem_enabled&lt;br /&gt;回显延迟| sudo tee /sys/kernel/mm/transparent_hugepage/defrag&lt;br /&gt;回声 1 | sudo tee /sys/kernel/mm/transparent_hugepage/khugpaged/defrag&lt;/pre&gt;&lt;h3&gt;哪些工作负载不适合？&lt;/h3&gt;&lt;p&gt;没有最好的垃圾收集器。每个都根据垃圾收集器的目标权衡收集吞吐量、应用程序延迟和资源利用率。&lt;/p&gt;&lt;p&gt;对于使用 G1 与 ZGC 相比表现更好的工作负载，我们发现它们往往更注重吞吐量，分配率非常高，并且长时间运行的任务在不可预测的时间内保存对象。&lt;/p&gt;&lt;p&gt;一个值得注意的例子是一个服务，其中的分配率非常高，并且存在大量长期存在的对象，这恰好非常适合 G1 的暂停时间目标和旧区域收集启发式方法。它使 G1 能够避免 GC 周期中的非生产性工作，而 ZGC 则无法做到这一点。&lt;/p&gt;&lt;p&gt;默认情况下切换到 ZGC 为应用程序所有者提供了考虑垃圾收集器选择的绝佳机会。一些批处理/预计算案例默认使用 G1，他们会看到并行收集器具有更好的吞吐量。在一个大型预计算工作负载中，我们发现应用程序吞吐量提高了 6-8%，与 G1 相比，批处理时间缩短了一个小时。&lt;/p&gt;&lt;h3&gt;亲自尝试一下！&lt;/h3&gt;&lt;p&gt;如果不加质疑，假设和期望可能会导致我们错过十年来我们对运营默认值所做的最有影响力的改变之一。我们鼓励您亲自尝试世代 ZGC。它可能会让您感到惊讶，就像我们感到惊讶一样。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=256629c9386b" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b"&gt;使用 Generational ZGC 让暂停时间随心所欲&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Wed, 06 Mar 2024 01:35:08 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/bending-pause-times-to-your-will-with-generational-zgc-256629c9386b?source=rss----2615bd06b42e---4</guid></item><item><title>【Evolving from Rule-based Classifier: Machine Learning Powered Auto Remediation in Netflix Data…】</title><link>https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b?source=rss----2615bd06b42e---4</link><description>&lt;h3&gt;从基于规则的分类器演变而来：Netflix 数据平台中机器学习支持的自动修复&lt;/h3&gt;&lt;p&gt;作者： &lt;a href="https://www.linkedin.com/in/binbing-hou/overlay/about-this-profile/"&gt;Binbing Hou&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/stephanievezich/overlay/about-this-profile/"&gt;Stephanie Vezich Tamayo&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/chenxiao000/overlay/about-this-profile/"&gt;Xiao Chen&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/liangtian/overlay/about-this-profile/"&gt;Liang Tian&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/troy-ristow-4899b49/overlay/about-this-profile/"&gt;Troy Ristow&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/haoyuanwang/overlay/about-this-profile/"&gt;Haoyuan Wang&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/snehalchennuru/overlay/about-this-profile/"&gt;Snehal Chennuru&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/pawan-dixit-b4307b2/overlay/about-this-profile/"&gt;Pawan Dixit&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;这是 Netflix 关于利用数据洞察和机器学习 (ML) 围绕大数据作业的性能和成本效率提高运营自动化的系列工作中的第一篇。操作自动化——包括但不限于自动诊断、自动修复、自动配置、自动调整、自动缩放、自动调试和自动测试——是现代数据平台成功的关键。在这篇博文中，我们介绍了我们的自动修复项目，该项目将当前使用的基于规则的分类器与机器学习服务集成在一起，旨在自动修复失败的作业，无需人工干预。我们在生产中部署了 Auto Remediation，用于处理 Spark 作业的内存配置错误和未分类错误，并观察了其效率和效果（例如，自动修复 56% 的内存配置错误，节省了所有错误造成的 50% 的金钱成本），效果非常好。进一步改进的潜力。&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;介绍&lt;/h3&gt;&lt;p&gt;在 Netflix，每天有数十万个工作流程和数百万个作业在大数据平台的多个层上运行。鉴于这种分布式大型系统固有的广泛范围和错综复杂的复杂性，即使失败的作业只占总工作负载的一小部分，诊断和修复作业失败也会造成相当大的运营负担。&lt;/p&gt;&lt;p&gt;为了高效地处理错误，Netflix 开发了一种名为 Pitive 的错误分类服务，它利用基于规则的分类器进行错误分类。基于规则的分类器根据一组预定义的规则对作业错误进行分类，并为调度程序提供见解以决定是否重试作业，并为工程师提供诊断和修复作业失败的信息。&lt;/p&gt;&lt;p&gt;然而，随着系统规模和复杂性的增加，基于规则的分类器由于对操作自动化的支持有限，特别是在处理内存配置错误和未分类错误方面一直面临挑战。因此，运营成本随着失败作业的数量线性增加。在某些情况下，例如，诊断和修复由内存不足 (OOM) 错误引起的作业失败，需要跨团队的共同努力，不仅涉及用户本身，还涉及支持工程师和领域专家。&lt;/p&gt;&lt;p&gt;为了应对这些挑战，我们开发了一项名为&lt;em&gt;“自动修复”&lt;/em&gt;的新功能，它将基于规则的分类器与机器学习服务集成在一起。基于基于规则的分类器的分类，它使用机器学习服务来预测重试成功概率和重试成本，并选择最佳候选配置作为建议；以及自动应用建议的配置服务。其主要优点如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;集成智能。&lt;/strong&gt;自动修复并没有完全弃用当前基于规则的分类器，而是将分类器与 ML 服务集成，以便它可以利用两者的优点：基于规则的分类器为每个错误类别提供静态、确定性的分类结果，该结果基于领域专家的背景； ML 服务利用 ML 的强大功能，为每个作业提供性能和成本感知建议。通过集成智能，我们可以很好地满足修复不同错误的要求。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;完全自动化。&lt;/strong&gt;错误分类、获取建议和应用建议的流程是完全自动化的。它将建议连同重试决策一起提供给调度程序，并且特别使用在线配置服务来存储和应用推荐的配置。这样，修复过程中就不需要人为干预。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;多目标优化。&lt;/strong&gt;自动修复通过考虑性能（即重试成功概率）和计算成本效率（即运行作业的货币成本）来生成建议，以避免盲目推荐资源消耗过多的配置。例如，对于内存配置错误，它会搜索与作业执行的内存使用相关的多个参数，并推荐最小化故障概率和计算成本的线性组合的组合。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些优势已通过修复 Spark 作业故障的生产部署得到验证。我们的观察表明，自动修复&lt;em&gt; &lt;/em&gt;通过在线应用推荐的内存配置，无需人工干预即可成功修复约 56% 的内存配置错误；同时，由于它能够推荐新配置以使内存配置成功并禁止对未分类错误进行不必要的重试，因此可降低约 50% 的成本。我们还注意到通过模型调整进一步改进的巨大潜力（请参阅生产中的推出部分）。&lt;/p&gt;&lt;h3&gt;基于规则的分类器：基础知识和挑战&lt;/h3&gt;&lt;h4&gt;基本&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pnViNRB4q-LX7rcdn6MgHA.png" /&gt;&lt;/figure&gt;&lt;p&gt;图1展示了数据平台中的错误分类服务，即Pitive。它利用基于规则的分类器，由三个组件组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;日志收集器&lt;/strong&gt;负责从不同平台层（例如调度程序、作业编排器和计算集群）提取日志以进行错误分类。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;规则执行引擎&lt;/strong&gt;负责将收集的日志与一组预定义的规则进行匹配。规则包括（1）错误的名称、来源、日志和摘要以及错误是否可重新启动； (2) 用于从日志中识别错误的正则表达式。例如，名称为 SparkDriverOOM 的规则包含指示如果 Spark 作业的 stdout 日志可以匹配正则表达式&lt;em&gt;SparkOutOfMemoryError: 的&lt;/em&gt;信息，则该错误被分类为用户错误，不可重新启动。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Result Finalizer&lt;/strong&gt;负责根据匹配的规则最终确定错误分类结果。如果匹配一条或多条规则，则以第一个匹配的规则的分类决定最终的分类结果（规则优先级由规则排序决定，第一个规则的优先级最高）。另一方面，如果没有匹配的规则，则该错误将被视为未分类。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;挑战&lt;/h4&gt;&lt;p&gt;虽然基于规则的分类器简单且有效，但由于处理错误配置引起的错误和对新错误进行分类的能力有限，它面临着挑战：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;内存配置错误。&lt;/strong&gt;基于规则的分类器提供错误分类结果，指示是否重新启动作业；然而，对于非暂时性错误，仍然需要工程师手动修复。最值得注意的例子是内存配置错误。此类错误通常是由于作业内存配置错误引起的。设置过小的内存可能会导致内存不足 (OOM) 错误，而设置过大的内存会浪费集群内存资源。更具挑战性的是，一些内存配置错误需要更改多个参数的配置。因此，设置合适的内存配置不仅需要手动操作，还需要Spark作业执行的专业知识。此外，即使作业的内存配置最初调整得很好，数据大小和作业定义等更改也可能导致性能下降。鉴于数据平台每月观察到约 600 个内存配置错误，仅及时修复内存配置错误就需要付出不小的工程努力。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;未分类的错误。&lt;/strong&gt;基于规则的分类器依靠数据平台工程师根据已知的上下文手动添加用于识别错误的规则；否则，错误将无法分类。由于数据平台不同层的迁移和应用的多样性，现有规则可能无效，添加新规则需要工程工作，也取决于部署周期。分类器中已添加 300 多个规则，但所有故障中约有 50% 仍未分类。对于未分类的错误，作业可能会使用默认重试策略多次重试。如果错误是非暂时性的，这些失败的重试会产生不必要的作业运行成本。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;演变为自动修复：服务架构&lt;/h3&gt;&lt;h4&gt;方法&lt;/h4&gt;&lt;p&gt;为了解决上述挑战，我们的基本方法是将基于规则的分类器与机器学习服务集成以生成建议，并使用配置服务自动应用建议：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;生成建议。&lt;/strong&gt;我们使用基于规则的分类器作为第一遍，根据预定义的规则对所有错误进行分类，并使用 ML 服务作为第二遍，为内存配置错误和未分类错误提供建议。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;应用建议。&lt;/strong&gt;我们使用在线配置服务来存储和应用推荐的配置。该管道是完全自动化的，并且用于生成和应用建议的服务是解耦的。&lt;/li&gt;&lt;/ul&gt;&lt;h4&gt;服务整合&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*2eENd1mhwyGpMWNccEwqlQ.png" /&gt;&lt;/figure&gt;&lt;p&gt;图 2 说明了数据平台中生成和应用建议的服务的集成。主要服务内容如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; &lt;strong&gt;Nightingale&lt;/strong&gt;是一项运行使用&lt;a href="https://metaflow.org/"&gt;Metaflow&lt;/a&gt;训练的 ML 模型的服务，负责生成重试建议。建议包括 (1) 错误是否可重新启动； (2) 如果是，则重新启动作业的推荐配置。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;ConfigService&lt;/strong&gt;是一个在线配置服务。推荐配置作为 JSON 补丁保存在&lt;strong&gt;ConfigService&lt;/strong&gt;中，其范围定义为指定可以使用推荐配置的作业。当&lt;strong&gt;Scheduler&lt;/strong&gt;调用&lt;strong&gt;ConfigService&lt;/strong&gt;获取推荐配置时， &lt;strong&gt;Scheduler&lt;/strong&gt;将原始配置传递给&lt;strong&gt;ConfigService&lt;/strong&gt; ， &lt;strong&gt;ConfigService&lt;/strong&gt;通过将 JSON 补丁应用于原始配置来返回变异的配置。然后，&lt;strong&gt;调度程序&lt;/strong&gt;可以使用变异的配置（包括推荐的配置）重新启动作业。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Pitive&lt;/strong&gt;是一种利用基于规则的分类器的错误分类服务。它调用&lt;strong&gt;Nightingale&lt;/strong&gt;来获取建议，并将建议存储到&lt;strong&gt;ConfigService&lt;/strong&gt; ，以便&lt;strong&gt;调度程序&lt;/strong&gt;可以选择它来重新启动作业。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Scheduler&lt;/strong&gt;是服务调度作业（我们当前的实现是使用&lt;a href="https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c"&gt;Netflix Maestro&lt;/a&gt; ）。每次作业失败时，都会调用&lt;strong&gt;Pective&lt;/strong&gt;获取错误分类来决定是否重启作业，并调用&lt;strong&gt;ConfigServices&lt;/strong&gt;获取重启作业的推荐配置。 &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gyXv3JyvhUODQWecQqy1zg.png" /&gt;&lt;/figure&gt;&lt;p&gt;图 3 说明了使用自动修复的服务调用顺序：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;当作业失败时， &lt;strong&gt;Scheduler&lt;/strong&gt;会调用&lt;strong&gt;Pively&lt;/strong&gt;来获取错误分类。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Plenty&lt;/strong&gt;根据基于规则的分类器对错误进行分类。如果错误被识别为内存配置错误或未分类错误，它会调用&lt;strong&gt;Nightingale&lt;/strong&gt;来获取建议。&lt;/li&gt;&lt;li&gt;根据获得的推荐， &lt;strong&gt;Pitive&lt;/strong&gt;更新错误分类结果，并将推荐配置保存到&lt;strong&gt;ConfigService&lt;/strong&gt;中；然后将错误分类结果返回给&lt;strong&gt;Scheduler&lt;/strong&gt; 。&lt;/li&gt;&lt;li&gt; &lt;strong&gt;Scheduler根据从Pective&lt;/strong&gt;收到的错误分类结果来决定是否重新启动作业。&lt;/li&gt;&lt;li&gt;在重新启动作业之前，&lt;strong&gt;调度程序&lt;/strong&gt;会调用&lt;strong&gt;ConfigService&lt;/strong&gt;来获取推荐的配置，并使用新配置重试作业。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;演变为自动修复：ML 服务&lt;/h3&gt;&lt;h4&gt;概述&lt;/h4&gt;&lt;p&gt;ML 服务，即 Nightingale，旨在为失败的作业生成重试策略，在重试成功概率和作业运行成本之间进行权衡。它由两个主要部分组成：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;一种预测模型&lt;/strong&gt;，根据重试的属性，联合估计 a) 重试成功的概率和 b) 以美元为单位的重试成本。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;一个优化器&lt;/strong&gt;，它探索 Spark 配置参数空间以推荐可最小化重试失败概率和成本的线性组合的配置。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;预测模型每天都会离线重新训练，并由优化器调用来评估每个候选配置参数值集。优化器在 RESTful 服务中运行，该服务在作业失败时调用。如果优化有可行的配置解决方案，响应将包含此建议，ConfigService 使用该建议来更改配置以进行重试。如果没有可行的解决方案（换句话说，仅通过更改 Spark 配置参数重试不太可能成功），响应中会包含一个禁用重试的标志，从而消除浪费的计算成本。&lt;/p&gt;&lt;h4&gt;预测模型&lt;/h4&gt;&lt;p&gt;鉴于我们想要探索重试成功和重试成本在不同配置场景下可能如何变化，我们需要某种方法使用我们拥有的有关作业的信息来预测这两个值。数据平台记录重试成功结果和执行成本，为我们提供可靠的标签。由于我们使用共享特征集来预测两个目标，具有良好的标签，并且需要快速在线运行推理以满足 SLO，因此我们决定将问题表述为多输出监督学习任务。特别是，我们使用一个简单的前馈多层感知器（MLP），它有两个头，一个用于预测每个结果。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;训练：&lt;/strong&gt;训练集中的每条记录都代表一次可能的重试，该重试之前由于内存配置错误或未分类错误而失败。标签是：a）重试失败，b）重试成本。原始功能输入主要是有关作业的非结构化元数据，例如 Spark 执行计划、运行该作业的用户以及 Spark 配置参数和其他作业属性。我们将这些特征分为可以解析为数值的特征（例如，Spark 执行器内存参数）和不能解析为数值的特征（例如，用户名）。我们使用特征哈希来处理非数字值，因为它们来自高基数和动态值集。然后，我们创建一个较低维度的嵌入，该嵌入与归一化的数值连接并通过更多层。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;推论：&lt;/strong&gt;通过验证审核后，每个新模型版本都存储在&lt;a href="https://metaflow.org/"&gt;Metaflow&lt;/a&gt; Hosting 中，这是我们内部 ML 平台提供的服务。优化器针对每个传入的配置推荐请求对模型预测函数进行多次调用，如下文更详细地描述。&lt;/p&gt;&lt;h4&gt;优化器&lt;/h4&gt;&lt;p&gt;当作业尝试失败时，它会向 Nightingale 发送带有作业标识符的请求。服务根据该标识符构建要在推理调用中使用的特征向量。如前所述，其中一些功能是 Spark 配置参数，它们是要变异的候选者（例如，spark.executor.memory、spark.executor.cores）。 Spark 配置参数集基于广泛从事 Spark 性能调优的领域专家的提炼知识。我们使用贝叶斯优化（通过 Meta 的&lt;a href="https://ax.dev/"&gt;Axe 库&lt;/a&gt;实现）来探索配置空间并生成推荐。在每次迭代中，优化器都会生成候选参数值组合（例如，spark.executor.memory=7192 mb、spark.executor.cores=8），然后通过调用预测模型来评估该候选参数，以估计重试失败概率和成本：候选配置（即，改变特征向量中的值）。在用完固定次数的迭代后，优化器会返回“最佳”配置解决方案（即，最小化组合重试失败和成本目标的解决方案）供 ConfigService 使用（如果可行）。如果找不到可行的解决方案，我们将禁用重试。&lt;/p&gt;&lt;p&gt;优化器迭代设计的一个缺点是任何瓶颈都可能阻止完成并导致超时，我们最初在大量案例中观察到这种情况。经过进一步分析，我们发现大部分延迟来自候选生成步骤（即，在上一次迭代的评估结果之后确定配置空间中的步骤）。我们发现这个问题已向 Ax 库所有者提出，他们&lt;a href="https://github.com/facebook/Ax/issues/810"&gt;在 API 中添加了 GPU 加速选项&lt;/a&gt;。利用这个选项大大降低了我们的超时率。&lt;/p&gt;&lt;h3&gt;投入生产&lt;/h3&gt;&lt;p&gt;我们在生产中部署了自动修复来处理 Spark 作业的内存配置错误和未分类错误。除了重试成功概率和成本效率外，对用户体验的影响也是主要关注点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;对于内存配置错误：&lt;/strong&gt;自动修复可改善用户体验，因为如果没有针对内存配置错误的新配置，作业重试很少会成功。这意味着使用推荐配置重试成功可以减少运行负载并节省作业运行成本，而重试失败并不会让用户体验变得更差。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;对于未分类的错误：&lt;/strong&gt;如果错误无法通过基于规则的分类器中的现有规则进行分类，自动修复会建议是否重新启动作业。特别是，如果 ML 模型预测重试很可能会失败，则会建议禁用重试，这可以节省不必要的重试的作业运行成本。对于业务关键型作业，并且即使重试成功概率很低，用户也喜欢总是重试作业的情况，我们可以向基于规则的分类器添加新规则，以便通过该规则对相同的错误进行分类下次基于 - 的分类器，跳过 ML 服务的建议。这体现了基于规则的分类器和机器学习服务的集成智能的优势。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;生产中的部署表明自动修复&lt;em&gt; &lt;/em&gt;可以针对内存配置错误提供有效的配置，无需人工干预即可成功修复约 56% 的内存配置。它还可以将这些作业的计算成本降低约 50%，因为它可以推荐新配置以使重试成功或禁用不必要的重试。由于性能和成本效率之间的权衡是可调的，因此我们可以决定通过调整 ML 服务来实现更高的成功率或更多的成本节省。&lt;/p&gt;&lt;p&gt;值得注意的是，ML 服务目前采用保守的策略来禁用重试。如上所述，这是为了避免对用户喜欢在作业失败时总是重试作业的情况产生影响。尽管这些情况是预期的，并且可以通过向基于规则的分类器添加新规则来解决，但我们认为以增量方式调整目标函数以逐渐禁用更多重试有助于提供理想的用户体验。鉴于当前禁用重试的政策较为保守，自动修复具有巨大的潜力，最终可以在不影响用户体验的情况下节省更多成本。&lt;/p&gt;&lt;h3&gt;超越错误处理：走向正确的规模调整&lt;/h3&gt;&lt;p&gt;自动修复是我们利用数据洞察和机器学习 (ML) 来改善用户体验、减轻运营负担并提高数据平台成本效率的第一步。它专注于自动修复失败的作业，但也为自动化除错误处理之外的操作铺平了道路。&lt;/p&gt;&lt;p&gt;我们正在采取的举措之一，称为&lt;em&gt;Right Sizing&lt;/em&gt; ，是重新配置计划的大数据作业，以请求适当的资源来执行作业。例如，我们注意到 Spark 作业的平均请求执行程序内存约为其最大已用内存的四倍，表明存在严重的过度配置。除了作业本身的配置之外，还可以减少请求执行作业的容器的资源过剩，以节省成本。通过基于启发式和机器学习的方法，我们可以推断作业执行的正确配置，以最大限度地减少资源过度配置，并在不影响性能的情况下每年节省数百万美元。与自动修复类似，这些配置可以通过 ConfigService 自动应用，无需人工干预。 Right Sizing 正在进行中，稍后将在专门的技术博客文章中介绍更多详细信息。敬请关注。&lt;/p&gt;&lt;h3&gt;致谢&lt;/h3&gt;&lt;p&gt;自动修复是来自不同团队和组织的工程师的共同工作。如果没有扎实、深入的合作，这项工作是不可能完成的。我们衷心感谢所有人，包括 Spark 专家、数据科学家、机器学习工程师、调度程序和作业编排工程师、数据工程师和支持工程师，感谢他们分享背景信息并提供建设性建议和宝贵的反馈（例如&lt;a href="https://www.linkedin.com/in/jzhuge/"&gt;John Zhuge&lt;/a&gt; 、 &lt;a href="https://www.linkedin.com/in/jheua/"&gt;Jun他&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/holdenkarau/"&gt;霍尔顿·卡劳&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/samarthjain11/"&gt;萨玛斯·杰恩&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/julianjaffe/"&gt;朱利安·贾菲&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/batul-shajapurwala-3274b863/"&gt;巴图尔·沙贾普尔瓦拉&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/michael-sachs-b2453b/overlay/about-this-profile/"&gt;迈克尔·萨克斯&lt;/a&gt;、&lt;a href="https://www.linkedin.com/in/fzsiddiqi/overlay/about-this-profile/"&gt;费萨尔·西迪奇&lt;/a&gt;）。 &lt;/p&gt;&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=039d5efd115b" width="1" /&gt;&lt;hr /&gt;&lt;p&gt; &lt;a href="https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b"&gt;从基于规则的分类器演变而来：Netflix 数据中机器学习支持的自动修复……&lt;/a&gt;最初发布于 Medium 上的&lt;a href="https://netflixtechblog.com"&gt;Netflix 技术博客&lt;/a&gt;，人们通过突出显示和回应这个故事来继续对话。&lt;/p&gt;</description><pubDate>Mon, 04 Mar 2024 18:01:55 GMT</pubDate><guid isPermaLink="true">https://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b?source=rss----2615bd06b42e---4</guid></item></channel></rss>